{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-09-19T11:45:42-07:00"
    },
    {
      "path": "index.html",
      "title": "PSYC 575 Multilevel Modeling (2021 Fall)",
      "description": "Welcome to the website. Let's have a great semester!\n",
      "author": [],
      "contents": "\nThis is the course website for PSYC 575 in the 2021 Fall semester. The course follows a flipped course model so that students will review assigned readings and prerecorded lecture videos at their own time, while the class meeting time will be dedicated to Q&A and exercises. You will find the syllabus, lecture materials, and R codes in the navigation bar in the top right corner. Homework submission will be through Blackboard.\nThe lecture videos are licensed under the standard YouTube License\nCode content is licensed under the MIT license\nOther course materials are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\n\n\n\n",
      "last_modified": "2021-09-19T11:45:43-07:00"
    },
    {
      "path": "rcode1.html",
      "title": "R Codes (Week 1)",
      "author": [],
      "contents": "\n\nContents\nRStudio Intro\nRecommended Options\n\nR Markdown\nYAML options\nChunk options\n\nIncluding Plots\nInstall packages\nLoad a package\nData Frame\nBasic Markdown Elements\nItalic and bolded texts\nLists (Ordered and Unordered)\nEquations (LaTeX)\n\nCheatsheet\nExercise\n\nClick here to download the Rmd file: week1-intro-RMarkdown.Rmd\n\n\n\n\n\n\n\n\n\n\nRStudio Intro\n\n\nprint(\"This is Thursday.\")\n\n\n[1] \"This is Thursday.\"\n\n\n\n\n\nIf you’re new to R, follow the online ModernDrive chapter https://moderndive.netlify.app/1-getting-started.html\nFour Panes:\nSource panel\nConsole\nGlobal environment\nFiles/Plots/Packages/Help/Viewer\n\nRecommended Options\nTools --> Global Options --> \n- Uncheck \"Restore .RData into workspace at startup\".\n- Set \"Save workspace to .RData on exit\" to \"Never\".  \n- (Optional) R Markdown --> Set \"Show output preview in\" to \"Viewer Pane\".\n\nUse Project\nR Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nNote: R is case sensitive. So Cars and cars are different.\n\n\n\nsummary(cars[-(1:4), ])\n\n\n\nYAML options\ntitle, author, date, output\nChunk options\ninclude\necho\neval\nresults = 'hide'\nIncluding Plots\nYou can also embed plots, for example:\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\nInstall packages\n\n\ntweetrmd::include_tweet(\"https://twitter.com/visnut/status/1248087845589274624\")\n\n\nStudents struggle with install vs library every semester, and keep including their install code in the RMarkdown report. Hope this meme might help in the future #rstats pic.twitter.com/I8xg053y0x— Dr Di Cook (@visnut) April 9, 2020\n\n\n\n\n\nCheck out https://rladiessydney.org/courses/ryouwithme/01-basicbasics-2/\n\n\n# Install the tidyverse \"meta\" package\n# install.packages(\"tidyverse\")\n# Install the lme4 package\n# install.packages(\"lme4\")\n\n\n\nLoad a package\n\nTip 1: Try the (PC) Ctrl + Alt + I/(Mac) Cmd + Option + I shortcut for a new code chunk\n\n\nTip 2: Use Tab for code completeion\n\n\nTip 3: Use Ctrl + Enter/(Mac) Cmd + Return for running a line of R code\n\n\nTip 4: Set message = FALSE to suppress messages in loading packages\n\n\n\n# Load tidyverse\n\n# Load lme4\n\n\n\nData Frame\n\n\n# Load SleepStudy\n\n# Extract one column\n\n# Extract column by index (same as last line)\n\n# Extract two rows\n\n# Compute the mean and sd, and chain them together\n\n# Correlation matrix with psych::pairs.panel()\n\n# Find out what a function does (use `?function_name`, e.g., `?pairs.panel`)\n\n\n\nBasic Markdown Elements\nFrom RStudio, click Help –> Markdown Quick Reference\nItalic and bolded texts\nThis is italic\nLists (Ordered and Unordered)\nItem 1\nItem 2\nEquations (LaTeX)\nInline: The correlation between \\(a = b + c + \\tau\\)\nDisplay:\n\\[a = b + c + \\tau\\]\nCheatsheet\nMore detailed cheatsheet: https://rmarkdown.rstudio.com/lesson-15.HTML\nExercise\nDownload the Rmd file for the exercise on Blackboard\nInsert your name on line 3.\nComplete the following in this R Markdown document:\nCopy the following LaTex equation to below: A_1 = \\pi r^2. How does this say about writing Greek letters and subscripts/superscripts? \\[[Insert equation here]\\]\nInstall and then load the modelsummary package, and run the following. You’ll need to remove eval = FALSE so that it runs. Find out what this code chunk does.\n\n\n# Install and load the modelsummary package first; otherwise it won't run\n library(modelsummary)\n fm1 <- lm(dist ~ speed, data = cars)\n fm2 <- lm(dist ~ poly(speed, 2), data = cars)\n fm3 <- lm(log(dist) ~ log(speed), data = cars)\n msummary(list(fm1, fm2, fm3))\n \n\n\nRun the following. You’ll need to remove eval = FALSE so that it runs. Find out what this code chunk does.\n\n\nggplot(cars, aes(x = log(speed), y = log(dist))) +\n   geom_point() +\n   geom_smooth()\n \n\n\nAdd a code chunk below to show the output of running sessionInfo(), which prints out the session information of your computer. Make the code chunk to show only the output, but not the code.\nKnit the document to HTML, PDF, and Word. If you run into an error when knitting to any one of the formats, record the error message. Which format do you prefer?\nGo to the top of this Rmd file, and change the line inside YAML\n  html_document: default\nto\n  html_document: \n      toc: TRUE\nKnit the document again. What does it do?\nSubmit the knitted document to Blackboard in your preferred format (HTML, PDF, or WORD)\n\n\n",
      "last_modified": "2021-09-19T11:45:46-07:00"
    },
    {
      "path": "rcode2.html",
      "title": "R Codes (Week 2)",
      "author": [],
      "contents": "\n\nContents\nSimulation\nLoad Packages and Import Data\nImport Data\n\nQuick Scatterplot Matrix\n1. Linear Regression of salary on pub\nVisualize the data\nLinear regression\nVisualize fitted regression line:\nConfidence intervals\nInterpretations\nSimulations\n\\(p\\) values\nCentering\n\n2. Categorical Predictor\nEquivalence to the \\(t\\)-test\n\n3. Multiple Predictors (Multiple Regression)\nPlotting\nInterpretations\nDiagnostics\nEffect size\n\n4. Interaction\nInteraction Plots\n\n5. Tabulate the Regression Results\nBonus: Matrix Form of Regression\nBonus: More Options in Formatting Tables\n\n\n\\[\n\\newcommand{\\bv}[1]{\\boldsymbol{\\mathbf{#1}}}\n\\]\n\nClick here to download the Rmd file: week2-review-multiple-regression.Rmd\nSimulation\n\n\n\n\n\nPractice yourself: From the simulation code provided, try to increase the sample size from 62 to something larger, like 200. How does this affect the uncertainty in the sample slopes?\nLoad Packages and Import Data\nYou can use add the message=FALSE option to suppress the package loading messages\n\n\n# To install a package, run the following ONCE (and only once on your computer)\n# install.packages(\"psych\")  \nlibrary(psych)  # for scatterplot matrix\nlibrary(here)  # makes reading data more consistent\nlibrary(tidyverse)  # for data manipulation and plotting\nlibrary(car)  # some useful functions for regression\nlibrary(modelsummary)  # for making tables\nlibrary(sjPlot)  # for plotting slopes\nlibrary(interactions)  # for plotting interactions\ntheme_set(theme_bw())  # Theme; just my personal preference\n\n\n\nImport Data\nFirst, download the data file salary.txt from https://raw.githubusercontent.com/marklhc/marklai-pages/master/data_files/salary.txt, and import the data. A robust way to do so is to download the data to a folder called data_files under the project directory, and then use the here package. This avoids a lot of data import issues that I’ve seen.\n\n\n# The `here()` function forces the use of the project directory\nhere(\"data_files\", \"salary.txt\")\n# Read in the data\nsalary_dat <- read.table(here(\"data_files\", \"salary.txt\"), header = TRUE)\n\n\n\nAlternatively, from the menu, click File → Import Dataset → From Text (base)..., and select the file.\n\n\n# Show the data\nsalary_dat\n\n\n>#    id time pub sex citation salary\n># 1   1    3  18   1       50  51876\n># 2   2    6   3   1       26  54511\n># 3   3    3   2   1       50  53425\n># 4   4    8  17   0       34  61863\n># 5   5    9  11   1       41  52926\n># 6   6    6   6   0       37  47034\n># 7   7   16  38   0       48  66432\n># 8   8   10  48   0       56  61100\n># 9   9    2   9   0       19  41934\n># 10 10    5  22   0       29  47454\n># 11 11    5  30   1       28  49832\n># 12 12    6  21   0       31  47047\n># 13 13    7  10   1       25  39115\n># 14 14   11  27   0       40  59677\n># 15 15   18  37   0       61  61458\n># 16 16    6   8   0       32  54528\n># 17 17    9  13   1       36  60327\n># 18 18    7   6   0       69  56600\n># 19 19    7  12   1       47  52542\n># 20 20    3  29   1       29  50455\n># 21 21    7  29   1       35  51647\n># 22 22    5   7   0       35  62895\n># 23 23    7   6   0       18  53740\n># 24 24   13  69   0       90  75822\n># 25 25    5  11   0       60  56596\n># 26 26    8   9   1       30  55682\n># 27 27    8  20   1       27  62091\n># 28 28    7  41   1       35  42162\n># 29 29    2   3   1       14  52646\n># 30 30   13  27   0       56  74199\n># 31 31    5  14   0       50  50729\n># 32 32    3  23   0       25  70011\n># 33 33    1   1   0       35  37939\n># 34 34    3   7   0        1  39652\n># 35 35    9  19   0       69  68987\n># 36 36    3  11   0       69  55579\n># 37 37    9  31   0       27  54671\n># 38 38    3   9   0       50  57704\n># 39 39    4  12   1       32  44045\n># 40 40   10  32   0       33  51122\n># 41 41    1  26   0       45  47082\n># 42 42   11  12   0       54  60009\n># 43 43    5   9   0       47  58632\n># 44 44    1   6   0       29  38340\n># 45 45   21  39   0       69  71219\n># 46 46    7  16   1       47  53712\n># 47 47    5  12   1       43  54782\n># 48 48   16  50   0       55  83503\n># 49 49    5  18   0       33  47212\n># 50 50    4  16   1       28  52840\n># 51 51    5   5   0       42  53650\n># 52 52   11  20   0       24  50931\n># 53 53   16  50   1       31  66784\n># 54 54    3   6   1       27  49751\n># 55 55    4  19   1       83  74343\n># 56 56    4  11   1       49  57710\n># 57 57    5  13   0       14  52676\n># 58 58    6   3   1       36  41195\n># 59 59    4   8   1       34  45662\n># 60 60    8  11   1       70  47606\n># 61 61    3  25   1       27  44301\n># 62 62    4   4   1       28  58582\n\nYou can see the description of the variables here: https://rdrr.io/cran/MBESS/man/prof.salary.html\nQuick Scatterplot Matrix\nImport to screen your data before any statistical modeling\n\n\npairs.panels(salary_dat[ , -1],  # not plotting the first column\n             ellipses = FALSE)\n\n\n\n\n1. Linear Regression of salary on pub\nVisualize the data\n\n\n# Visualize the data (\"gg\" stands for grammar of graphics)\np1 <- ggplot(salary_dat,  # specify data\n             # aesthetics: mapping variable to axes)\n             aes(x = pub, y = salary)) +  \n             # geom: geometric objects, such as points, lines, shapes, etc\n             geom_point()\n# Add a smoother geom to visualize mean salary as a function of pub\np1 + geom_smooth()\n\n\n\n\nA little bit of non-linearity on the plot. Now fit the regression model\nLinear regression\nYou can type equations (with LaTeX; see a quick reference).\nP.S. Use \\text{} to specify variable names\nP.S. Pay attention to the subscripts\n\\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}_i + e_i\\]\nOutcome: salary\nPredictor: pub\n\\(\\beta_0\\): regression intercept\n\\(\\beta_1\\): regression slope\n\\(e\\): error\n\n\n# left hand side of ~ is outcome; right hand side contains predictors\n# salary ~ (beta_0) * 1 + (beta_1) * pub\n# remove beta_0 and beta_1 to get the formula\nm1 <- lm(salary ~ 1 + pub, data = salary_dat)\n# In R, the output is not printed out if it is saved to an object (e.g., m1).\n# Summary:\nsummary(m1)\n\n\n># \n># Call:\n># lm(formula = salary ~ 1 + pub, data = salary_dat)\n># \n># Residuals:\n>#      Min       1Q   Median       3Q      Max \n># -20660.0  -7397.5    333.7   5313.9  19238.7 \n># \n># Coefficients:\n>#             Estimate Std. Error t value Pr(>|t|)    \n># (Intercept) 48439.09    1765.42  27.438  < 2e-16 ***\n># pub           350.80      77.17   4.546 2.71e-05 ***\n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 8440 on 60 degrees of freedom\n># Multiple R-squared:  0.2562, Adjusted R-squared:  0.2438 \n># F-statistic: 20.67 on 1 and 60 DF,  p-value: 2.706e-05\n\nVisualize fitted regression line:\n\n\np1 +\n  # Non-parametric fit\n  geom_smooth(se = FALSE) +\n  # Linear regression line (in red)\n  geom_smooth(method = \"lm\", col = \"red\")\n\n\n\n\nConfidence intervals\n\n\n# Confidence intervals\nconfint(m1)\n\n\n>#                 2.5 %     97.5 %\n># (Intercept) 44907.729 51970.4450\n># pub           196.441   505.1625\n\nInterpretations\n\nBased on our model, faculty with one more publication have predicted salary of $350.8, 95% CI [$196.4, $505.2], higher than those with one less publication.\n\nBut what do the confidence intervals and the standard errors mean? To understanding what exactly a regression model is, let’s run some simulations.\nSimulations\nBefore you go on, take a look on a brief introductory video by Clark Caylord on Youtube on simulating data based on a simple linear regression model.\nTo my delight, I also found out a gentle introduction of simulation method by one of our clinical science students at USC (Thanks Kayla!). You can access it through the USC library here (Sign-on required). Here’s the citation:\nTureson, K. & Odland, A. (2018). Monte Carlo simulation studies. In B. Frey (Ed.),The SAGE Encyclopedia of Educational Research, Measurement, and Evaluation. Thousand Oaks, CA: SAGE Publications.\nBased on the analyses, the sample regression line is \\[\\widehat{\\text{salary}} = 48439.09 + 350.80 \\text{pub}.\\] The numbers are only sample estimates as there are sampling errors. However, if we assume that this line truly describe the relation between salary and pub, then we can simulate some fake data, which represents what we could have obtained in a different sample.\nGoing back to the equation \\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}_i + e_i,\\] the only thing that changes across different samples, based on the statistical model, is \\(e_i\\). Conceptually, you can think of the error term \\(e_i\\) as the deviation of person \\(i\\)’s salary from the mean salary of everyone in the population who has the same number of publications as person \\(i\\). Here we assume that \\(e_i\\) is normally distributed, written as \\(e_i \\sim N(0, \\sigma)\\), where \\(\\sigma\\) describes the conditional standard deviation (i.e., the standard deviation across individuals who have the same number of publications). From the regression results, \\(\\sigma\\) is estimated as the Residual standard error = 8,440.\nBased on these model assumptions, we can imagine a large population, say with 10,000 people\n\n\n# Simulating a large population. The code in this chunk \n# is not essential for conceptual understanding\nNpop <- 10000\nbeta0 <- 48439.09\nbeta1 <- 350.80\nsigma <- 8440\n# Simulate population data\nsimulated_population <- tibble(\n  # Simulate population pub\n  pub = local({\n    dens <- density(c(-salary_dat$pub, salary_dat$pub),\n      bw = \"SJ\",\n      n = 1024\n    )\n    dens_x <- c(0, dens$x[dens$x > 0])\n    dens_y <- c(0, dens$y[dens$x > 0])\n    round(\n      approx(\n        cumsum(dens_y) / sum(dens_y),\n        dens_x,\n        runif(Npop)\n      )$y\n    )\n  }),\n  # Simulate error\n  e = rnorm(Npop, mean = 0, sd = sigma)\n)\n# Compute salary variable\nsimulated_population$salary <-\n  beta0 + beta1 * simulated_population$pub + simulated_population$e\n# Plot\np_pop <- ggplot(\n  data = simulated_population,\n  aes(x = pub, y = salary)\n) +\n  geom_point(alpha = 0.1) +\n  # Add population regression line in blue\n  geom_smooth(se = FALSE, method = \"lm\")\np_pop\n\n\n\n\nNow, we can simulate some fake (but plausible) samples. An important thing to remember is we want to simulate data that have the same size as the original sample, because we’re comparing to other plausible samples with equal size. In R there is a handy simulate() function to do that.\n\n\nsimulated_salary <- simulate(m1)\n# Add simulated salary to the original data\n# Note: the simulated variable is called `sim_1`\nsim_data1 <- bind_cols(salary_dat, simulated_salary)\n# Show the first six rows\nhead(sim_data1)\n\n\n>#   id time pub sex citation salary    sim_1\n># 1  1    3  18   1       50  51876 59924.58\n># 2  2    6   3   1       26  54511 58792.92\n># 3  3    3   2   1       50  53425 43897.33\n># 4  4    8  17   0       34  61863 56445.60\n># 5  5    9  11   1       41  52926 49479.98\n># 6  6    6   6   0       37  47034 49203.16\n\n# Plot the data (add on to the population)\np_pop + \n  geom_point(data = sim_data1, \n             aes(x = pub, y = sim_1),\n             col = \"red\") + \n  # Add sample regression line\n  geom_smooth(data = sim_data1, \n              aes(x = pub, y = sim_1),\n              method = \"lm\", se = FALSE, col = \"red\")\n\n\n\n\n\n\n# To be more transparent, here's what the simulate() function essentially is\n# doing\nsample_size <- nrow(salary_dat)\nsim_data1 <- tibble(\n  pub = salary_dat$pub,\n  # simulate error\n  e = rnorm(sample_size, mean = 0, sd = sigma)\n)\n# Compute new salary data\nsim_data1$sim_1 <- beta0 + beta1 * sim_data1$pub + sim_data1$e\n\n\n\nAs you can see, the sample regression line in red is different from the blue line.\nDrawing 100 samples\nLet’s draw more samples\n\n\nnum_samples <- 100\nsimulated_salary100 <- simulate(m1, nsim = num_samples)\n# Form a giant data set with 100 samples\nsim_data100 <- bind_cols(salary_dat, simulated_salary100) %>%\n  pivot_longer(sim_1:sim_100,\n    names_prefix = \"sim_\",\n    names_to = \"sim\",\n    values_to = \"simulated_salary\"\n  )\n# Plot by samples\np_sim100 <- ggplot(\n  sim_data100,\n  aes(x = pub, y = simulated_salary, group = sim)\n) +\n  geom_point(col = \"red\", alpha = 0.1) +\n  geom_smooth(col = \"red\", se = FALSE, method = \"lm\")\n# Use gganimate (this takes some time to render)\nlibrary(gganimate)\np_sim100 + transition_states(sim) +\n  ggtitle(\"Simulation {frame} of {nframes}\")\nanim_save(here::here(\"sim-100-samples.gif\"))\n\n\n\n\nWe can show the regression lines for all 100 samples\n\n\nggplot(sim_data100,\n       aes(x = pub, y = simulated_salary, group = sim)) +\n  stat_smooth(\n    geom = \"line\",\n    col = \"red\",\n    se = FALSE,\n    method = \"lm\",\n    alpha = 0.4\n  )\n\n\n\n\nThe confidence intervals of the intercept and the slope how much uncertainty there is.\n\\(p\\) values\nNow, we can also understand the \\(p\\) value for the regression slope of pub is. Remember that the \\(p\\) value is the probability that, if the regression slope of pub is zero (i.e., no association), how likely/unlikely would we get the sample slope (i.e., 350.80) in our data. So now we’ll repeat the simulation, but without pub as a predictor (i.e., assuming \\(\\beta_1 = 0\\)).\n\n\nnum_samples <- 100\nm0 <- lm(salary ~ 1, data = salary_dat)  # null model\nsimulated_salary100_null <- simulate(m0, nsim = num_samples)\n# Form a giant data set with 100 samples\nsim_null100 <- bind_cols(salary_dat, simulated_salary100_null) %>% \n  pivot_longer(sim_1:sim_100, \n               names_prefix = \"sim_\", \n               names_to = \"sim\", \n               values_to = \"simulated_salary\")\n# Show the null slopes\nggplot(data = salary_dat,\n       aes(x = pub, y = salary)) +\n  stat_smooth(\n    data = sim_null100,\n    aes(x = pub, y = simulated_salary, group = sim),\n    geom = \"line\",\n    col = \"darkgrey\",\n    se = FALSE,\n    method = \"lm\"\n  ) +\n  geom_smooth(method = \"lm\", col = \"red\", se = FALSE) +\n  geom_point()\n\n\n\n\nSo you can see the sample slope is larger than what you would expect to see if the true slope is zero. So the \\(p\\) value is very small, and the result is statistically significant (at, say, .05 level).\nCentering\nSo that the intercept refers to a more meaningful value. It’s a major issue in multilevel modeling.\n\\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}^c_i + e_i\\]\n\n\n# Using pipe operator\nsalary_dat <- salary_dat %>% \n  mutate(pub_c = pub - mean(pub))\n# Equivalent to:\n# salary_dat <- mutate(salary_dat, \n#                      pub_c = pub - mean(pub))\nm1c <- lm(salary ~ pub_c, data = salary_dat)\nsummary(m1c)\n\n\n># \n># Call:\n># lm(formula = salary ~ pub_c, data = salary_dat)\n># \n># Residuals:\n>#      Min       1Q   Median       3Q      Max \n># -20660.0  -7397.5    333.7   5313.9  19238.7 \n># \n># Coefficients:\n>#             Estimate Std. Error t value Pr(>|t|)    \n># (Intercept) 54815.76    1071.93  51.137  < 2e-16 ***\n># pub_c         350.80      77.17   4.546 2.71e-05 ***\n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 8440 on 60 degrees of freedom\n># Multiple R-squared:  0.2562, Adjusted R-squared:  0.2438 \n># F-statistic: 20.67 on 1 and 60 DF,  p-value: 2.706e-05\n\nThe only change is the intercept coefficient\n\n\np1 + \n  geom_smooth(method = \"lm\", col = \"red\") + \n  # Intercept without centering\n  geom_vline(aes(col = \"Not centered\", xintercept = 0)) + \n  # Intercept with centering\n  geom_vline(aes(col = \"Centered\", xintercept = mean(salary_dat$pub))) + \n  labs(col = \"\")\n\n\n\n\n2. Categorical Predictor\nRecode sex as factor variable in R (which allows R to automatically do dummy coding). This should be done in general for categorical predictors.\n\n\nsalary_dat <- salary_dat %>% \n  mutate(sex = factor(sex, levels = c(0, 1), \n                      labels = c(\"male\", \"female\")))\n\n\n\n\\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{sex}_i + e_i\\]\n\n\n(p2 <- ggplot(salary_dat, aes(x = sex, y = salary)) + \n    geom_boxplot() + \n    geom_jitter(height = 0, width = 0.1))  # move the points to left/right a bit\n\n\n\n\n\n\nm2 <- lm(salary ~ sex, data = salary_dat)\nsummary(m2)\n\n\n># \n># Call:\n># lm(formula = salary ~ sex, data = salary_dat)\n># \n># Residuals:\n>#    Min     1Q Median     3Q    Max \n># -18576  -5736    -19   4853  26988 \n># \n># Coefficients:\n>#             Estimate Std. Error t value Pr(>|t|)    \n># (Intercept)    56515       1620  34.875   <2e-16 ***\n># sexfemale      -3902       2456  -1.589    0.117    \n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 9587 on 60 degrees of freedom\n># Multiple R-squared:  0.04039,    Adjusted R-squared:  0.02439 \n># F-statistic: 2.525 on 1 and 60 DF,  p-value: 0.1173\n\nThe (Intercept) coefficient is for the ‘0’ category, i.e., predicted salary for males; the female coefficient is the difference between males and females.\nPredicted female salary = 56515 + (-3902) = 52613.\nEquivalence to the \\(t\\)-test\nWhen assuming homogeneity of variance\n\n\nt.test(salary ~ sex, data = salary_dat, var.equal = TRUE)\n\n\n># \n>#  Two Sample t-test\n># \n># data:  salary by sex\n># t = 1.5891, df = 60, p-value = 0.1173\n># alternative hypothesis: true difference in means between group male and group female is not equal to 0\n># 95 percent confidence interval:\n>#  -1009.853  8814.041\n># sample estimates:\n>#   mean in group male mean in group female \n>#             56515.06             52612.96\n\n3. Multiple Predictors (Multiple Regression)\nNow add one more predictor, time \\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}^c_i + \\beta_2 \\text{time}_i + e_i\\]\n\n\nggplot(salary_dat, aes(x = time, y = salary)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\n\n\nm3 <- lm(salary ~ pub_c + time, data = salary_dat)\nsummary(m3)  # summary\n\n\n># \n># Call:\n># lm(formula = salary ~ pub_c + time, data = salary_dat)\n># \n># Residuals:\n>#    Min     1Q Median     3Q    Max \n># -15919  -5537   -985   4861  22476 \n># \n># Coefficients:\n>#             Estimate Std. Error t value Pr(>|t|)    \n># (Intercept) 47373.38    2281.78  20.762  < 2e-16 ***\n># pub_c         133.00      92.73   1.434 0.156797    \n># time         1096.03     303.58   3.610 0.000632 ***\n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 7703 on 59 degrees of freedom\n># Multiple R-squared:  0.3908, Adjusted R-squared:  0.3701 \n># F-statistic: 18.92 on 2 and 59 DF,  p-value: 4.476e-07\n\nconfint(m3)  # confidence interval\n\n\n>#                   2.5 %     97.5 %\n># (Intercept) 42807.55413 51939.2044\n># pub_c         -52.56216   318.5589\n># time          488.56251  1703.4921\n\nThe regression coefficients are the partial effects.\n\nPlotting\nThe sjPlot::plot_model() function is handy\n\n\nsjPlot::plot_model(m3, type = \"pred\", show.data = TRUE,\n                   title = \"\")  # remove title\n\n\n># $pub_c\n\n># \n># $time\n\n\nInterpretations\n\nFaculty who have worked longer tended to have more publications For faculty who graduate around the same time, a difference of 1 publication is associated with an estimated difference in salary of $133.0, 95% CI [$-52.6, $318.6], which was not significant.\n\nDiagnostics\n\n\ncar::mmps(m3)  # marginal model plots for linearity assumptions\n\n\n\n\nThe red line is the implied association based on the model, whereas the blue line is a non-parametric smoother not based on the model. If the two lines show big discrepancies (especially if in the middle), it may suggest the linearity assumptions in the model does not hold.\nEffect size\n\n\n# Extract the R^2 number (it's sometimes tricky to \n# figure out whether R stores the numbers you need)\nsummary(m3)$r.squared\n\n\n># [1] 0.3907761\n\n# Adjusted R^2\nsummary(m3)$adj.r.squared\n\n\n># [1] 0.3701244\n\nProportion of predicted variance: \\(R^2\\) = 39%, adj. \\(R^2\\) = 37%.\n4. Interaction\nFor interpretation purposes, it’s recommended to center the predictors (at least the continuous ones)\n\\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}^c_i + \\beta_2 \\text{time}^c_i + \\beta_3 (\\text{pub}^c_i)(\\text{time}^c_i) + e_i\\]\n\n\nsalary_dat <- salary_dat %>% \n  mutate(time_c = time - mean(time))\n# Fit the model with interactions:\nm4 <- lm(salary ~ pub_c * time_c, data = salary_dat)\nsummary(m4)  # summary\n\n\n># \n># Call:\n># lm(formula = salary ~ pub_c * time_c, data = salary_dat)\n># \n># Residuals:\n>#    Min     1Q Median     3Q    Max \n># -14740  -5305   -373   4385  22744 \n># \n># Coefficients:\n>#              Estimate Std. Error t value Pr(>|t|)    \n># (Intercept)  54238.08    1183.01  45.847  < 2e-16 ***\n># pub_c          104.72      98.41   1.064  0.29169    \n># time_c         964.17     339.68   2.838  0.00624 ** \n># pub_c:time_c    15.07      17.27   0.872  0.38664    \n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 7719 on 58 degrees of freedom\n># Multiple R-squared:  0.3987, Adjusted R-squared:  0.3676 \n># F-statistic: 12.82 on 3 and 58 DF,  p-value: 1.565e-06\n\nInteraction Plots\nInterpreting interaction effects is hard. Therefore,\n\nAlways plot the interaction to understand the dynamics\n\n\n\ninteractions::interact_plot(m4,\n  pred = \"pub_c\",\n  modx = \"time_c\",\n  # Insert specific values to plot the slopes.\n  # Pay attention that `time_c` has been centered\n  modx.values = c(1, 7, 15) - 6.79,\n  modx.labels = c(1, 7, 15),\n  plot.points = TRUE,\n  x.label = \"Number of publications (mean-centered)\",\n  y.label = \"Salary\",\n  legend.main = \"Time since Ph.D.\"\n)\n\n\n\n\nAnother approach is to plug in numbers to the equation: \\[\\widehat{\\text{salary}} = \\hat \\beta_0 + \\hat \\beta_1 \\text{pub}^c + \\hat \\beta_2 \\text{time}^c + \\hat \\beta_3 (\\text{pub}^c)(\\text{time}^c)\\] For example, consider people who’ve graduated for seven years, i.e., time = 7. First, be careful that in the model we have time_c, and time = 7 corresponds to time_c = 0.2096774 years. So if we plug that into the equation, \\[\\widehat{\\text{salary}} |_{\\text{time} = 7} = \\hat \\beta_0 + \\hat \\beta_1 \\text{pub}^c + \\hat \\beta_2 (0.21) + \\hat \\beta_3 (\\text{pub}^c)(0.21)\\] Combining terms with pubc, \\[\\widehat{\\text{salary}} |_{\\text{time} = 7} = [\\hat \\beta_0 + \\hat \\beta_2 (0.21)] + [\\hat \\beta_1 + \\hat \\beta_3 (0.21)] (\\text{pub}^c)\\] Now plug in the numbers for \\(\\hat \\beta_0\\), \\(\\hat \\beta_1\\), \\(\\hat \\beta_2\\), \\(\\hat \\beta_3\\),\n\n\n# beta0 + beta2 * 0.21\n54238.08 + 964.17 * 0.21\n\n\n># [1] 54440.56\n\n# beta1 + beta3 * 0.21\n104.72 + 15.07 * 0.21\n\n\n># [1] 107.8847\n\nresulting in \\[\\widehat{\\text{salary}} |_{\\text{time} = 7} = 54440.6 + 107.9 (\\text{pub}^c), \\] which is the regression line for time = 7. Note, however, when an interaction is present, the regression slope will be different with a different value of time. So remember that\n\nAn interaction means that the regression slope of a predictor depends on another predictor.\n\nWe will further explore this in the class exercise this week.\n5. Tabulate the Regression Results\n\n\nmsummary(list(\n  \"M1\" = m1,\n  \"M2\" = m2,\n  \"M3\" = m3,\n  \"M3 + Interaction\" = m4\n),\nfmt = \"%.1f\"\n) # keep one digit\n\n\n\n\n\nM1\n\n\nM2\n\n\nM3\n\n\nM3 + Interaction\n\n\n(Intercept)\n\n\n48439.1\n\n\n56515.1\n\n\n47373.4\n\n\n54238.1\n\n\n\n\n(1765.4)\n\n\n(1620.5)\n\n\n(2281.8)\n\n\n(1183.0)\n\n\npub\n\n\n350.8\n\n\n\n\n\n\n\n\n\n\n(77.2)\n\n\n\n\n\n\n\n\nsexfemale\n\n\n\n\n−3902.1\n\n\n\n\n\n\n\n\n\n\n(2455.6)\n\n\n\n\n\n\npub_c\n\n\n\n\n\n\n133.0\n\n\n104.7\n\n\n\n\n\n\n\n\n(92.7)\n\n\n(98.4)\n\n\ntime\n\n\n\n\n\n\n1096.0\n\n\n\n\n\n\n\n\n\n\n(303.6)\n\n\n\n\ntime_c\n\n\n\n\n\n\n\n\n964.2\n\n\n\n\n\n\n\n\n\n\n(339.7)\n\n\npub_c × time_c\n\n\n\n\n\n\n\n\n15.1\n\n\n\n\n\n\n\n\n\n\n(17.3)\n\n\nNum.Obs.\n\n\n62\n\n\n62\n\n\n62\n\n\n62\n\n\nR2\n\n\n0.256\n\n\n0.040\n\n\n0.391\n\n\n0.399\n\n\nR2 Adj.\n\n\n0.244\n\n\n0.024\n\n\n0.370\n\n\n0.368\n\n\nAIC\n\n\n1301.0\n\n\n1316.8\n\n\n1290.6\n\n\n1291.8\n\n\nBIC\n\n\n1307.4\n\n\n1323.1\n\n\n1299.1\n\n\n1302.4\n\n\nLog.Lik.\n\n\n−647.486\n\n\n−655.383\n\n\n−641.299\n\n\n−640.895\n\n\nF\n\n\n20.665\n\n\n2.525\n\n\n18.922\n\n\n12.817\n\n\nBonus: Matrix Form of Regression\nThe regression model can be represented more succintly in matrix form: \\[\\bv y = \\bv X \\bv \\beta + \\bv e,\\] where \\(\\bv y\\) is a column vector (which can be considered a \\(N \\times 1\\) matrix). For example, for our data\n\n\nhead(salary_dat)\n\n\n>#   id time pub    sex citation salary       pub_c     time_c\n># 1  1    3  18 female       50  51876  -0.1774194 -3.7903226\n># 2  2    6   3 female       26  54511 -15.1774194 -0.7903226\n># 3  3    3   2 female       50  53425 -16.1774194 -3.7903226\n># 4  4    8  17   male       34  61863  -1.1774194  1.2096774\n># 5  5    9  11 female       41  52926  -7.1774194  2.2096774\n># 6  6    6   6   male       37  47034 -12.1774194 -0.7903226\n\nSo \\[\\bv y = \\begin{bmatrix}\n            51,876 \\\\\n            54,511 \\\\\n            53,425 \\\\\n            \\vdots\n          \\end{bmatrix}\\] \\(\\bv X\\) is the predictor matrix (sometimes also called the design matrix), where the first column is the constant 1, and each subsequent column represent a predictor. You can see this in R\n\n\nhead(\n  model.matrix(m3)\n)\n\n\n>#   (Intercept)       pub_c time\n># 1           1  -0.1774194    3\n># 2           1 -15.1774194    6\n># 3           1 -16.1774194    3\n># 4           1  -1.1774194    8\n># 5           1  -7.1774194    9\n># 6           1 -12.1774194    6\n\nThe coefficient \\(\\bv \\beta\\) is a vector, with elements \\(\\beta_0, \\beta_1, \\ldots\\). The least square estimation method is used to find estimates of \\(\\beta\\) that minimizes the sum of squared differences between \\(\\bv y\\) and \\(\\bv X \\hat{\\bv \\beta}\\), which can be written as \\[(\\bv y - \\bv X \\bv \\beta)^\\top(\\bv y - \\bv X \\bv \\beta).\\] The above means that: for each value observation, subtract the predicted value of \\(y\\) from the observed \\(y\\) (i.e., \\(y_i - \\beta_0 + \\beta_1 x_{1i} + \\ldots\\)), then squared the value (\\([y_i - \\beta_0 + \\beta_1 x_{1i} + \\ldots]^2\\)), then sum these squared values across observations. Or sometimes you’ll see it written as \\[\\lVert\\bv y - \\bv X \\bv \\beta)\\rVert^2\\] It can be shown that the least square estimates can be obtained as \\[(\\bv X^\\top \\bv X)^{-1} \\bv X^\\top \\bv y\\] You can do the matrix form in R:\n\n\ny <- salary_dat$salary\nX <- model.matrix(m3)\n# beta = (X'X)^{-1} X'y\n# solve() is matrix inverse; t(X) is the transpose of X; use `%*%` for matrix multiplication\n(betahat <- solve(t(X) %*% X, t(X) %*% y))  # same as the coefficients in m3\n\n\n>#                   [,1]\n># (Intercept) 47373.3792\n># pub_c         132.9984\n># time         1096.0273\n\n# Sum of squared residual\nsum((y - X %*% betahat)^2)\n\n\n># [1] 3500978295\n\n# Root mean squared residual (Residual standard error)\nsqrt(sum((y - X %*% betahat)^2) / 59)  # same as in R\n\n\n># [1] 7703.156\n\nBonus: More Options in Formatting Tables\nHere’s some code you can explore to make the table output from msummary() to look more lik APA style (with an example here: https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables#regression). However, for this course I don’t recommend spending too much time on tailoring the tables; something clear and readable will be good enough.\n\n\n# Show confidence intervals and p values\nmsummary(\n  list(\n    \"Estimate\" = m4,\n    \"95% CI\" = m4,\n    \"p\" = m4\n  ),\n  estimate = c(\"estimate\", \"[{conf.low}, {conf.high}]\", \"p.value\"),\n  statistic = NULL,\n  # suppress gof indices (e.g., R^2)\n  gof_omit = \".*\",\n  # Rename the model terms (\"current name\" = \"new name\")\n  coef_rename = c(\n    \"(Intercept)\" = \"Intercept\",\n    \"pub_c\" = \"Number of publications\",\n    \"time_c\" = \"Time since PhD\",\n    \"pub_c:time_c\" = \"Publications x Time\"\n  )\n)\n\n\n\n\n\nEstimate\n\n\n95% CI\n\n\np\n\n\nIntercept\n\n\n54238.084\n\n\n[51870.023, 56606.145]\n\n\n0.000\n\n\nNumber of publications\n\n\n104.724\n\n\n[−92.273, 301.720]\n\n\n0.292\n\n\nTime since PhD\n\n\n964.170\n\n\n[284.218, 1644.122]\n\n\n0.006\n\n\nPublications x Time\n\n\n15.066\n\n\n[−19.506, 49.637]\n\n\n0.387\n\n\n\n\n\n\n",
      "last_modified": "2021-09-19T11:48:04-07:00"
    },
    {
      "path": "rcode3.html",
      "title": "R Codes (Week 3)",
      "author": [],
      "contents": "\n\nContents\nLoad Packages and Import Data\nImport Data\n\nRun the Random Intercept Model\nModel equations\nRunning the model in R\nShowing the variations across (a subset of) schools\nSimulating data based on the random intercept model\nPlotting the random effects (i.e., \\(u_{0j}\\))\nIntraclass correlations\n\nAdding Lv-2 Predictors\nModel Equation\nProportion of variance predicted\nComparing to OLS regression\n\n\n\n\\[\n\\newcommand{\\bv}[1]{\\boldsymbol{\\mathbf{#1}}}\n\\]\n\nClick here to download the Rmd file: week3-random-intercept-model.Rmd\nLoad Packages and Import Data\nYou can use the message=FALSE option to suppress the package loading messages\n\n\n# To install a package, run the following ONCE (and only once on your computer)\n# install.packages(\"psych\")  \nlibrary(here)  # makes reading data more consistent\nlibrary(tidyverse)  # for data manipulation and plotting\nlibrary(haven)  # for importing SPSS/SAS/Stata data\nlibrary(lme4)  # for multilevel analysis\nlibrary(lattice)  # for dotplot (working with lme4)\nlibrary(sjPlot)  # for plotting effects\nlibrary(MuMIn)  # for computing r-squared\nlibrary(r2mlm)  # for computing r-squared\nlibrary(broom.mixed)  # for summarizing results\nlibrary(modelsummary)  # for making tables\ntheme_set(theme_bw())  # Theme; just my personal preference\n\n\n\nIn R, there are many packages for multilevel modeling, two of the most common ones are the lme4 package and the nlme package. In this note I will show how to run different basic multilevel models using the lme4 package, which is newer. However, some of the models, like unstructured covariance structure, will need the nlme package or other packages (like the brms and the rstanarm packages with Bayesian estimation).\nImport Data\nFirst, download the data from https://github.com/marklhc/marklai-pages/raw/master/data_files/hsball.sav. We’ll import the data in .sav format using the read_sav() function from the haven package.\n\n\n# Read in the data (pay attention to the directory)\nhsball <- read_sav(here(\"data_files\", \"hsball.sav\"))\nhsball  # print the data\n\n\n># # A tibble: 7,185 × 11\n>#    id    minority female    ses mathach  size sector pracad disclim\n>#    <chr>    <dbl>  <dbl>  <dbl>   <dbl> <dbl>  <dbl>  <dbl>   <dbl>\n>#  1 1224         0      1 -1.53    5.88    842      0   0.35    1.60\n>#  2 1224         0      1 -0.588  19.7     842      0   0.35    1.60\n>#  3 1224         0      0 -0.528  20.3     842      0   0.35    1.60\n>#  4 1224         0      0 -0.668   8.78    842      0   0.35    1.60\n>#  5 1224         0      0 -0.158  17.9     842      0   0.35    1.60\n>#  6 1224         0      0  0.022   4.58    842      0   0.35    1.60\n>#  7 1224         0      1 -0.618  -2.83    842      0   0.35    1.60\n>#  8 1224         0      0 -0.998   0.523   842      0   0.35    1.60\n>#  9 1224         0      1 -0.888   1.53    842      0   0.35    1.60\n># 10 1224         0      0 -0.458  21.5     842      0   0.35    1.60\n># # … with 7,175 more rows, and 2 more variables: himinty <dbl>,\n># #   meanses <dbl>\n\nRun the Random Intercept Model\nModel equations\nLv-1: \\[\\text{mathach}_{ij} = \\beta_{0j} + e_{ij}\\] where \\(\\beta_{0j}\\) is the population mean math achievement of the \\(j\\)th school, and \\(e_{ij}\\) is the level-1 random error term for the \\(i\\)th individual of the \\(j\\)th school.\nLv-2: \\[\\beta_{0j} = \\gamma_{00} + u_{0j}\\] where \\(\\gamma_{00}\\) is the grand mean, and \\(u_{0j}\\) is the deviation of the mean of the \\(j\\)th school from the grand mean.\nRunning the model in R\nThe lme4 package require input in the format of\noutcome ~ fixed + (random | cluster ID)\nFor our data, the combined equation is \\[\\text{mathach}_{ij} = \\gamma_{00} + u_{0j} + e_{ij}, \\] which we can explicitly write \\[\\color{red}{\\text{mathach}}_{ij} = \\color{green}{\\gamma_{00} (1)} \n                                     + \\color{blue}{u_{0j} (1)}\n                                     + e_{ij}. \\] With that, we can see\noutcome = mathach,\nfixed = 1,\nrandom = 1, and\ncluster ID = id.\nThus the following syntax:\n\n\n# outcome = mathach\n# fixed = gamma_{00} * 1\n# random = u_{0j} * 1, with j indexing school id\nran_int <- lmer(mathach ~ 1 + (1 | id), data = hsball)\n# Summarize results\nsummary(ran_int)\n\n\n># Linear mixed model fit by REML ['lmerMod']\n># Formula: mathach ~ 1 + (1 | id)\n>#    Data: hsball\n># \n># REML criterion at convergence: 47116.8\n># \n># Scaled residuals: \n>#     Min      1Q  Median      3Q     Max \n># -3.0631 -0.7539  0.0267  0.7606  2.7426 \n># \n># Random effects:\n>#  Groups   Name        Variance Std.Dev.\n>#  id       (Intercept)  8.614   2.935   \n>#  Residual             39.148   6.257   \n># Number of obs: 7185, groups:  id, 160\n># \n># Fixed effects:\n>#             Estimate Std. Error t value\n># (Intercept)  12.6370     0.2444   51.71\n\nShowing the variations across (a subset of) schools\n\n\n# Randomly select 10 school ids\nrandom_ids <- sample(unique(hsball$id), size = 10)\n(p_subset <- hsball %>%\n    filter(id %in% random_ids) %>%  # select only 10 schools\n    ggplot(aes(x = id, y = mathach)) +\n    geom_jitter(height = 0, width = 0.1, alpha = 0.3) +\n    # Add school means\n    stat_summary(\n      fun = \"mean\",\n      geom = \"point\",\n      col = \"red\",\n      shape = 17,\n      # use triangles\n      size = 4\n    )  # make them larger\n)\n\n\n\n\nSimulating data based on the random intercept model\n\\[Y_{ij} = \\gamma_{00} + u_{0j} + e_{ij},\\]\n\n\ngamma00 <- 12.6370\ntau0 <- 2.935\nsigma <- 6.257\nnum_students <- nrow(hsball)\nnum_schools <- length(unique(hsball$id))\n# Simulate with only gamma00 (i.e., tau0 = 0 and sigma = 0)\nsimulated_data1 <- tibble(\n  id = hsball$id, \n  mathach = gamma00\n)\n# Show data with no variation\n# The `%+%` operator is use to substitute with a different data set\np_subset %+%\n  (simulated_data1 %>%\n     filter(id %in% random_ids))\n\n\n\n# Simulate with gamma00 + e_ij (i.e., tau0 = 0)\nsimulated_data2 <- tibble(\n  id = hsball$id, \n  mathach = gamma00 + rnorm(num_students, sd = sigma)\n)\n# Show data with no school-level variation\np_subset %+%\n  (simulated_data2 %>%\n     filter(id %in% random_ids))\n\n\n\n# Simulate with gamma00 + u_0j + e_ij\n# First, obtain group indices that starts from 1 to 160\ngroup_idx <- group_by(hsball, id) %>% group_indices()\n# Then simulate 160 u0j\nu0j <- rnorm(num_schools, sd = tau0)\nsimulated_data3 <- tibble(\n  id = hsball$id, \n  mathach = gamma00 + \n    u0j[group_idx] +  # expand the u0j's from 160 to 7185\n    rnorm(num_students, sd = sigma)\n)\n# Show data with both school and student variations\np_subset %+%\n  (simulated_data3 %>%\n     filter(id %in% random_ids))\n\n\n\n\nThe handy simulate() function can also be used to simulate the data\n\n\nsimulated_math <- simulate(ran_int, nsim = 1)\nsimulated_data4 <- tibble(\n  id = hsball$id, \n  mathach = simulated_math$sim_1\n)\np_subset %+%\n  (simulated_data4 %>%\n     filter(id %in% random_ids))\n\n\n\nPlotting the random effects (i.e., \\(u_{0j}\\))\nYou can easily plot the estimated school means (also called BLUP, best linear unbiased predictor, or the empirical Bayes (EB) estimates, which are different from the mean of the sample observations for a particular school) using the lattice package:\n\n\ndotplot(ranef(ran_int, condVar = TRUE))\n\n\n># $id\n\n\nHere’s a plot showing the sample schools means (with no borrowing of information) vs. the EB means (borrowing information).\n\n\n# Compute raw school means and EB means\nhsball %>% \n  group_by(id) %>% \n  # Raw means\n  summarise(mathach_raw_means = mean(mathach)) %>% \n  arrange(mathach_raw_means) %>%  # sort by the means\n  # EB means (the \".\" means using the current data)\n  mutate(mathach_eb_means = predict(ran_int, .), \n         index = row_number()) %>%  # add row number as index for plotting\n  ggplot(aes(x = index, y = mathach_raw_means)) + \n  geom_point(aes(col = \"Raw\")) + \n  # Add EB means\n  geom_point(aes(y = mathach_eb_means, col = \"EB\"), shape = 1) + \n  geom_segment(aes(x = index, xend = index, \n                   y = mathach_eb_means, yend = mathach_raw_means, \n                   col = \"EB\")) + \n  labs(y = \"School mean achievement\", col = \"\")\n\n\n\n\nIntraclass correlations\n\n\nvariance_components <- as.data.frame(VarCorr(ran_int))\nbetween_var <- variance_components$vcov[1]\nwithin_var <- variance_components$vcov[2]\n(icc <- between_var / (between_var + within_var))\n\n\n># [1] 0.1803518\n\n# 95% confidence intervals (require installing the bootmlm package)\n# if (!require(\"devtools\")) {\n#   install.packages(\"devtools\")\n# }\n# devtools::install_github(\"marklhc/bootmlm\")\nbootmlm:::prof_ci_icc(ran_int)\n\n\n>#     2.5 %    97.5 % \n># 0.1471784 0.2210131\n\nAdding Lv-2 Predictors\nWe have one predictor, meanses, in the fixed part.\n\n\nhsball %>% \n  ggplot(aes(x = meanses, y = mathach, col = id)) + \n  geom_point(alpha = 0.5, size = 0.5) + \n  guides(col = \"none\")\n\n\n\n\nModel Equation\nLv-1:\n\\[\\text{mathach}_{ij} = \\beta_{0j} + e_{ij}\\]\nLv-2:\n\\[\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{meanses}_j + u_{0j}\\] where \\(\\gamma_{00}\\) is the grand intercept, \\(\\gamma_{10}\\) is the regression coefficient of meanses that represents the expected difference in school mean achievement between two schools with one unit difference in meanses, and and \\(u_{0j}\\) is the deviation of the mean of the \\(j\\)th school from the grand mean.\n\n\nm_lv2 <- lmer(mathach ~ meanses + (1 | id), data = hsball)\nsummary(m_lv2)\n\n\n># Linear mixed model fit by REML ['lmerMod']\n># Formula: mathach ~ meanses + (1 | id)\n>#    Data: hsball\n># \n># REML criterion at convergence: 46961.3\n># \n># Scaled residuals: \n>#      Min       1Q   Median       3Q      Max \n># -3.13480 -0.75256  0.02409  0.76773  2.78501 \n># \n># Random effects:\n>#  Groups   Name        Variance Std.Dev.\n>#  id       (Intercept)  2.639   1.624   \n>#  Residual             39.157   6.258   \n># Number of obs: 7185, groups:  id, 160\n># \n># Fixed effects:\n>#             Estimate Std. Error t value\n># (Intercept)  12.6494     0.1493   84.74\n># meanses       5.8635     0.3615   16.22\n># \n># Correlation of Fixed Effects:\n>#         (Intr)\n># meanses -0.004\n\n# Likelihood-based confidence intervals for fixed effects\n# `parm = \"beta_\"` requests confidence intervals only for the fixed effects\nconfint(m_lv2, parm = \"beta_\")\n\n\n>#                 2.5 %    97.5 %\n># (Intercept) 12.356615 12.941707\n># meanses      5.155769  6.572415\n\nThe 95% confidence intervals (CIs) above showed the uncertainty associated with the estimates. Also, as the 95% CI for meanses does not contain zero, there is evidence for the positive association of SES and mathach at the school level.\n\n\nsjPlot::plot_model(m_lv2, type = \"pred\", terms = \"meanses\", \n                   show.data = TRUE, title = \"\", \n                   dot.size = 0.5) + \n  # Add the group means\n  stat_summary(data = hsball, aes(x = meanses, y = mathach), \n               fun = mean, geom = \"point\",\n               col = \"red\",\n               shape = 17,\n               # use triangles\n               size = 3, \n               alpha = 0.7)\n\n\n\n\nProportion of variance predicted\nWe will use the \\(R^2\\) statistic proposed by Nakagawa, Johnson & Schielzeth (2017) to obtain an \\(R^2\\) statistic. There are multiple versions of \\(R^2\\) in the literature, but personally I think this \\(R^2\\) avoids many of the problems in other variants and is most meaningful to interpret. Note that I only interpret the marginal \\(R^2\\).\n\n\n# Generally, you should use the marginal R^2 (R2m) for the variance predicted by\n# your predictors (`meanses` in this case).\nMuMIn::r.squaredGLMM(m_lv2)\n\n\n>#            R2m       R2c\n># [1,] 0.1233346 0.1786815\n\nAn alternative, more comprehensive approach is by Rights & Sterba (2019, Psychological Methods, https://doi.org/10.1037/met0000184), with the r2mlm package\n\n\nr2mlm::r2mlm(m_lv2)\n\n\n\n># $Decompositions\n>#                 total              within between          \n># fixed, within   0                  0      NA               \n># fixed, between  0.123334641367122  NA     0.690248683624938\n># slope variation 0                  0      NA               \n># mean variation  0.0553468169145697 NA     0.309751316375062\n># sigma2          0.821318541718308  1      NA               \n># \n># $R2s\n>#     total              within between          \n># f1  0                  0      NA               \n># f2  0.123334641367122  NA     0.690248683624938\n># v   0                  0      NA               \n># m   0.0553468169145697 NA     0.309751316375062\n># f   0.123334641367122  NA     NA               \n># fv  0.123334641367122  0      NA               \n># fvm 0.178681458281692  NA     NA\n\nNote the fixed, between number in the total column is the same as the one from MuMIn::r.squaredGLMM(). Including school means of SES in the model accounted for about 12% of the total variance of math achievement.\nComparing to OLS regression\nNotice that the standard error with regression is only half of that with MLM.\n\n\nm_lm <- lm(mathach ~ meanses, data = hsball)\nmsummary(list(\"MLM\" = m_lv2, \n              \"Linear regression\" = m_lm))\n\n\n\n\n\nMLM\n\n\nLinear regression\n\n\n(Intercept)\n\n\n12.649\n\n\n12.713\n\n\n\n\n(0.149)\n\n\n(0.076)\n\n\nmeanses\n\n\n5.864\n\n\n5.717\n\n\n\n\n(0.361)\n\n\n(0.184)\n\n\nsd__(Intercept)\n\n\n1.624\n\n\n\n\nsd__Observation\n\n\n6.258\n\n\n\n\nNum.Obs.\n\n\n\n\n7185\n\n\nR2\n\n\n\n\n0.118\n\n\nR2 Adj.\n\n\n\n\n0.118\n\n\nAIC\n\n\n46969.3\n\n\n47202.4\n\n\nBIC\n\n\n46996.8\n\n\n47223.0\n\n\nLog.Lik.\n\n\n−23480.642\n\n\n−23598.190\n\n\nF\n\n\n\n\n962.329\n\n\nREMLcrit\n\n\n46961.285\n\n\n\n\n\n\n\n\n",
      "last_modified": "2021-09-19T11:48:24-07:00"
    },
    {
      "path": "rcode4.html",
      "title": "R Codes (Week 4)",
      "author": [],
      "contents": "\n\nContents\nLoad Packages and Import Data\nImport Data\n\nBetween-Within Decomposition\nCluster-mean centering\nModel Equation\nRunning in R\n\nContextual Effect\nModel Equation\nRunning in R\n\nRandom-Coefficients Model\nExplore Variations in Slopes\nModel Equation\nRunning in R\nPlotting Random Slopes\nIncluding the effect of meanses\n\nEffect Size\n\nAnalyzing Cross-Level Interactions\nModel Equation\nRunning in R\nEffect Size\nSimple Slopes\nPlotting the Interactions\n\nSummaizing Analyses in a Table\nBonus: More “APA-like” Table\n\n\n\n\\[\n\\newcommand{\\bv}[1]{\\boldsymbol{\\mathbf{#1}}}\n\\]\n\nClick here to download the Rmd file: week4-level-1-predictor.Rmd\nLoad Packages and Import Data\n\n\n# To install a package, run the following ONCE (and only once on your computer)\n# install.packages(\"psych\")  \nlibrary(here)  # makes reading data more consistent\nlibrary(tidyverse)  # for data manipulation and plotting\nlibrary(haven)  # for importing SPSS/SAS/Stata data\nlibrary(lme4)  # for multilevel analysis\nlibrary(sjPlot)  # for plotting effects\nlibrary(MuMIn)  # for computing r-squared\nlibrary(broom.mixed)  # for summarizing results\nlibrary(modelsummary)  # for making tables\nlibrary(interactions)  # for plotting interactions\ntheme_set(theme_bw())  # Theme; just my personal preference\n\n\n\n\n\nmsummary_mixed <- function(models, coef_map = NULL, ...) {\n  if (is.null(coef_map)) {\n    if (!\"list\" %in% class(models)) {\n      models <- list(models)\n    }\n    for (model in models) {\n      coef_map <- union(coef_map, tidy(model)$term)\n    }\n    ranef_index <- grep(\"^(sd|cor)__\", x = coef_map)\n    coef_map <- c(coef_map[-ranef_index], coef_map[ranef_index])\n    names(coef_map) <- coef_map\n  } else {\n    ranef_index <- grep(\"^(sd|cor)__\", x = names(coef_map))\n  }\n  rows <- data.frame(term = c(\"Fixed Effects\", \"Random Effects\"))\n  rows <- cbind(rows, rbind(\n    rep(\"\", length(models)),\n    rep(\"\", length(models))\n  ))\n  length_fes <- length(coef_map) - length(ranef_index)\n  if (\"statistic\" %in% names(list(...)) && is.null(list(...)$statistic)) {\n    attr(rows, \"position\") <- c(1, (length_fes + 1))\n  } else {\n    attr(rows, \"position\") <- c(1, (length_fes + 1) * 2)\n  }\n  modelsummary::msummary(models, coef_map = coef_map, add_rows = rows, ...)\n}\n\n\n\nImport Data\n\n\n# Read in the data (pay attention to the directory)\nhsball <- read_sav(here(\"data_files\", \"hsball.sav\"))\n\n\n\nBetween-Within Decomposition\nCluster-mean centering\nTo separate the effects of a lv-1 predictor into different levels, one needs to first center the predictor on the cluster means:\n\n\nhsball <- hsball %>% \n  group_by(id) %>%   # operate within schools\n  mutate(ses_cm = mean(ses),   # create cluster means (the same as `meanses`)\n         ses_cmc = ses - ses_cm) %>%   # cluster-mean centered\n  ungroup()  # exit the \"editing within groups\" mode\n# The meanses variable already exists in the original data, but it's slightly\n# different from computing it by hand\nhsball %>% \n  select(id, ses, meanses, ses_cm, ses_cmc)\n\n\n># # A tibble: 7,185 × 5\n>#    id       ses meanses ses_cm ses_cmc\n>#    <chr>  <dbl>   <dbl>  <dbl>   <dbl>\n>#  1 1224  -1.53   -0.428 -0.434 -1.09  \n>#  2 1224  -0.588  -0.428 -0.434 -0.154 \n>#  3 1224  -0.528  -0.428 -0.434 -0.0936\n>#  4 1224  -0.668  -0.428 -0.434 -0.234 \n>#  5 1224  -0.158  -0.428 -0.434  0.276 \n>#  6 1224   0.022  -0.428 -0.434  0.456 \n>#  7 1224  -0.618  -0.428 -0.434 -0.184 \n>#  8 1224  -0.998  -0.428 -0.434 -0.564 \n>#  9 1224  -0.888  -0.428 -0.434 -0.454 \n># 10 1224  -0.458  -0.428 -0.434 -0.0236\n># # … with 7,175 more rows\n\nModel Equation\nLv-1:\n\\[\\text{mathach}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{ses_cmc}_{ij} + e_{ij}\\]\nLv-2:\n\\[\n\\begin{aligned}\n  \\beta_{0j} & = \\gamma_{00} + \\gamma_{01} \\text{meanses}_j + u_{0j}  \\\\\n  \\beta_{1j} & = \\gamma_{10}\n\\end{aligned}\n\\] where\n\\(\\gamma_{10}\\) = regression coefficient of student-level ses representing the expected difference in student achievement between two students in the same school with one unit difference in ses,\n\\(\\gamma_{01}\\) = between-school effect, which is the expected difference in mean achievement between two schools with one unit difference in meanses.\nRunning in R\nWe can specify the model as:\n\n\nm_bw <- lmer(mathach ~ meanses + ses_cmc + (1 | id), data = hsball)\n\n\n\nYou can summarize the results using\n\n\nsummary(m_bw)\n\n\n># Linear mixed model fit by REML ['lmerMod']\n># Formula: mathach ~ meanses + ses_cmc + (1 | id)\n>#    Data: hsball\n># \n># REML criterion at convergence: 46568.6\n># \n># Scaled residuals: \n>#     Min      1Q  Median      3Q     Max \n># -3.1666 -0.7253  0.0174  0.7557  2.9453 \n># \n># Random effects:\n>#  Groups   Name        Variance Std.Dev.\n>#  id       (Intercept)  2.692   1.641   \n>#  Residual             37.019   6.084   \n># Number of obs: 7185, groups:  id, 160\n># \n># Fixed effects:\n>#             Estimate Std. Error t value\n># (Intercept)  12.6481     0.1494   84.68\n># meanses       5.8662     0.3617   16.22\n># ses_cmc       2.1912     0.1087   20.16\n># \n># Correlation of Fixed Effects:\n>#         (Intr) meanss\n># meanses -0.004       \n># ses_cmc  0.000  0.000\n\nContextual Effect\nWith a level-1 predictor like ses, which has both student-level and school-level variance, we can include both the level-1 variable and the school mean variable as predictors. When the level-1 predictor is present, the coefficient for the group mean variable becomes the contextual effect.\nModel Equation\nLv-1:\n\\[\\text{mathach}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{ses}_{ij} + e_{ij}\\]\nLv-2:\n\\[\n\\begin{aligned}\n  \\beta_{0j} & = \\gamma_{00} + \\gamma_{01} \\text{meanses}_j + u_{0j}  \\\\\n  \\beta_{1j} & = \\gamma_{10}\n\\end{aligned}\n\\] where\n\\(\\gamma_{10}\\) = regression coefficient of student-level ses representing the expected difference in student achievement between two students in the same school with one unit difference in ses,\n\\(\\gamma_{01}\\) = contextual effect, which is the expected difference in student achievement between two students with the same ses but from two schools with one unit difference in meanses.\nRunning in R\nWe can specify the model as:\n\n\ncontextual <- lmer(mathach ~ meanses + ses + (1 | id), data = hsball)\n\n\n\nYou can summarize the results using\n\n\nsummary(contextual)\n\n\n># Linear mixed model fit by REML ['lmerMod']\n># Formula: mathach ~ meanses + ses + (1 | id)\n>#    Data: hsball\n># \n># REML criterion at convergence: 46568.6\n># \n># Scaled residuals: \n>#     Min      1Q  Median      3Q     Max \n># -3.1666 -0.7253  0.0174  0.7557  2.9453 \n># \n># Random effects:\n>#  Groups   Name        Variance Std.Dev.\n>#  id       (Intercept)  2.692   1.641   \n>#  Residual             37.019   6.084   \n># Number of obs: 7185, groups:  id, 160\n># \n># Fixed effects:\n>#             Estimate Std. Error t value\n># (Intercept)  12.6613     0.1494  84.763\n># meanses       3.6750     0.3777   9.731\n># ses           2.1912     0.1087  20.164\n># \n># Correlation of Fixed Effects:\n>#         (Intr) meanss\n># meanses -0.005       \n># ses      0.004 -0.288\n\nIf you compare the REML criterion at convergence number you can see this is the same as the between-within model. The estimated contextual effect is the coefficient of meanses minus the coefficient of ses_cmc, which is the same as what you will get in the contextual effect model.\nRandom-Coefficients Model\nExplore Variations in Slopes\n\n\nhsball %>%\n  # randomly sample 16 schools\n  filter(id %in% sample(unique(id), 16)) %>%\n  ggplot(aes(x = ses, y = mathach)) +\n  geom_point(size = 0.5) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~id)\n\n\n\n\nModel Equation\nLv-1:\n\\[\\text{mathach}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{ses_cmc}_{ij} + e_{ij}\\]\nLv-2:\n\\[\n\\begin{aligned}\n  \\beta_{0j} & = \\gamma_{00} + \\gamma_{01} \\text{meanses}_j + u_{0j}  \\\\\n  \\beta_{1j} & = \\gamma_{10} + u_{1j}  \n\\end{aligned}\n\\] The additional term is \\(u_{1j}\\), which represents the deviation of the slope of school \\(j\\) from the average slope (i.e., \\(\\gamma_{10}\\)).\nRunning in R\nWe have to put ses in the random part:\n\n\nran_slp <- lmer(mathach ~ meanses + ses_cmc + (ses_cmc | id), data = hsball)\n\n\n\nYou can summarize the results using\n\n\nsummary(ran_slp)\n\n\n># Linear mixed model fit by REML ['lmerMod']\n># Formula: mathach ~ meanses + ses_cmc + (ses_cmc | id)\n>#    Data: hsball\n># \n># REML criterion at convergence: 46557.7\n># \n># Scaled residuals: \n>#     Min      1Q  Median      3Q     Max \n># -3.1768 -0.7269  0.0160  0.7560  2.9406 \n># \n># Random effects:\n>#  Groups   Name        Variance Std.Dev. Corr \n>#  id       (Intercept)  2.6931  1.6411        \n>#           ses_cmc      0.6858  0.8281   -0.19\n>#  Residual             36.7132  6.0591        \n># Number of obs: 7185, groups:  id, 160\n># \n># Fixed effects:\n>#             Estimate Std. Error t value\n># (Intercept)  12.6454     0.1492   84.74\n># meanses       5.8963     0.3600   16.38\n># ses_cmc       2.1913     0.1280   17.11\n># \n># Correlation of Fixed Effects:\n>#         (Intr) meanss\n># meanses -0.004       \n># ses_cmc -0.088  0.004\n\nPlotting Random Slopes\n\n\naugment(ran_slp, data = hsball) %>% # augmented data (adding EB estimates)\n  ggplot(aes(x = ses, y = .fitted, color = factor(id))) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.5) +\n  labs(y = \"Predicted mathach\") +\n  guides(color = \"none\")\n\n\n\n\nIncluding the effect of meanses\n\n\n# augmented data (adding EB estimates)\naugment(ran_slp, data = hsball) %>%\n  ggplot(aes(x = ses, y = mathach, color = factor(id))) +\n  # Add points\n  geom_point(size = 0.2, alpha = 0.2) +\n  # Add within-cluster lines\n  geom_smooth(aes(y = .fitted),\n              method = \"lm\", se = FALSE, size = 0.5) +\n  # Add group means\n  stat_summary(aes(x = meanses, y = .fitted,\n                   fill = factor(id)),\n               color = \"red\",  # add border\n               fun = mean, \n               geom = \"point\",\n               shape = 24,\n               # use triangles\n               size = 2.5) +\n  # Add between coefficient\n  geom_smooth(aes(x = meanses, y = .fitted),\n              method = \"lm\", se = FALSE,\n              color = \"black\") +\n  labs(y = \"Math Achievement\") +\n  # Suppress legend\n  guides(color = \"none\", fill = \"none\")\n\n\n\n\nOr on separate graphs:\n\n\n# Create a common base graph\npbase <- augment(ran_slp, data = hsball) %>%\n  ggplot(aes(x = ses, y = mathach, color = factor(id))) +\n  # Add points\n  geom_point(size = 0.2, alpha = 0.2) +\n  labs(y = \"Math Achievement\") +\n  # Suppress legend\n  guides(color = \"none\")\n# Lv-1 effect\np1 <- pbase + \n  # Add within-cluster lines\n  geom_smooth(aes(y = .fitted),\n              method = \"lm\", se = FALSE, size = 0.5)\n# Lv-2 effect\np2 <- pbase +\n  # Add group means\n  stat_summary(aes(x = meanses, y = .fitted),\n               fun = mean,\n               geom = \"point\",\n               shape = 17,\n               # use triangles\n               size = 2.5) +\n  # Add between coefficient\n  geom_smooth(aes(x = meanses, y = .fitted),\n              method = \"lm\", se = FALSE,\n              color = \"black\")\n# Put the two graphs together (need the gridExtra package)\ngridExtra::grid.arrange(p1, p2, ncol = 2)  # in two columns\n\n\n\n\nEffect Size\nAs discussed in the previous week, we can obtain the effect size (\\(R^2\\)) for the model:\n\n\n(r2_ran_slp <- r.squaredGLMM(ran_slp))\n\n\n>#            R2m       R2c\n># [1,] 0.1684161 0.2310862\n\nAnalyzing Cross-Level Interactions\nsector is added to the level-2 intercept and slope equations\nModel Equation\nLv-1:\n\\[\\text{mathach}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{ses_cmc}_{ij} + e_{ij}\\]\nLv-2:\n\\[\n\\begin{aligned}\n  \\beta_{0j} & = \\gamma_{00} + \\gamma_{01} \\text{meanses}_j + \n                 \\gamma_{02} \\text{sector}_j + u_{0j}  \\\\\n  \\beta_{1j} & = \\gamma_{10} + \\gamma_{11} \\text{sector}_j + u_{1j}  \n\\end{aligned}\n\\] where - \\(\\gamma_{02}\\) = regression coefficient of school-level sector variable representing the expected difference in achievement between two students with the same SES level and from two schools with the same school-level SES, but one is a catholic school and the other a private school. - \\(\\gamma_{11}\\) = cross-level interaction coefficient of the expected slope difference between a catholic and a private school with the same school-level SES.\nRunning in R\nWe have to put the sector * ses interaction to the fixed part:\n\n\n# The first step is optional, but let's recode sector into a factor variable\nhsball <- hsball %>% \n  mutate(sector = factor(sector, levels = c(0, 1), \n                         labels = c(\"Public\", \"Catholic\")))\ncrlv_int <- lmer(mathach ~ meanses + sector * ses_cmc + (ses_cmc | id), \n                 data = hsball)\n\n\n\nYou can summarize the results using\n\n\nsummary(crlv_int)\n\n\n># Linear mixed model fit by REML ['lmerMod']\n># Formula: mathach ~ meanses + sector * ses_cmc + (ses_cmc | id)\n>#    Data: hsball\n># \n># REML criterion at convergence: 46514.5\n># \n># Scaled residuals: \n>#      Min       1Q   Median       3Q      Max \n># -3.11560 -0.72905  0.01592  0.75280  3.01543 \n># \n># Random effects:\n>#  Groups   Name        Variance Std.Dev. Corr\n>#  id       (Intercept)  2.3807  1.5430       \n>#           ses_cmc      0.2716  0.5212   0.24\n>#  Residual             36.7077  6.0587       \n># Number of obs: 7185, groups:  id, 160\n># \n># Fixed effects:\n>#                        Estimate Std. Error t value\n># (Intercept)             12.0846     0.1987   60.81\n># meanses                  5.2450     0.3682   14.24\n># sectorCatholic           1.2523     0.3062    4.09\n># ses_cmc                  2.7877     0.1559   17.89\n># sectorCatholic:ses_cmc  -1.3478     0.2348   -5.74\n># \n># Correlation of Fixed Effects:\n>#             (Intr) meanss sctrCt ss_cmc\n># meanses      0.245                     \n># sectorCthlc -0.697 -0.355              \n># ses_cmc      0.071 -0.002 -0.046       \n># sctrCthlc:_ -0.048 -0.001  0.071 -0.664\n\nEffect Size\nBy comparing the \\(R^2\\) of this model (with interaction) and the previous model (without the interaction), we can obtain the proportion of variance accounted for by the cross-level interaction:\n\n\n(r2_crlv_int <- r.squaredGLMM(crlv_int))\n\n\n>#            R2m       R2c\n># [1,] 0.1759122 0.2284431\n\n# Change\nr2_crlv_int - r2_ran_slp\n\n\n>#              R2m          R2c\n># [1,] 0.007496143 -0.002643047\n\nTherefore, the cross-level interaction between sector and SES accounted for an effect size of \\(\\Delta R^2\\) = 0.7%. This is usually considered a small effect size (i.e., < 1%), but interpreting the magnitude of an effect size needs to take into account the area of research, the importance of the outcome, and the effect sizes when using other predictors to predict the same outcome.\nSimple Slopes\nWith a cross-level interaction, the slope between ses_cmc and mathach depends on a moderator, sector. To find out what the model suggests to be the slope at a particular level of sector, which is also called a simple slope in the literature, you just need to look at the equation for \\(\\beta_{1j}\\), which shows that the predicted slope is \\[\\hat \\beta_{1} = \\hat \\gamma_{10} + \\hat \\gamma_{11} \\text{sector}.\\] So when sector = 0 (public school), the simple slope is \\[\\hat \\beta_{1} = \\hat \\gamma_{10} + \\hat \\gamma_{11} (0),\\] or 2.7877417. When sector = 1 (private school), the simple slope is \\[\\hat \\beta_{1} = \\hat \\gamma_{10} + \\hat \\gamma_{11} (1),\\] or 2.7877417 + (-1.3477638)(1) = 1.4399779.\nPlotting the Interactions\n\n\ncrlv_int %>%\n  augment(data = hsball) %>%\n  ggplot(aes(\n    x = ses, y = .fitted, group = factor(id),\n    color = factor(sector)  # use `sector` for coloring lines\n  )) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.5) +\n  labs(y = \"Predicted mathach\", color = \"sector\")\n\n\n\n\nYou can also use the interactions::interact_plot() function for just the fixed effects:\n\n\ninteract_plot(crlv_int, \n              pred = \"ses_cmc\",\n              modx = \"sector\",\n              modx.labels = c(\"Public\", \"Catholic\"), \n              plot.points = TRUE, \n              point.size = 0.5, \n              point.alpha = 0.2, \n              facet.modx = TRUE,  # use this to make two panels\n              x.label = \"SES (cluster-mean centered)\", \n              y.label = \"Math achievement\")\n\n\n\n\nSummaizing Analyses in a Table\nYou can again use the msummary() function from the modelsummary package to quickly summarize the results in a table. However, there is an issue as the order of the coefficients are messed up, as shown below.\n\n\n# Basic table from `modelsummary` (ordering messed up)\nmsummary(list(\n  \"Between-within\" = m_bw,\n  \"Contextual\" = contextual,\n  \"Random slope\" = ran_slp,\n  \"Cross-level interaction\" = crlv_int\n))\n\n\n\n\n\nBetween-within\n\n\nContextual\n\n\nRandom slope\n\n\nCross-level interaction\n\n\n(Intercept)\n\n\n12.648\n\n\n12.661\n\n\n12.645\n\n\n12.085\n\n\n\n\n(0.149)\n\n\n(0.149)\n\n\n(0.149)\n\n\n(0.199)\n\n\nmeanses\n\n\n5.866\n\n\n3.675\n\n\n5.896\n\n\n5.245\n\n\n\n\n(0.362)\n\n\n(0.378)\n\n\n(0.360)\n\n\n(0.368)\n\n\nses_cmc\n\n\n2.191\n\n\n\n\n2.191\n\n\n2.788\n\n\n\n\n(0.109)\n\n\n\n\n(0.128)\n\n\n(0.156)\n\n\nsd__(Intercept)\n\n\n1.641\n\n\n1.641\n\n\n1.641\n\n\n1.543\n\n\nsd__Observation\n\n\n6.084\n\n\n6.084\n\n\n6.059\n\n\n6.059\n\n\nses\n\n\n\n\n2.191\n\n\n\n\n\n\n\n\n\n\n(0.109)\n\n\n\n\n\n\ncor__(Intercept).ses_cmc\n\n\n\n\n\n\n−0.195\n\n\n0.244\n\n\nsd__ses_cmc\n\n\n\n\n\n\n0.828\n\n\n0.521\n\n\nsectorCatholic\n\n\n\n\n\n\n\n\n1.252\n\n\n\n\n\n\n\n\n\n\n(0.306)\n\n\nsectorCatholic × ses_cmc\n\n\n\n\n\n\n\n\n−1.348\n\n\n\n\n\n\n\n\n\n\n(0.235)\n\n\nAIC\n\n\n46578.6\n\n\n46578.6\n\n\n46571.7\n\n\n46532.5\n\n\nBIC\n\n\n46613.0\n\n\n46613.0\n\n\n46619.8\n\n\n46594.4\n\n\nLog.Lik.\n\n\n−23284.289\n\n\n−23284.290\n\n\n−23278.825\n\n\n−23257.264\n\n\nREMLcrit\n\n\n46568.577\n\n\n46568.580\n\n\n46557.651\n\n\n46514.529\n\n\nI’ve made a function msummary_mixed(), defined in the beginning of this page, as a hack to get a nicer table. Basically, you can just replace msummary() with msummary_mixed(), with everything else being the same:\n\n\n# Basic table using `msummary_mixed`\nmsummary_mixed(list(\n  \"Between-within\" = m_bw,\n  \"Contextual\" = contextual,\n  \"Random slope\" = ran_slp,\n  \"Cross-level interaction\" = crlv_int\n))\n\n\n\n\n\nBetween-within\n\n\nContextual\n\n\nRandom slope\n\n\nCross-level interaction\n\n\nFixed Effects\n\n\n\n\n\n\n\n\n\n\n(Intercept)\n\n\n12.648\n\n\n12.661\n\n\n12.645\n\n\n12.085\n\n\n\n\n(0.149)\n\n\n(0.149)\n\n\n(0.149)\n\n\n(0.199)\n\n\nmeanses\n\n\n5.866\n\n\n3.675\n\n\n5.896\n\n\n5.245\n\n\n\n\n(0.362)\n\n\n(0.378)\n\n\n(0.360)\n\n\n(0.368)\n\n\nses_cmc\n\n\n2.191\n\n\n\n\n2.191\n\n\n2.788\n\n\n\n\n(0.109)\n\n\n\n\n(0.128)\n\n\n(0.156)\n\n\nses\n\n\n\n\n2.191\n\n\n\n\n\n\n\n\n\n\n(0.109)\n\n\n\n\n\n\nsectorCatholic\n\n\n\n\n\n\n\n\n1.252\n\n\n\n\n\n\n\n\n\n\n(0.306)\n\n\nsectorCatholic:ses_cmc\n\n\n\n\n\n\n\n\n−1.348\n\n\n\n\n\n\n\n\n\n\n(0.235)\n\n\nRandom Effects\n\n\n\n\n\n\n\n\n\n\nsd__(Intercept)\n\n\n1.641\n\n\n1.641\n\n\n1.641\n\n\n1.543\n\n\nsd__Observation\n\n\n6.084\n\n\n6.084\n\n\n6.059\n\n\n6.059\n\n\ncor__(Intercept).ses_cmc\n\n\n\n\n\n\n−0.195\n\n\n0.244\n\n\nsd__ses_cmc\n\n\n\n\n\n\n0.828\n\n\n0.521\n\n\nAIC\n\n\n46578.6\n\n\n46578.6\n\n\n46571.7\n\n\n46532.5\n\n\nBIC\n\n\n46613.0\n\n\n46613.0\n\n\n46619.8\n\n\n46594.4\n\n\nLog.Lik.\n\n\n−23284.289\n\n\n−23284.290\n\n\n−23278.825\n\n\n−23257.264\n\n\nREMLcrit\n\n\n46568.577\n\n\n46568.580\n\n\n46557.651\n\n\n46514.529\n\n\nBonus: More “APA-like” Table\nMake it look like the table at https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables#regression. You can see a recent recommendation in this paper published in the Journal of Memory and Language\n\n\n# Step 1. Create a map for the predictors and the terms in the model.\n# Need to use '\\\\( \\\\)' to show math.\n# Rename and reorder the rows. Need to use '\\\\( \\\\)' to\n# show math. If this does not work for you, don't worry about it.\ncm <- c(\"(Intercept)\" = \"Intercept\",\n        \"meanses\" = \"school mean SES\",\n        \"ses_cmc\" = \"SES (cluster-mean centered)\",\n        \"sectorCatholic\" = \"Catholic school\",\n        \"sectorCatholic:ses_cmc\" = \"Catholic school x SES (cmc)\",\n        \"sd__(Intercept)\" = \"\\\\(\\\\tau_0\\\\)\",\n        \"sd__ses_cmc\" = \"\\\\(\\\\tau_1\\\\) (SES)\",\n        \"sd__Observation\" = \"\\\\(\\\\sigma\\\\)\")\n# Step 2. Create a list of models to have one column for coef, \n# one for SE, one for CI, and one for p\nmodels <- list(\n  \"Estimate\" = crlv_int,\n  \"SE\" = crlv_int,\n  \"95% CI\" = crlv_int\n)\n# Step 3. Add rows to say fixed and random effects\n# (Need same number of columns as the table)\nnew_rows <- data.frame(\n  term = c(\"Fixed effects\", \"Random effects\"),\n  est = c(\"\", \"\"),\n  se = c(\"\", \"\"),\n  ci = c(\"\", \"\")\n)\n# Specify which rows to add\nattr(new_rows, \"position\") <- c(1, 7)\n# Step 4. Render the table\nmsummary(models,\n         estimate = c(\"estimate\", \"std.error\",\n                      \"[{conf.low}, {conf.high}]\"),\n         statistic = NULL,  # suppress the extra rows for SEs\n         coef_map = cm,\n         add_rows = new_rows)\n\n\n\n\n\nEstimate\n\n\nSE\n\n\n95% CI\n\n\nFixed effects\n\n\n\n\n\n\n\n\nIntercept\n\n\n12.085\n\n\n0.199\n\n\n[11.695, 12.474]\n\n\nschool mean SES\n\n\n5.245\n\n\n0.368\n\n\n[4.523, 5.967]\n\n\nSES (cluster-mean centered)\n\n\n2.788\n\n\n0.156\n\n\n[2.482, 3.093]\n\n\nCatholic school\n\n\n1.252\n\n\n0.306\n\n\n[0.652, 1.852]\n\n\nCatholic school x SES (cmc)\n\n\n−1.348\n\n\n0.235\n\n\n[−1.808, −0.888]\n\n\nRandom effects\n\n\n\n\n\n\n\n\n\\(\\tau_0\\)\n\n\n1.543\n\n\n\n\n\n\n\\(\\tau_1\\) (SES)\n\n\n0.521\n\n\n\n\n\n\n\\(\\sigma\\)\n\n\n6.059\n\n\n\n\n\n\nAIC\n\n\n46532.5\n\n\n46532.5\n\n\n46532.5\n\n\nBIC\n\n\n46594.4\n\n\n46594.4\n\n\n46594.4\n\n\nLog.Lik.\n\n\n−23257.264\n\n\n−23257.264\n\n\n−23257.264\n\n\nREMLcrit\n\n\n46514.529\n\n\n46514.529\n\n\n46514.529\n\n\n\n\n\n\n",
      "last_modified": "2021-09-19T11:48:54-07:00"
    },
    {
      "path": "rcode5.html",
      "title": "R Codes (Week 5)",
      "author": [],
      "contents": "\n\nContents\nLoad Packages and Import Data\nLog-Likelihood Function \\(\\ell\\)\nExample: \\(\\ell(\\gamma_{01})\\)\n\nML vs REML\nTesting Fixed Effects\nSmall-Sample Correction: Kenward-Roger Approximation of Degrees of Freedom\n\nTesting Random Slopes\nBootstrap\nFixed Effects\nVariance Components\n\\(R^2\\)\n\n\n\n\\[\n\\newcommand{\\bv}[1]{\\boldsymbol{\\mathbf{#1}}}\n\\]\n\nClick here to download the Rmd file: week5-estimation-testing.Rmd\nLoad Packages and Import Data\n\n\n# To install a package, run the following ONCE (and only once on your computer)\n# install.packages(\"psych\")  \nlibrary(here)  # makes reading data more consistent\nlibrary(tidyverse)  # for data manipulation and plotting\nlibrary(haven)  # for importing SPSS/SAS/Stata data\nlibrary(lme4)  # for multilevel analysis\nlibrary(MuMIn)  # for computing r-squared\nlibrary(broom.mixed)  # for summarizing results\nlibrary(modelsummary)  # for making tables\ntheme_set(theme_bw())  # Theme; just my personal preference\n\n\n\nIn addition, because the table obtained from modelsummary::msummary() mixed up the ordering of the fixed- and the random-effect coefficients, I provided a quick fix to that by defining the msummary_mixed() function, which you need to load every time if you want to use it:\n\n\nmsummary_mixed <- function(models, coef_map = NULL, ...) {\n  if (is.null(coef_map)) {\n    if (!\"list\" %in% class(models)) {\n      models <- list(models)\n    }\n    for (model in models) {\n      coef_map <- union(coef_map, tidy(model)$term)\n    }\n    ranef_index <- grep(\"^(sd|cor)__\", x = coef_map)\n    coef_map <- c(coef_map[-ranef_index], coef_map[ranef_index])\n    names(coef_map) <- coef_map\n  } else {\n    ranef_index <- grep(\"^(sd|cor)__\", x = names(coef_map))\n  }\n  rows <- data.frame(term = c(\"Fixed Effects\", \"Random Effects\"))\n  rows <- cbind(rows, rbind(\n    rep(\"\", length(models)),\n    rep(\"\", length(models))\n  ))\n  length_fes <- length(coef_map) - length(ranef_index)\n  if (\"statistic\" %in% names(list(...)) && is.null(list(...)$statistic)) {\n    attr(rows, \"position\") <- c(1, (length_fes + 1))\n  } else {\n    attr(rows, \"position\") <- c(1, (length_fes + 1) * 2)\n  }\n  modelsummary::msummary(models, coef_map = coef_map, add_rows = rows, ...)\n}\n\n\n\n\n\n# Read in the data (pay attention to the directory)\nhsball <- read_sav(here(\"data_files\", \"hsball.sav\"))\n\n\n\nTo demonstrate differences in smaller samples, we will use a subset of 16 schools\n\n\n# Randomly select 16 school ids\nset.seed(840)  # use the same seed so that the same 16 schools are selected\nrandom_ids <- sample(unique(hsball$id), size = 16)\nhsbsub <- hsball %>%\n    filter(id %in% random_ids)\n\n\n\nLog-Likelihood Function \\(\\ell\\)\nIf you don’t feel comfortable with linear algebra and matrices, it is okay to skip this part, as it is more important to understand what the likelihood function is doing conceptually. If you are a stat/quant major or are interested in the math, then you may want to study the equation a little bit.\nThe mixed model can be written in matrix form. Let \\(\\bv y_j = [\\bv y_1, \\bv y_2, \\ldots, \\bv y_J]^\\top\\) be the column vector of length \\(N\\) for the outcome variable, \\(\\bv X\\) be the \\(N \\times p\\) predictor matrix (with the first column as the intercept), and \\(\\bv Z\\) be the \\(N \\times Jq\\) design matrix for the random effects with \\(q\\) being the number of random coefficients. To make things more concrete, if we have the model\n\n\n\n\\(\\bv y\\) is the outcome mathach\n\n\nmatrix(head(getME(m1, \"y\")))\n\n\n>#        [,1]\n># [1,]  5.876\n># [2,] 19.708\n># [3,] 20.349\n># [4,]  8.781\n># [5,] 17.898\n># [6,]  4.583\n\n\\(\\bv X\\) is a \\(N \\times 3\\) matrix, with the first column containing all 1s (for the intercept), the second column is meanses, and the third column is ses\n\n\nhead(getME(m1, \"X\"))\n\n\n>#   (Intercept) meanses    ses\n># 1           1  -0.428 -1.528\n># 2           1  -0.428 -0.588\n># 3           1  -0.428 -0.528\n># 4           1  -0.428 -0.668\n># 5           1  -0.428 -0.158\n># 6           1  -0.428  0.022\n\nAnd \\(\\bv Z\\) is a block-diagonal matrix \\(\\mathrm{diag}[\\bv Z_1, \\bv Z_2, \\ldots, \\bv Z_J]\\), where each \\(\\bv Z_j\\) is an \\(n_j \\times 2\\) matrix with the first column containing all 1s and the second column containing the ses variable for cluster \\(j\\)\n\n\ndim(getME(m1, \"Z\"))\n\n\n># [1] 7185  320\n\n# Show first two blocks\ngetME(m1, \"Z\")[1:72, 1:4]\n\n\n># 72 x 4 sparse Matrix of class \"dgCMatrix\"\n>#    1224   1224 1288   1288\n># 1     1 -1.528    .  .    \n># 2     1 -0.588    .  .    \n># 3     1 -0.528    .  .    \n># 4     1 -0.668    .  .    \n># 5     1 -0.158    .  .    \n># 6     1  0.022    .  .    \n># 7     1 -0.618    .  .    \n># 8     1 -0.998    .  .    \n># 9     1 -0.888    .  .    \n># 10    1 -0.458    .  .    \n># 11    1 -1.448    .  .    \n># 12    1 -0.658    .  .    \n># 13    1 -0.468    .  .    \n># 14    1 -0.988    .  .    \n># 15    1  0.332    .  .    \n># 16    1 -0.678    .  .    \n># 17    1 -0.298    .  .    \n># 18    1 -1.528    .  .    \n># 19    1  0.042    .  .    \n># 20    1 -0.078    .  .    \n># 21    1  0.062    .  .    \n># 22    1 -0.128    .  .    \n># 23    1  0.472    .  .    \n># 24    1 -0.468    .  .    \n># 25    1 -1.248    .  .    \n># 26    1 -0.628    .  .    \n># 27    1  0.832    .  .    \n># 28    1 -0.568    .  .    \n># 29    1 -0.258    .  .    \n># 30    1 -0.138    .  .    \n># 31    1 -0.478    .  .    \n># 32    1 -0.948    .  .    \n># 33    1  0.282    .  .    \n># 34    1 -0.118    .  .    \n># 35    1 -0.878    .  .    \n># 36    1 -0.938    .  .    \n># 37    1 -0.548    .  .    \n># 38    1  0.142    .  .    \n># 39    1  0.972    .  .    \n># 40    1  0.372    .  .    \n># 41    1 -1.658    .  .    \n># 42    1 -1.068    .  .    \n># 43    1 -0.248    .  .    \n># 44    1 -1.398    .  .    \n># 45    1  0.752    .  .    \n># 46    1  0.012    .  .    \n># 47    1 -0.418    .  .    \n># 48    .  .        1 -0.788\n># 49    .  .        1 -0.328\n># 50    .  .        1  0.472\n># 51    .  .        1  0.352\n># 52    .  .        1 -1.468\n># 53    .  .        1  0.202\n># 54    .  .        1 -0.518\n># 55    .  .        1 -0.158\n># 56    .  .        1  0.042\n># 57    .  .        1  0.682\n># 58    .  .        1  1.262\n># 59    .  .        1  0.152\n># 60    .  .        1 -0.678\n># 61    .  .        1  0.332\n># 62    .  .        1 -0.728\n># 63    .  .        1  1.262\n># 64    .  .        1  0.612\n># 65    .  .        1  0.692\n># 66    .  .        1  1.082\n># 67    .  .        1  0.222\n># 68    .  .        1  0.032\n># 69    .  .        1 -0.118\n># 70    .  .        1 -0.488\n># 71    .  .        1  0.322\n># 72    .  .        1  0.592\n\nThe matrix form of the model is \\[\\bv y = \\bv X \\bv \\gamma + \\bv Z \\bv u + \\bv e,\\] with \\(\\bv \\gamma\\) being a \\(N \\times p\\) vector of fixed effects, \\(\\bv \\tau\\) a vector of random effect variances, \\(\\sigma\\) the level-1 error term, \\(\\bv u\\) containing the \\(u_0\\) and \\(u_1\\) values for all clusters (320 in total), and \\(\\bv e\\) containing the level-1 errors. The distributional assumptions are \\(\\bv u_j \\sim N_2(\\bv 0, \\bv G)\\) and \\(e_{ij} \\sim N(0, \\sigma)\\).\nThe marginal distribution of \\(\\bv y\\) is thus \\[\\bv y \\sim N_p\\left(\\bv X \\bv \\gamma, \\bv V(\\bv \\tau, \\sigma)\\right),\\] where \\(\\bv V(\\bv \\tau, \\sigma) = \\bv Z \\bv G \\bv Z^\\top + \\sigma^2 \\bv I\\) with \\(\\bv I\\) being an \\(N \\times N\\) identity matrix.\nThe log-likelihood function for a multilevel model is \\[\\ell(\\bv \\gamma, \\bv \\tau, \\sigma; \\bv y) = - \\frac{1}{2} \\left\\{\\log | \\bv V(\\bv \\tau, \\sigma)| + (\\bv y - \\bv X \\bv \\gamma)^\\top \\bv V^{-1}(\\bv \\tau, \\sigma) (\\bv y - \\bv X \\bv \\gamma) \\right\\} + K\\] with \\(K\\) not involving the model parameters. Check out this paper if you want to learn more about the log likelihood function used in lme4.\n\nNote: When statistician says log it means the natural logarithm, i.e., log with base e (sometimes written as ln)\n\nExample: \\(\\ell(\\gamma_{01})\\)\nWe’ll use the model with meanses as the predictor on the full data set (hsball), and use R to write the log-likelihood function for \\(\\gamma_{01}\\)\n\n\n\n\n\n# Extract V from the model\nV_m_lv2 <- (crossprod(getME(m_lv2, \"A\")) + Matrix::Diagonal(7185)) *\n  sigma(m_lv2)^2\n# Log-likelihood function with respect to gamma01\nllfun <- function(gamma01, \n                  gamma00 = fixef(m_lv2)[1], \n                  y = m_lv2@resp$y, \n                  X = cbind(1, m_lv2@frame$meanses), \n                  V = V_m_lv2) {\n  gamma <- c(gamma00, gamma01)\n  y_minus_Xgamma <- y - X %*% gamma\n  as.numeric(\n   - crossprod(y_minus_Xgamma, solve(V, y_minus_Xgamma)) / 2\n  )\n}\n# Vectorize\nllfun <- Vectorize(llfun)\n# Plot\nggplot(tibble(gamma01 = c(5, 7)), aes(x = gamma01)) +\n  stat_function(fun = llfun) +\n  labs(x = expression(gamma[0][1]), y = \"log(Likelihood)\")\n\n\n\n\nML vs REML\nWe’ll use the cross-level interaction model on the subset\n\n\n# Cluster-mean centering\nhsbsub <- hsbsub %>% \n  group_by(id) %>%   # operate within schools\n  mutate(ses_cm = mean(ses),   # create cluster means (the same as `meanses`)\n         ses_cmc = ses - ses_cm) %>%   # cluster-mean centered\n  ungroup()  # exit the \"editing within groups\" mode\n# Default is REML\ncrlv_int <- lmer(mathach ~ meanses + sector * ses_cmc + (ses_cmc | id),\n                 data = hsbsub)\n# Use REML = FALSE for ML\ncrlv_int_ml <- lmer(mathach ~ meanses + sector * ses_cmc + (ses_cmc | id),\n                    data = hsbsub, REML = FALSE)\n# Alternatively, you can use refitML()\n# refitML(crlv_int_ml)\n# Compare the models\nmsummary_mixed(list(\"REML\" = crlv_int,\n                    \"ML\" = crlv_int_ml))\n\n\n\n\n\nREML\n\n\nML\n\n\nFixed Effects\n\n\n\n\n\n\n(Intercept)\n\n\n11.728\n\n\n11.728\n\n\n\n\n(0.670)\n\n\n(0.605)\n\n\nmeanses\n\n\n6.633\n\n\n6.492\n\n\n\n\n(1.139)\n\n\n(1.035)\n\n\nsector\n\n\n1.890\n\n\n1.901\n\n\n\n\n(0.903)\n\n\n(0.815)\n\n\nses_cmc\n\n\n2.860\n\n\n2.862\n\n\n\n\n(0.464)\n\n\n(0.459)\n\n\nsector:ses_cmc\n\n\n−0.885\n\n\n−0.888\n\n\n\n\n(0.661)\n\n\n(0.655)\n\n\nRandom Effects\n\n\n\n\n\n\nsd__(Intercept)\n\n\n1.520\n\n\n1.317\n\n\ncor__(Intercept).ses_cmc\n\n\n1.000\n\n\n1.000\n\n\nsd__ses_cmc\n\n\n0.354\n\n\n0.311\n\n\nsd__Observation\n\n\n5.698\n\n\n5.689\n\n\nAIC\n\n\n4365.1\n\n\n4369.2\n\n\nBIC\n\n\n4405.9\n\n\n4410.0\n\n\nLog.Lik.\n\n\n−2173.563\n\n\n−2175.612\n\n\nREMLcrit\n\n\n4347.125\n\n\n\n\nNotice that the standard errors are generally larger for REML than for ML, and it’s generally more accurate with REML in small samples. Also the \\(\\tau^2\\) estimates (i.e., labelled sd__(Intercept) for \\(\\tau^2_0\\) and sd__ses_cmc for \\(\\tau^2_1\\)) are larger (and more accurately estimated) for REML.\nTo see more details on how lme4 iteratively tries to arrive at the REML/ML estimates, try\n\n\ncrlv_int2 <- lmer(mathach ~ meanses + sector * ses_cmc + (ses_cmc | id),\n                  data = hsbsub, verbose = 1)\n\n\n># iteration: 1\n>#  f(x) = 4396.943300\n># iteration: 2\n>#  f(x) = 4410.471845\n># iteration: 3\n>#  f(x) = 4398.478905\n># iteration: 4\n>#  f(x) = 4411.654851\n># iteration: 5\n>#  f(x) = 4377.931282\n># iteration: 6\n>#  f(x) = 4399.307646\n># iteration: 7\n>#  f(x) = 4371.367019\n># iteration: 8\n>#  f(x) = 4347.537222\n># iteration: 9\n>#  f(x) = 4367.879802\n># iteration: 10\n>#  f(x) = 4350.984197\n># iteration: 11\n>#  f(x) = 4351.658449\n># iteration: 12\n>#  f(x) = 4349.986861\n># iteration: 13\n>#  f(x) = 4347.709342\n># iteration: 14\n>#  f(x) = 4347.884914\n># iteration: 15\n>#  f(x) = 4347.701287\n># iteration: 16\n>#  f(x) = 4347.194148\n># iteration: 17\n>#  f(x) = 4347.172072\n># iteration: 18\n>#  f(x) = 4347.140652\n># iteration: 19\n>#  f(x) = 4347.137326\n># iteration: 20\n>#  f(x) = 4347.127020\n># iteration: 21\n>#  f(x) = 4347.128477\n># iteration: 22\n>#  f(x) = 4347.143600\n># iteration: 23\n>#  f(x) = 4347.125497\n># iteration: 24\n>#  f(x) = 4347.125875\n># iteration: 25\n>#  f(x) = 4347.125844\n># iteration: 26\n>#  f(x) = 4347.125305\n># iteration: 27\n>#  f(x) = 4347.125138\n># iteration: 28\n>#  f(x) = 4347.125098\n># iteration: 29\n>#  f(x) = 4347.125226\n># iteration: 30\n>#  f(x) = 4347.125068\n># iteration: 31\n>#  f(x) = 4347.125119\n># iteration: 32\n>#  f(x) = 4347.125062\n># iteration: 33\n>#  f(x) = 4347.125065\n># iteration: 34\n>#  f(x) = 4347.125063\n># iteration: 35\n>#  f(x) = 4347.125066\n># iteration: 36\n>#  f(x) = 4347.125059\n># iteration: 37\n>#  f(x) = 4347.125056\n># iteration: 38\n>#  f(x) = 4347.125056\n\nThe numbers shown above are the deviance, that is, -2 \\(\\times\\) log-likelihood. Because probabilities (as well as likelihood) are less than or equal to 1, the log will be less than or equal to 0, meaning that the log likelihood values are generally negative. Multiplying it by -2 results in positive deviance that is a bit easier to work with (and the factor 2 is related to converting a normal distribution to a \\(\\chi^2\\) distribution).\nTesting Fixed Effects\nTo test the null that a predictor has a non-zero coefficient given other predictors in the model, an easy way is to use the likelihood-based 95% CI (also called the profile-likelihood CI, as discussed in your text):\n\n\n# Use parm = \"beta_\" for only fixed effects\nconfint(crlv_int, parm = \"beta_\")\n\n\n>#                     2.5 %     97.5 %\n># (Intercept)    10.4692542 12.9863640\n># meanses         4.0885277  8.9516323\n># sector          0.1915185  3.5912387\n># ses_cmc         1.9257690  3.7740789\n># sector:ses_cmc -2.1748867  0.4629693\n\nIf 0 is not in the 95% CI, the null is rejected at .05 significance level. This is basically equivalent to the likelihood ratio test, which can be obtained by comparing the model without one of the coefficients (e.g., the cross-level interaction):\n\n\nmodel_no_crlv <- lmer(mathach ~ meanses + sector + ses_cmc + (ses_cmc | id),\n                      data = hsbsub)\n# Note: lme4 will refit the model to use ML when performing LRT\nanova(crlv_int, model_no_crlv)\n\n\n># Data: hsbsub\n># Models:\n># model_no_crlv: mathach ~ meanses + sector + ses_cmc + (ses_cmc | id)\n># crlv_int: mathach ~ meanses + sector * ses_cmc + (ses_cmc | id)\n>#               npar    AIC    BIC  logLik deviance  Chisq Df\n># model_no_crlv    8 4369.0 4405.2 -2176.5   4353.0          \n># crlv_int         9 4369.2 4410.0 -2175.6   4351.2 1.7377  1\n>#               Pr(>Chisq)\n># model_no_crlv           \n># crlv_int          0.1874\n\nOr with the drop1() function and adding the formula:\n\n\n# Note: lme4 will refit the model to use ML when performing LRT for fixed\n# effects\ndrop1(crlv_int, ~ meanses + sector * ses_cmc, test = \"Chisq\")\n\n\n># Single term deletions\n># \n># Model:\n># mathach ~ meanses + sector * ses_cmc + (ses_cmc | id)\n>#                npar    AIC     LRT   Pr(Chi)    \n># <none>              4369.2                      \n># meanses           1 4384.8 17.6087 2.713e-05 ***\n># sector            1 4371.8  4.5846   0.03226 *  \n># ses_cmc           1 4387.1 19.8347 8.444e-06 ***\n># sector:ses_cmc    1 4369.0  1.7377   0.18743    \n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSmall-Sample Correction: Kenward-Roger Approximation of Degrees of Freedom\nIn small sample situation (with < 50 clusters), the Kenward-Roger (KR) approximation of degrees of freedom will give more accurate tests. To do this in R, you should load the lmerTest package before running the model. Since I’ve already run the model, I will load the package and convert the results using the as_lmerModLmerTest() function:\n\n\nlibrary(lmerTest)\ncrlv_int_test <- as_lmerModLmerTest(crlv_int)\n# Try anova()\nanova(crlv_int_test, ddf = \"Kenward-Roger\")\n\n\n># Type III Analysis of Variance Table with Kenward-Roger's method\n>#                 Sum Sq Mean Sq NumDF   DenDF F value    Pr(>F)    \n># meanses         959.83  959.83     1 13.0241 29.5678 0.0001129 ***\n># sector          141.61  141.61     1 13.2258  4.3624 0.0566181 .  \n># ses_cmc        1150.22 1150.22     1  8.6559 35.4328 0.0002505 ***\n># sector:ses_cmc   55.15   55.15     1 11.7341  1.6990 0.2174123    \n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nYou can see now sector was actually not significant (although it’s part of an interaction so the test of the conditional effect may not be very meaningful). Note KR requires using REML.\nIf you’re interested in knowing more, KR is based on an \\(F\\) test, as opposed to a \\(\\chi^2\\) test for LRT. When the denominator degrees of freedom for \\(F\\) is large, which happens in large sample (with many clusters), the \\(F\\) distribution converges to a \\(\\chi^2\\) distribution, so there is no need to know exactly the degrees of freedom. However, in small samples, these two look different, and to get more accurate \\(p\\) values one needs to have a good estimate of the denominator degrees of freedom, but it’s not straightforward with MLM, especially with unbalanced data (i.e., clusters having different sizes). There are several methods to approximate the degrees of freedom, but KR has generally been found to perform the best.\nTesting Random Slopes\nTo test the null hypothesis that the random slope variance, \\(\\tau^2_1\\), is zero, one can again rely on the LRT. However, because \\(\\tau^2_1\\) is non-negative (i.e., zero or positive), using the regular LRT will lead to a overly conservative test, meaning that power is too small. Technically, as pointed out in your text, under the null the sampling distribution of the LRT will follow approximately a mixture \\(\\chi^2\\) distribution. There are several ways to improve the test, but as shown in LaHuis and Ferguson (2009, https://doi.org/10.1177/1094428107308984), a simple way is to divide the \\(p\\) value you obtain from software by 2 so that it resembles a one-tailed test.\n\n\nran_slp <- lmer(mathach ~ meanses + ses_cmc + (ses_cmc | id), data = hsbsub)\n# Compare to the model without random slopes\nm_bw <- lmer(mathach ~ meanses + ses_cmc + (1 | id), data = hsbsub)\n# Compute the difference in deviance (4356.769 - 4356.754)\nREMLcrit(m_bw) - REMLcrit(ran_slp)\n\n\n># [1] 0.01432169\n\n# Find the p value from a chi-squared distribution\npchisq(REMLcrit(m_bw) - REMLcrit(ran_slp), df = 2, lower.tail = FALSE)\n\n\n># [1] 0.9928647\n\n# Need to divide the p by 2 for a one-tailed test\n\n\n\nSo the \\(p\\) value in our example for testing the random slope variance was 0.5, which suggested that for the subsample, there was insufficient evidence that the slope between ses_cmc and mathach varied across schools.\nYou can also use ranova() to get the same results (again, the \\(p\\) values need to be divded by 2)\n\n\nranova(ran_slp)\n\n\n># ANOVA-like table for random-effects: Single term deletions\n># \n># Model:\n># mathach ~ meanses + ses_cmc + (ses_cmc | id)\n>#                           npar  logLik    AIC      LRT Df Pr(>Chisq)\n># <none>                       7 -2178.4 4370.8                       \n># ses_cmc in (ses_cmc | id)    5 -2178.4 4366.8 0.014322  2     0.9929\n\nBootstrap\nThe bootstrap is a simulation-based method to approximate the sampling distribution of parameter estimates. Indeed, you’ve already seen a version of it in previous weeks when we talk about simulation. In the previous weeks, we generated data assuming that the sample model perfectly described the population, which notably includes the normality assumption. That simulation method is also called parametric bootstrap. Here we’ll use another version of the bootstrap, called the residual bootstrap, which does not assume that the normality assumptions hold for the population. You can check out Lai (2020).\nTo perform the bootstrap, you’ll need to supply functions that extract different parameter estimates or other quantities (e.g., effect sizes) from a fitted model. Below are some examples:\nFixed Effects\n\n\nlibrary(boot)\n# If you have not installed bootmlm, you can uncomment the following line:\n# remotes::install_github(\"marklhc/bootmlm\")\nlibrary(bootmlm)\n\n\n\n\nNote: Because running the bootstrap takes a long time, you can include the chunk option chache=TRUE so that when knitting the file, it will not rerun the chunk unless the content of it has been changed.\n\n\n\n# This takes a few minutes to run\nboot_crlv_int <- bootstrap_mer(\n  crlv_int,  # model\n  # function for extracting information from model (fixef = fixed effect)\n  fixef,\n  nsim = 999,  # number of bootstrap samples\n  type = \"residual\"  # residual bootstrap\n)\n# Bootstrap CI for cross-level interaction (index = 5 for 5th coefficient)\n# Note: type = \"perc\" extracts the percentile CI, which is one of the several\n# possible CI options with the bootstrap\nboot.ci(boot_crlv_int, index = 5, type = \"perc\")\n\n\n># BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n># Based on 999 bootstrap replicates\n># \n># CALL : \n># boot.ci(boot.out = boot_crlv_int, type = \"perc\", index = 5)\n># \n># Intervals : \n># Level     Percentile     \n># 95%   (-2.1774,  0.4343 )  \n># Calculations and Intervals on Original Scale\n\nVariance Components\nExtracting variance components from lmer() results requires a little bit more efforts.\n\n\n# Define function to extract variance components\nget_vc <- function(object) {\n  vc_df <- data.frame(VarCorr(object))\n  vc_df[ , \"vcov\"]\n}\n# Verfiy that the function extracts the right quantities\nget_vc(ran_slp)\n\n\n># [1]  3.04994425  0.04159963  0.08850399 32.55971461\n\n# This again takes a few minutes to run\nboot_ran_slp <- bootstrap_mer(\n  ran_slp,  # model\n  # function for extracting information from model (fixef = fixed effect)\n  get_vc,\n  nsim = 999,  # number of bootstrap samples\n  type = \"residual\"  # residual bootstrap\n)\n# Bootstrap CI for random slope variance (index = 2)\nboot.ci(boot_ran_slp, index = 2, type = \"perc\")\n\n\n># BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n># Based on 999 bootstrap replicates\n># \n># CALL : \n># boot.ci(boot.out = boot_ran_slp, type = \"perc\", index = 2)\n># \n># Intervals : \n># Level     Percentile     \n># 95%   ( 0.0004,  1.3443 )  \n># Calculations and Intervals on Original Scale\n\n\\(R^2\\)\n\n\n# This again takes a few minutes\nboot_r2 <- bootstrap_mer(crlv_int, MuMIn::r.squaredGLMM, nsim = 999, \n                         type = \"residual\")\nboot.ci(boot_r2, index = 1, type = \"perc\")  # index = 1 for marginal R2\n\n\n># BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n># Based on 999 bootstrap replicates\n># \n># CALL : \n># boot.ci(boot.out = boot_r2, type = \"perc\", index = 1)\n># \n># Intervals : \n># Level     Percentile     \n># 95%   ( 0.1705,  0.3370 )  \n># Calculations and Intervals on Original Scale\n\nYou can check the bootstrap sampling distribution using\n\n\nplot(boot_r2, index = 1)\n\n\n\n\n\n\n\n\n",
      "last_modified": "2021-09-19T11:50:26-07:00"
    },
    {
      "path": "syllabus.html",
      "title": "Syllabus",
      "description": "Course syllabus (PSYC 575, 2021 Fall)\n",
      "author": [],
      "contents": "\n\nContents\nPSYC 575 Multilevel Modeling\nCourse Description\nLearning Objectives\nCourse Notes\nCommunication\nTechnological Proficiency and Hardware/Software Required\nUSC technology rental program\nUSC Technology Support Links\n\nRecommended Materials\nOptional Materials\nDescription and Assessment of Assignments\nParticipation\nGrading Breakdown\nGrading Scale\nAssignment Submission\nGrading Timeline\nLate work\nTechnology in the classroom\nPhones\nTablets and Laptops\n\nAttendance\nClassroom Norms\nCOVID-19 Guidance\nCourse Evaluation\n(Tentative) Course Schedule: A Weekly Breakdown\n\nStatement on Academic Conduct and Support Systems\nAcademic Conduct:\nSupport Systems:\n\n\n\n\n\nPSYC 575 Multilevel Modeling\nUnits: 4Term–Day–Time: Fall 2021–Tues & Thurs–10:00-11:50 am\nLocation: WPH 205\nInstructor: Hok Chio (Mark) LaiOffice Hours: Tues 12:00–1:00 pm, and by appointment.Contact Info: (Email) hokchiol@usc.edu, (Slack) https://usc.enterprise.slack.com/.\nTimeline for replying to emails: within 48 hours.\nIT Help: ITS, BlackboardContact Info:\nITS (Email, Monday – Friday, 8:00 A.M. – 6:00 P.M.) consult@usc.edu, (Phone, 24/7/365) 213-740-5555, (Online) ServiceNow Portal\nBlackboard (Email, 24/7/365) blackboard@usc.edu, (Online Help) Blackboard Help for Students\nCourse Description\nThis is a graduate-level class in statistical methods on multilevel modeling, a popular technique in behavioral and social science research. The course covers topics in multilevel modeling, including two- and three-level hierarchical linear models (HLM), random intercepts and slopes, longitudinal models and growth curve models, and some recent development in multilevel modeling.\nThe course begins with a brief overview of the ubiquity of multilevel data and the problems of using conventional methods to handle such data. It then transitions to the conceptual and statistical foundations of two-level multilevel models. Students will learn from different real data examples, and perform analyses using data of their own or provided by the instructor. Later material covers the use of multilevel modeling as a general framework for longitudinal data analysis, and other modeling considerations such as categorical data, non-hierarchical (e.g., cross-classified) data structure, and study designs. Students are also encouraged to provide input in suggesting topics to be covered for this course.\nLearning Objectives\nAfter the successful completion of this course, students will be able to . . .\nExplain the problems of analyzing clustered data with multiple regression/ANOVA;\nIdentify the types of multilevel data structure in different research scenarios;\nDescribe the statistical and conceptual foundations of multilevel modeling;\nIndependently analyze real data using statistical software for multilevel modeling;\nEvaluate published research that uses multilevel modeling;\nApply multilevel modeling in a research project, and effectively communicate findings/products in an oral research presentation or a written research report.\nPrerequisite(s): None\nCo-Requisite(s): None\nConcurrent Enrollment: None\nRecommended Preparation: PSYC 503: Regression and the General Linear Model (or a similar regression class); Experience with statistical software (preferably R)\nCourse Notes\nThis class will be in-person and will follow a flipped course design. The benefit of a flipped course model is that the lecturer can spend more time with students to go through applications of concepts and hands-on exercises of data analyses.\nThe lecture videos and course materials will be available at https://psyc575-2021fall.netlify.app by 9:00 am of each Monday, and students are expected to review these materials and the assigned readings on their own. Please note that the lecture slides only serve to guide class discussions and cannot replace the assigned readings.\nExcept for Week 1, the Tuesday meetings will be optional Q&A sessions where students can bring their questions so that the instructor and the class can discuss unclear concepts. During the mandatory Thursday meetings, students will work on quizzes, in-class exercises, and discuss questions regarding the learning materials and homework assignments. Students are expected to have reviewed the posted materials for that week before attending that week’s group exercise session.\nBefore attending the Thursday sessions, students are expected to have\nCompleted the assigned readings and reviewed the posted videos.\nIdentified questions that come up in their learning.\nStarted working on the homework problems.\nCommunication\nTo promote independence and critical thinking, students are encouraged to work through the following process for obtaining answers to course-related questions before contacting the instructor:\nconsult the course syllabus;\nconsult a classmate or post your questions on Slack;\nmeet with the instructor during office hours or Q&A sessions on Tuesdays;\nfor personal questions, email the instructor at hokchiol@usc.edu\nTechnological Proficiency and Hardware/Software Required\nR and RStudio are needed to complete the course assignments. It is highly recommended that students update to the latest versions of both software (R 4.1.0, RStudio 1.4.10XX, or above). We will discuss how to set up R and RStudio in Week 1.\nStable internet connection (for reviewing lecture videos)\nUSC technology rental program\nIf you need resources to successfully participate in your classes, such as a laptop or internet hotspot, you may be eligible for the university’s equipment rental program. To apply, please submit an application.\nUSC Technology Support Links\nBlackboard help for studentsSlack information for studentsSoftware available to USC Campus\nRecommended Materials\nSnijders, T. A. B., & Bosker, R. J. (2012). Multilevel analysis: An introduction to basic and advanced multilevel modeling (2nd ed.). Thousand Oaks, CA: Sage.\n(Alternative text) Hox, J. J., Moerbeek, M., & van de Schoot, R. (2018). Multilevel analysis: Techniques and Applications (3rd ed.). New York, NY: Routledge.\nOther required readings will be posted on Slack\nOptional Materials\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear models: applications and data analysis methods (2nd ed.). Thousand Oaks, CA: Sage.\nGelman, A., & Hill, J. (2006). _Data analysis using regression and multilevel/hierarchical models. Cambridge, UK: Cambridge University Press.\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford, UK: Oxford University Press. [For longitudinal data analysis]\nWest B. T., Welch, K. B., & Gałecki, A. T. (2014). Linear mixed models: A practical guide using statistical software (2nd ed.). Boca Raton, FL: CRC. [A reference for using different software]\nGałecki, A. T., & Burzykowski, T. (2013). Linear mixed-effects models using R: A step-by-step approach. Springer.\nLuke, D. A. (2020). Multilevel modeling (2nd ed.). Sage.\nHeck, R. H., Thomas, S. L., & Tabata, L. N. (2014). Multilevel and longitudinal modeling with IBM SPSS (2nd ed.). New York, NY: Routledge. [A reference for SPSS users]\nDescription and Assessment of Assignments\nIn-class exercises (10%). During the Thursday sessions, students will participate in group exercises. If students miss a session for any reason, they can complete the exercise posted on Blackboard within 36 hours (i.e., Friday by end of day, Pacific Time) to get credits.\nHomework problems (60%). There will be 10 homework assignments for students to apply the concepts and techniques discussed in class to analytic problems. The assignments typically involve performing data analyses using data sets of your own or provided by the instructor, and interpreting the results with some guided questions.\nYou must submit your work electronically to Blackboard by Friday 11:59 p.m. Pacific Time on the assigned due date. See policy on late work.\nFinal project (30%: 5% prospectus, 5% peer review, 20% presentation/final paper). You will complete a research project related to multilevel modeling, typically a research report of an empirical study using real data or a theoretical/methodological paper about certain aspects of multilevel modeling. Students interested in other project ideas (e.g., software package development) should discuss their ideas with the instructor. Each student can choose to work on their own or in a group of up to three people. Each student/group will schedule an appointment with the instructor to talk about their project during week 9 (October 18–22).\nThere are three grading components for your final project:\nProspectus (5%)\nA prospectus about your project should be submitted by the day before the individual meeting. The prospectus should contain a concise description of what you (or your group) plan to do for your project, including a preliminary plan for statistical analysis. The prospectus should be limited to 1 single-spaced page (excluding tables, figures, references, and other supplemental materials).\nPeer Review (5%)\nAfter the individual meeting with the instructor, each individual/group will refine their research questions and post a summary of their research questions and preliminary analyses to the dedicated forum on the Discussion Board by Friday, November 5. Each student will then give specific comments to the summaries of two other students/groups by Monday, November 15. More information on what feedback to give will be included in the grading rubric.\nFinal Presentation/Paper (20%)\nIf you choose to do a presentation, on November 30 or December 2, you or your group will give a 20-minute presentation (including Q&A) on your project. The final presentation should include the following four sections: introduction, method, results, and discussion; for methodological or theoretical work, students should follow organizations that are typical in previous papers in their areas of research. More emphasis should be put on describing the technical details of the analysis and the interpretations of the results. You will also need to submit your slides to Blackboard for grading on the day of your presentation, which should include a link to the reproducible codes for your analyses. A grading rubric on the research presentation will be posted on Blackboard.\nIf you choose to do a final paper, your paper will be due Tuesday, December 9, at 1:00 p.m. Pacific Time (the assigned final exam time for the class). The final paper should include four sections: introduction, method, results, and discussion, or comparable sections; however, more emphasis should be put on describing the technical details of the analysis and the interpretations of the results. There should also be a link to the reproducible codes for your analyses. The final paper should be 8-15 double-spaced pages of text (i.e., excluding title page, abstract, references, tables, figures, and appendices).\n\nParticipation\nParticipation accounts for 10% of the course grade. To earn full credit for participation, students should complete and turn in all in-class exercises.\nGrading Breakdown\nAssignment\n% of Grade\nIn-class exercises\n10\nHomework 1-9\n60\nProspectus\n5\nPeer Review\n5\nFinal Presentation/Paper\n20\nTOTAL\n100\nGrading Scale\nCourse final grades will be determined using the following scale\nA\n93-100\nA-\n89-92\nB+\n85-88\nB\n81-84\nB-\n77-80\nC+\n73-76\nC\n70-72\nC-\nBelow 70 (failing)\nAssignment Submission\nThe assignments should be submitted through Blackboard by Friday at 11:59 p.m. Pacific Time, before the class starts.\nGrading Timeline\nGenerally, all graded work will be returned no later than one week from the submission deadline. However, given the high number of students in the class, the instructor may only grade selected questions in each assignment. Solutions will be posted so that students can check their own work.\nLate work\nLate work will be penalized by a 10% deduction in the assignment grade every 24 hours late unless due to an emergency excused by the instructor. Email the instructor as soon as possible to discuss alternate arrangements due to an emergency.\nTechnology in the classroom\nPhones\nYour phone should be turned off or in silent mode (not on vibrate), and should not be used in the classroom.\nTablets and Laptops\nDuring lecture time in the classroom, students can use tablets and laptops only for purposes of viewing course materials and taking notes. Use of tablets and laptops for note taking is strongly discouraged as it may distract both yourself and your peers (Sana, Weston, & Cepeda, 2013). During the in-class exercises, students should use their laptops to complete the assignments.\nAttendance\nStudents are expected to attend all Thursday class sessions on time. If they miss a session, they should complete the class exercises and turn in their work within the timeframe specified in Description and Assessment of Assignments.\nClassroom Norms\nFrom USC’s FALL 2021 GUIDE: Return To Campus Protocols document,\n\nStudents, faculty and staff need to be aware of COVID-19 symptoms, and are required to complete a daily self-screening via Trojan Check before coming onto campus or leaving their on-campus residence.\n\n\nStudents, faculty, and staff are required to wear masks indoors, including classrooms – and no food or drink is permitted during class\n\nThe following applies to both in-person and online communications (e.g., Slack discussions and email communications)\nRespect each other’s views.\nIn written communication messages, make sure they are something you could say to someone to their face.\nRecognize and/or remember that we have different backgrounds. \nCriticize ideas, not individuals or groups.\nEither support statements with evidence, or speak from personal experience.\nCOVID-19 Guidance\nStudents should consult the latest COVID-19 testing and health protocol requirements for on-campus courses. Continuously updated requirements can be found on the USC COVID-19 resource center website at https://coronavirus.usc.edu/ and https://we-are.usc.edu/.\nCourse Evaluation\nStudent feedback is essential to the instructor and the Department to keep improving this course. Students are encouraged to share their feedback and suggestions in an early-term feedback survey around week 4 to 5, and respond to the standard USC course evaluation survey at the end of the semester.\n(Tentative) Course Schedule: A Weekly Breakdown\n\n\n\n\nTopics/Daily Activities\nReadings\nAssignment Dates\nWeek 1\nAug 24 & 26\nOverview of Multilevel Models\nR Markdown\n\nSB ch 1, 2\nR Markdown Intro\nMarkdown Quick Reference\n\nExercise: R Markdown\nHW 1\nQuiz on Regression\n\nWeek 2\nAug 31 & Sep 2\nWhat are Statistical Models?\nReview of Regression\n\nGelman et al. ch 1.1, 1.2, 1.4\nGelman et al. ch 4.1, 4.2, 4.4, 4.5\n10 quick tips to improve your regression modeling\n\nExercise: Interpreting interactions\nHW 2\n\nWeek 3\nSep 7 & 9\nThe Random Intercept Model\n\nSB ch 3.1–3.4, 4.1–4.5, 4.8\n\nExercise: Empirical Bayes estimates\nHW 3\n\nWeek 4\nSep 14 & 16\nEffect Decomposition\nRandom Coefficient Model\nCross-level Interactions\n\nSB ch 4.6, 5.1–5.3\n\nExercise: Cross-level interaction\nHW 4\n\nWeek 5\nSep 21 & 23\nModel Estimation\nModel Testing\nReporting Results\n\nSB ch 4.7, 6\n\nExercise: Maximum likelihood estimation (MLE)\nHW 5\n\nWeek 6\nSep 28 & 30\nModel Assumptions and Diagnostics\n\nSB ch 10\n\nExercise: TBD\nHW 6\n\nWeek 7\nOct 5 & 7\nMLM for Experimental Designs\nCross-classified Models\n\nSB ch 13.1\nHoffman & Rovine (2007)\n\nExercise: Identifying data structure\nHW 7\n\nWeek 8\nOct 12 & 14 (Fall recess)\nModels for Longitudinal Data I\nUsing GitHub\n\nSB ch 15\n\n\nWeek 9\nOct 19 & 21\nIndividual meeting on final research project\n\nProspectus (due day before meeting)\n\nWeek 10\nOct 26 & 28\nModels for Longitudinal Data II\n\nHoffman (2014) ch 8\n\nExercise: Autoregressive and lagged effects\nHW 8\n\nWeek 11\nNov 2 & 4\nPredictive Modeling\n\nYarkoni & Westfall (2017)\n\nExercise: Model averaging\nPost draft for peer review\n\nWeek 12\nNov 9 & 11\nMultilevel logistic regression\nDiscrete outcomes\n\nSB ch 17\n\nExercise: Probability vs. odds ratio\nHW 9\nPeer review (due Nov 15)\n\nWeek 13\nNov 16 & 18\nSample size planning\n\nSB ch 11\n\nExercise: Required sample size\nHW 10\n\nWeek 14\nNov 23 & 25 (Thanksgiving)\nMissing Data\n\nSB ch 9\nTBD\n\n\nWeek 15\nNov 30 & Dec 2\nFinal Presentation\n\nUpload slides (on presentation day)\nFINAL\n\n\nFinal paper (due Dec 9, 1:00 pm)\nSB = Snijders & Bosker (2012)\nStatement on Academic Conduct and Support Systems\nAcademic Conduct:\nPlagiarism—presenting someone else’s ideas as your own, either verbatim or recast in your own words—is a serious academic offense with serious consequences. Please familiarize yourself with the discussion of plagiarism in SCampus in Part B, Section 11, “Behavior Violating University Standards” policy.usc.edu/scampus-part-b. Other forms of academic dishonesty are equally unacceptable. See additional information in SCampus and university policies on scientific misconduct, policy.usc.edu/scientific-misconduct.\nSupport Systems:\nCounseling and Mental Health - (213) 740-9355 – 24/7 on callstudenthealth.usc.edu/counseling\nFree and confidential mental health treatment for students, including short-term psychotherapy, group counseling, stress fitness workshops, and crisis intervention.\nNational Suicide Prevention Lifeline - 1 (800) 273-8255 – 24/7 on callsuicidepreventionlifeline.org\nFree and confidential emotional support to people in suicidal crisis or emotional distress 24 hours a day, 7 days a week.\nRelationship and Sexual Violence Prevention Services (RSVP) - (213) 740-9355(WELL), press “0” after hours - 24/7 on callstudenthealth.usc.edu/sexual-assault\nFree and confidential therapy services, workshops, and training for situations related to gender-based harm.\nOffice of Equity and Diversity (OED) - (213) 740-5086 | Title IX - (213) 821-8298equity.usc.edu, titleix.usc.edu\nInformation about how to get help or help someone affected by harassment or discrimination, rights of protected classes, reporting options, and additional resources for students, faculty, staff, visitors, and applicants.\nReporting Incidents of Bias or Harassment - (213) 740-5086 or (213) 821-8298usc-advocate.symplicity.com/care_report\nAvenue to report incidents of bias, hate crimes, and microaggressions to the Office of Equity and Diversity |Title IX for appropriate investigation, supportive measures, and response.\nThe Office of Disability Services and Programs - (213) 740-0776dsp.usc.edu\nSupport and accommodations for students with disabilities. Services include assistance in providing readers/notetakers/interpreters, special accommodations for test taking needs, assistance with architectural barriers, assistive technology, and support for individual needs.\nUSC Campus Support and Intervention - (213) 821-4710campussupport.usc.edu\nAssists students and families in resolving complex personal, financial, and academic issues adversely affecting their success as a student.\nDiversity at USC - (213) 740-2101diversity.usc.edu\nInformation on events, programs and training, the Provost’s Diversity and Inclusion Council, Diversity Liaisons for each academic school, chronology, participation, and various resources for students.\nUSC Emergency - UPC: (213) 740-4321, HSC: (323) 442-1000 – 24/7 on calldps.usc.edu, emergency.usc.edu\nEmergency assistance and avenue to report a crime. Latest updates regarding safety, including ways in which instruction will be continued if an officially declared emergency makes travel to campus infeasible.\nUSC Department of Public Safety - UPC: (213) 740-6000, HSC: (323) 442-1200 – 24/7 on calldps.usc.edu\nNon-emergency assistance or information.\n\n\n\n",
      "last_modified": "2021-09-19T11:50:28-07:00"
    },
    {
      "path": "week1.html",
      "title": "Week 1",
      "description": "Overview of Multilevel Models\n",
      "author": [],
      "contents": "\n\nContents\nWeek Learning Objectives\nTask Lists\n\nLecture\nSlides\nMultilevel Data Structure\nName and History\nUsage of MLM\n\n\nBefore you start the materials for week 1, make sure you have reviewed the syllabus.\nWeek Learning Objectives\nBy the end of this module, you will be able to\nIdentify alternative names for multilevel modeling (MLM)\nDescribe the types of data MLM can handle\nKnit a simple R Markdown file\nTask Lists\nAttend the Tuesday session for course introduction and Q&A\nInstall/Update R and RStudio on your computer\nReview the resources (lecture videos and slides)\nComplete the assigned readings\nSnijders & Bosker chapters 1 and 2 (they’re relatively short)\nR Markdown Intro\nMarkdown Quick Reference\n\nIntroduce yourself on the #introduction Slack channel (as part of HW 1)\nAttend the Thursday session and participate in the class exercise\nComplete Homework 1\nLecture\nSlides\nYou can view and download the slides here: HTML PDF\nMultilevel Data Structure\n\n\n\n\n\nCheck your learning: How would you describe the data structure in the video?\n\n\nobservational educational hierarchical \n\n\n function validate_form_58955() {var x, text; var x = document.forms[\"form_58955\"][\"answer_58955\"].value;if (x == \"hierarchical\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_58955').innerHTML = text; return false;} \nName and History\n\n\n\n\n\nCheck your learning: What is another name for multilevel modeling?\n\n\nmixed-effect modelinghierarchical regressionfixed-effect modeling\n\n\n function validate_form_95885() {var x, text; var x = document.forms[\"form_95885\"][\"answer_95885\"].value;if (x == \"mixed-effect modeling\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_95885').innerHTML = text; return false;} \nUsage of MLM\n\n\n\n\n\n\nAnother example that relates to ecological fallacies is the “happiness paradox” in economics, which says that income is related to happiness in a cross-sectional analysis (i.e., between-person level) but not in a longitudinal/time series analysis (i.e., within-person level).\n\nCheck your learning: In the data structure where there are multiple measurements for the same person, which level is level 1?\n\n\nmeasurement person \n\n\n function validate_form_88862() {var x, text; var x = document.forms[\"form_88862\"][\"answer_88862\"].value;if (x == \"measurement\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_88862').innerHTML = text; return false;} \n\n\n\n",
      "last_modified": "2021-09-19T11:50:29-07:00"
    },
    {
      "path": "week2.html",
      "title": "Week 2",
      "description": "Review of Regression\n",
      "author": [],
      "contents": "\n\nContents\nWeek Learning Objectives\nTask Lists\n\nLecture\nSlides\nStatistical Model\nImport Data\nLinear Regression\nSample regression line\nCentering\n\nCategorical Predictor\nMultiple Regression\nInteraction\n\n\nWeek Learning Objectives\nBy the end of this module, you will be able to\nDescribe the statistical model for regression\nWrite out the model equations\nSimulate data based on a regression model\nPlot interactions\nTask Lists\nIf you have questions, attend the Tuesday Q&A session\nComplete the assigned readings\nGelman et al. ch 1.1, 1.2, 1.4\nGelman et al. ch 4.1, 4.2, 4.4, 4.5\n10 quick tips to improve your regression modeling\n\nAttend the Thursday session and participate in the class exercise\nComplete Homework 2\nLecture\nSlides\nYou can view and download the slides here: HTML PDF\nStatistical Model\n\n\n\n\n\nCheck your learning: In the example in the video, why do we need a random component?\n\n\nBecause there is only one predictorBecause the relation between the variables is linearBecause people who spend the same amount of time studying do not always have the same mastery level\n\n\n function validate_form_77728() {var x, text; var x = document.forms[\"form_77728\"][\"answer_77728\"].value;if (x == \"Because people who spend the same amount of time studying do not always have the same mastery level\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_77728').innerHTML = text; return false;} \nImport Data\n\n\n\n\n\nCheck your learning: What is the coding for the sex variable?\n\n\n1 = male, 2 = female2 = male, 1 = female0 = female, 1 = male0 = male, 1 = female\n\n\n function validate_form_31996() {var x, text; var x = document.forms[\"form_31996\"][\"answer_31996\"].value;if (x == \"0 = male, 1 = female\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_31996').innerHTML = text; return false;} \n\n\n\n\n\nTake a pause and look at the scatterplot matrix. Ask yourself the following:\nHow does the distribution of salary look?\nAre there more males or females in the data?\nHow would you describe the relationship between number of publications and salary?\nLinear Regression\n\n\n\n\n\nSample regression line\n\n\n\n\n\nCheck your learning: How would you translate the regression line \\(y = \\beta_0 + \\beta_1 \\text{predictor1}\\) into R?\n\n\ny ~ 0 + predictor1predictor1 ~ 1 + yy ~ predictor1\n\n\n function validate_form_71577() {var x, text; var x = document.forms[\"form_71577\"][\"answer_71577\"].value;if (x == \"y ~ predictor1\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_71577').innerHTML = text; return false;} \nCentering\n\n\n\n\n\nCheck your learning: The mean of the pub variable is 18.2. If we call the mean-centered version of it as pub_c, what should be the value of pub_c for someone with 10 publications?\n\n\n 8.2  1.8  -8.2  1.82 \n\n\n function validate_form_50623() {var x, text; var x = document.forms[\"form_50623\"][\"answer_50623\"].value;if (x == \" -8.2\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_50623').innerHTML = text; return false;} \nCategorical Predictor\n\n\n\n\n\nCheck your learning: In a regression analysis, assume that there is a binary predictor that indicates whether a school is public (coded as 0) or private (coded as 1). If the coefficient for that predictor is 1.5, which category has a higher predicted score?\n\n\nprivate public \n\n\n function validate_form_84835() {var x, text; var x = document.forms[\"form_84835\"][\"answer_84835\"].value;if (x == \"private\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_84835').innerHTML = text; return false;} \nMultiple Regression\n\n\n\n\n\nThink more: the coefficient of pub_c becomes smaller after adding time into the equation. Why do you think that is the case?\nInteraction\n\n\n\n\n\n\n\n\n\n\nPratice yourself: from the interaction model obtain the regression line when pub = 50.\n\n\n\n",
      "last_modified": "2021-09-19T11:50:31-07:00"
    },
    {
      "path": "week3.html",
      "title": "Week 3",
      "description": "The Random Intercept Model\n",
      "author": [],
      "contents": "\n\nContents\nWeek Learning Objectives\nTask Lists\n\nLecture\nSlides\nOverview\nUnconditional Random Intercept Model\nEquations\nPath diagram\n\nFixed and Random Effects\nThe Intraclass Correlation\nEmpirical Bayes Estimates\nAdding a Level-2 Predictor\nThe design effect\nAggregation\nStandard error estimates under OLS and MLM\nModel equations\nMLM with a level-2 predictor in R\nStatistical inference\n\n\n\nWeek Learning Objectives\nBy the end of this module, you will be able to\nExplain the components of a random intercept model\nInterpret intraclass correlations\nUse the design effect to decide whether MLM is needed\nExplain why ignoring clustering (e.g., regression) leads to inflated chances of Type I errors\nDescribe how MLM pools information to obtain more stable inferences of groups\nTask Lists\nReview the resources (lecture videos and slides)\nComplete the assigned readings\nSnijders & Bosker ch 3.1–3.4, 4.1–4.5, 4.8\n\nAttend the Thursday session and participate in the class exercise\nComplete Homework 3\nLecture\nSlides\nYou can view and download the slides here: PDF\nOverview\n\n\n\n\n\nCheck your learning: Here’s a snapshot of the sleepstudy data:\n\n   Reaction Days Subject\n1  249.5600    0     308\n2  258.7047    1     308\n3  250.8006    2     308\n11 222.7339    0     309\n12 205.2658    1     309\n13 202.9778    2     309\n21 199.0539    0     310\n22 194.3322    1     310\n23 234.3200    2     310\n\nwhere Subject is the cluster ID. Is Days a level-1 or a level-2 variable?\n\n\nLevel-1Level-2\n\n\n function validate_form_69938() {var x, text; var x = document.forms[\"form_69938\"][\"answer_69938\"].value;if (x == \"Level-1\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_69938').innerHTML = text; return false;} \nUnconditional Random Intercept Model\nEquations\n\n\n\n\n\nCheck your learning: \\(u_{0j}\\) is the new term in a multilevel model (compared to regression). Is it a level-1 or a level-2 deviation variable?\n\n\nLevel-1Level-2\n\n\n function validate_form_7616() {var x, text; var x = document.forms[\"form_7616\"][\"answer_7616\"].value;if (x == \"Level-2\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_7616').innerHTML = text; return false;} \nPath diagram\n\n\n\n\n\nCheck your learning: For the diagram in the video, which one is an actual variable in the data?\n\n\n\\(\\gamma_{00}\\)\\(\\beta_{0j}\\)\\(u_{0j}\\)\\(Y_{ij}\\)\n\n\n function validate_form_15241() {var x, text; var x = document.forms[\"form_15241\"][\"answer_15241\"].value;if (x == \"$Y_{ij}$\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_15241').innerHTML = text; return false;} \nFixed and Random Effects\n\n\n\n\n\nCheck your learning: For the unconditional model, which of the following is a fixed effect?\n\n\nschool meansindividual scoresvariance componentsthe grand mean\n\n\n function validate_form_70740() {var x, text; var x = document.forms[\"form_70740\"][\"answer_70740\"].value;if (x == \"the grand mean\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_70740').innerHTML = text; return false;} \nThe Intraclass Correlation\n\n\n\n\n\nNote: On the slide around the 9 minute mark, the numbers labeled the “Std.Dev.” is just the square root of the variance components. That is, the standard deviation of the school means and the within-school standard deviation.\nCheck your learning: For a study, if \\(\\tau^2_0 = 5\\), \\(\\sigma^2 = 10\\), what is the ICC?\n\n\n0.52.00.333\n\n\n function validate_form_79179() {var x, text; var x = document.forms[\"form_79179\"][\"answer_79179\"].value;if (x == \"0.333\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_79179').innerHTML = text; return false;} \nCheck your learning: The graph below shows the distribution of the Reaction variable in the sleepstudy data. What do you think is a good guess for the its ICC?\n\n\n\n\n\n00.40.9\n\n\n function validate_form_63409() {var x, text; var x = document.forms[\"form_63409\"][\"answer_63409\"].value;if (x == \"0.4\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_63409').innerHTML = text; return false;} \nEmpirical Bayes Estimates\nNote: OLS = ordinary least squares, the estimation method commonly used in regular regression.\n\n\n\n\n\nThinking exercise: When \\(\\sigma^2 / n_j = 0\\), \\(\\lambda_j = 1\\), and the empirical Bayes estimate will be the same as the sample school mean, meaning that there is no borrowing of information. Why is there no need to borrow information in this situation?\nAdding a Level-2 Predictor\nNote that the ses was standardized in the data set, meaning that ses = 0 is at the sample mean, and ses = 1 means one standard deviation above the mean.\n\n\n\n\n\nCheck your learning: In regression, the independent observation assumption means that\n\n\nThe predictor variables should be independentKnowing the score of one observation gives no information about other observationsThe data should follow a hierarchical structure\n\n\n function validate_form_23620() {var x, text; var x = document.forms[\"form_23620\"][\"answer_23620\"].value;if (x == \"Knowing the score of one observation gives no information about other observations\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_23620').innerHTML = text; return false;} \nThe design effect\n\n\n\n\n\nPractice yourself: Compute the design effect for mathach for the HSB data. Which of the following is the closest to your computation?\n\n\n8.18.929.61294.1\n\n\n function validate_form_68450() {var x, text; var x = document.forms[\"form_68450\"][\"answer_68450\"].value;if (x == \"8.9\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_68450').innerHTML = text; return false;} \nBonus Challenge: What is the design effect for a longitudinal study of 5 waves with 30 individuals, and with an ICC for the outcome of 0.5?\n\n\n1315.5\n\n\n function validate_form_84390() {var x, text; var x = document.forms[\"form_84390\"][\"answer_84390\"].value;if (x == \"3\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_84390').innerHTML = text; return false;} \nAggregation\nWhile disaggregation yields results with standard errors being too small, aggregation generally results in standard errors that are slightly larger. The main problem of aggregation, however, is that it removes all the information in the lower level, so level-1 predictors cannot be studied. MLM avoids problems of both disaggregation and aggregation.\nStandard error estimates under OLS and MLM\nThis part is optional but gives a mathematical explanation of why OLS underestimates the standard error\n\n\n\n\n\nModel equations\n\n\n\n\n\nCheck your learning: In the level-2 equation with meanses as the predictor, what is the outcome variable?\n\n\nstudent math achievement scoresschool mean math achievementvariance of school means\n\n\n function validate_form_2969() {var x, text; var x = document.forms[\"form_2969\"][\"answer_2969\"].value;if (x == \"school mean math achievement\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_2969').innerHTML = text; return false;} \nMLM with a level-2 predictor in R\n\n\n\n\n\nCheck your learning: How do you interpret the coefficient for meanses?\n\n\nThe predicted difference in math achievement between two schools with one unit difference in mean SESThe mean achievement for a school with meanses = 0The variance of math achievement accounted for by meanses\n\n\n function validate_form_20289() {var x, text; var x = document.forms[\"form_20289\"][\"answer_20289\"].value;if (x == \"The predicted difference in math achievement between two schools with one unit difference in mean SES\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_20289').innerHTML = text; return false;} \nStatistical inference\n\n\n\n\n\nNote: If the 95% CI exlcudes zero, there is evidence that the predictor has a nonzero relation with the outcome.\nCheck your learning: By default, what type of confidence interval is computed by the lme4 package?\n\n\nLikelihood-basedWaldScoreBootstrap\n\n\n function validate_form_33464() {var x, text; var x = document.forms[\"form_33464\"][\"answer_33464\"].value;if (x == \"Likelihood-based\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_33464').innerHTML = text; return false;} \n\n\n\n",
      "last_modified": "2021-09-19T11:50:33-07:00"
    },
    {
      "path": "week4.html",
      "title": "Week 4",
      "description": "Effect Decomposition, Random Coefficient Model, and Cross-level Interactions\n",
      "author": [],
      "contents": "\n\nContents\nWeek Learning Objectives\nTask Lists\n\nLecture\nSlides\nOverview\nEcological Fallacy\nDecomposing Effects\nBetween/within effects\nPath diagram and equations\nInterpret the between/within effects\n\nContextual Effects\nRandom Slopes/Random Coefficients\nDeveloping intuition\nEquations and path diagram\nInterpretations\n\nCross-Level Interaction\n\n\nWeek Learning Objectives\nBy the end of this module, you will be able to\nExplain what the ecological fallacy is\nUse cluster-mean/group-mean centering to decompose the effect of a lv-1 predictor\nDefine contextual effects\nExplain the concept of random slopes\nAnalyze and interpret cross-level interaction effects\nTask Lists\nReview the resources (lecture videos and slides)\nComplete the assigned readings\nSnijders & Bosker ch 4.6, 5.1–5.3\n\nAttend the Thursday session and participate in the class exercise\nComplete Homework 4\nNow that you have learned the basics of MLM, start thinking about your project (Prospectus due around Oct 18)\nLecture\nSlides\nYou can view and download the slides here: PDF\nOverview\n\n\n\n\n\nCheck your learning: The Type I error inflation problem when using OLS regression for clustered data applies to\n\n\nLevel-1 predictorsLevel-2 predictorsLevel-2 predictors and most level-1 predictors\n\n\n function validate_form_66411() {var x, text; var x = document.forms[\"form_66411\"][\"answer_66411\"].value;if (x == \"Level-2 predictors and most level-1 predictors\"){text = 'You are on your way to be an MLM expert :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_66411').innerHTML = text; return false;} \nEcological Fallacy\n\n\n\n\n\nCheck your learning: In a “bizarre” research finding that found a correlation between chocolate consumption and number of Nobel prize winners at the country level, which of the following is reasonable to infer?\n\n\nThat those who eat more chocolate are more likely to win a nobel prizeThat nutritional contents in chocolate is related to cognitive functioningThat there are potential third variables that relate to both a country’s chocolate consumption level and the number of Nobel prize winners\n\n\n function validate_form_57187() {var x, text; var x = document.forms[\"form_57187\"][\"answer_57187\"].value;if (x == \"That there are potential third variables that relate to both a country's chocolate consumption level and the number of Nobel prize winners\"){text = 'You are on your way to be an MLM expert :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_57187').innerHTML = text; return false;} \nCheck your learning: Summarize the “Big-Fish-Little-Pond Effect” in terms of how a person’s own academic performance and the overall performance of the person’s school on academic self-concept.\nDecomposing Effects\nBetween/within effects\nNote: What I called “cluster-mean centering” is the same as “within-group centering” in Snijders & Bosker (2012)\n\n\n\n\n\nCheck your learning: Why do we need to separate a level-1 predictor into two variables in the model?\n\n\nBecause the slope of the level-1 predictor and that of its cluster means may be differentBecause there is an interaction between the predictor and the cluster variableBecause the intraclass correlation for any level-1 predictor will be zero\n\n\n function validate_form_89916() {var x, text; var x = document.forms[\"form_89916\"][\"answer_89916\"].value;if (x == \"Because the slope of the level-1 predictor and that of its cluster means may be different\"){text = 'Well done :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_89916').innerHTML = text; return false;} \nPath diagram and equations\n\n\n\n\n\nThinking exercise: Based on the between-cluster level component in the path diagram and in the equations, meanses can predict\n\n\nBoth the school-level math achievement and a student’s relative standing within a schoolOnly a student’s relative standing within a schoolOnly the school means of math achievement\n\n\n function validate_form_3042() {var x, text; var x = document.forms[\"form_3042\"][\"answer_3042\"].value;if (x == \"Only the school means of math achievement\"){text = 'Well done :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_3042').innerHTML = text; return false;} \nCheck your learning: Based on the results shown in the video, is the school-level slope or the student-level slope larger for the association between SES and math achievement?\n\n\nSchool-levelStudent-level\n\n\n function validate_form_88760() {var x, text; var x = document.forms[\"form_88760\"][\"answer_88760\"].value;if (x == \"School-level\"){text = 'Well done :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_88760').innerHTML = text; return false;} \nInterpret the between/within effects\n\n\n\n\n\nTry it yourself: Obtain the predicted mathach for Student B, and compare with Students A and C.\nContextual Effects\n\n\n\n\n\nCheck your learning: The contextual effect is\n\n\nThe predicted difference in the outcome for every unit difference in the cluster mean predictor, holding constant the level-1 predictorThe predicted difference in the outcome for every unit difference in the level-1 predictor, within a given clusterThe between-level effect\n\n\n function validate_form_6163() {var x, text; var x = document.forms[\"form_6163\"][\"answer_6163\"].value;if (x == \"The predicted difference in the outcome for every unit difference in the cluster mean predictor, holding constant the level-1 predictor\"){text = 'Well done :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_6163').innerHTML = text; return false;} \nRandom Slopes/Random Coefficients\nDeveloping intuition\n\n\n\n\n\nCheck your learning: In a random-coefficient model, if there are \\(J\\) cluster, there are\n\n\none intercept, two slopesone intercept, J slopesJ intercepts, one slopesJ intercepts, J slopes\n\n\n function validate_form_24051() {var x, text; var x = document.forms[\"form_24051\"][\"answer_24051\"].value;if (x == \"J intercepts, J slopes\"){text = 'Well done :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_24051').innerHTML = text; return false;} \nEquations and path diagram\n\n\n\n\n\nCheck your learning: Which combination of \\(\\tau_0\\) and \\(\\tau_1\\) best describes the graph below?\n\n\n\n\n\ntau_0 = 0.1, tau_1 = 0.2tau_0 = 0, tau_1 = 0.2tau_0 = 0.1, tau_1 = 0tau_0 = 0, tau_1 = 0\n\n\n function validate_form_38868() {var x, text; var x = document.forms[\"form_38868\"][\"answer_38868\"].value;if (x == \"tau_0 = 0.1, tau_1 = 0.2\"){text = 'You are on your way to be an MLM expert :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_38868').innerHTML = text; return false;} \nInterpretations\n\n\n\n\n\nCheck your learning: In a random-slope model, if \\(\\gamma_{10}\\) (the average slope) = 0.2, \\(\\tau^2_1 = 0.04\\), what is the 68% plausible range for the slopes across clusters?\n\n\n[0, 0.4][-0.48, 0.88][0.16, 0.24][0.12, 0.28]\n\n\n function validate_form_56030() {var x, text; var x = document.forms[\"form_56030\"][\"answer_56030\"].value;if (x == \"[0, 0.4]\"){text = 'Well done :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_56030').innerHTML = text; return false;} \nCross-Level Interaction\nIn the video, there was a mistake in the path diagram, in that one of the circle should be \\(\\beta_{1j}\\), not \\(\\beta_{0j}\\)\n\n\n\n\n\nCheck your learning: Conceptually, a cross-level interaction is the same as\n\n\nAn interaction between two level-1 predictorsUsing a cluster-level predictor to predict variations in level-1 slopesRandom slopesAdding the main effect of a cluster-level predictor\n\n\n function validate_form_61063() {var x, text; var x = document.forms[\"form_61063\"][\"answer_61063\"].value;if (x == \"Using a cluster-level predictor to predict variations in level-1 slopes\"){text = 'Well done :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_61063').innerHTML = text; return false;} \n\n\n\n",
      "last_modified": "2021-09-19T11:50:36-07:00"
    },
    {
      "path": "week5.html",
      "title": "Week 5",
      "description": "Model Estimation and Model Testing\n",
      "author": [],
      "contents": "\n\nContents\nWeek Learning Objectives\nTask Lists\n\nLecture\nSlides\nEstimation Methods\nMaximum likelihood\nEstimation methods for MLM\n\nTesting\nLikelihood ratio test (LRT) for fixed effects\n\\(F\\) test with small-sample correction\nLRT for random slope variance\n\nMultilevel bootstrap\n\n\nWeek Learning Objectives\nBy the end of this module, you will be able to\nDescribe conceptually what likelihood function and maximum likelihood estimation are\nDescribe the differences between maximum likelihood and restricted maximum likelihood\nConduct statistical tests for fixed effects\nTest fixed effects using the F-test with the small-sample correction when the number of clusters is small\nUse the likelihood ratio test to test random slopes\nTask Lists\nReview the resources (lecture videos and slides)\nComplete the assigned readings\nSnijders & Bosker ch 4.7, 6\n\nAttend the Thursday session and participate in the class exercise\nComplete Homework 5\n(Optional) Fill out the early/mid-semester feedback survey on Blackboard\nLecture\nSlides\nYou can view and download the slides here: HTML PDF\nEstimation Methods\n\n\n\n\n\nCheck your learning: The values you obtained from MLM software (e.g., lme4) are\n\n\nParameter valuesSample estimates\n\n\n function validate_form_58342() {var x, text; var x = document.forms[\"form_58342\"][\"answer_58342\"].value;if (x == \"Sample estimates\"){text = 'Your statistical knowledge is getting better and better :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_58342').innerHTML = text; return false;} \nMaximum likelihood\n\n\n\n\n\nCheck your learning: Using R, verify that, if \\(\\mu = 10\\) and \\(\\sigma = 8\\) for a normally distributed population, the probability (joint density) of getting students with scores of 23, 16, 5, 14, 7.\n\n\n\n\n\nCheck your learning: Using the principle of maximum likelihood, the best estimate for a parameter is one that\n\n\nresults in the least squared erroris the sample meanmaximizes the log-likelihood function\n\n\n function validate_form_72330() {var x, text; var x = document.forms[\"form_72330\"][\"answer_72330\"].value;if (x == \"maximizes the log-likelihood function\"){text = 'Nice job :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_72330').innerHTML = text; return false;} \nThought exercise: Because a probability is less than 1, the logarithm of it will be a negative number. By that logic, if the log-likelihood is -16.5 with \\(N = 5\\), what should it be with a larger sample size (e.g., \\(N = 50\\))?\n\n\nmore negative (e.g., -160.5)unchanged (i.e., -16.5more positive (e.g., -1.65)\n\n\n function validate_form_8789() {var x, text; var x = document.forms[\"form_8789\"][\"answer_8789\"].value;if (x == \"more negative (e.g., -160.5)\"){text = 'Nice job :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_8789').innerHTML = text; return false;} \nMore about maximum likelihood estimation\nIf \\(\\sigma\\) is not known, the maximum likelihood estimate is \\[\\hat \\sigma = \\sqrt{\\frac{\\sum_{i = 1}^N (Y_i - \\bar X)^2}{N}},\\] which uses \\(N\\) in the denominator instead of \\(N - 1\\). Because of this, in small sample, maximum likelihood estimate tends to be biased, meaning that on average it tends to underestimate the population variance.\nOne useful property of maximum likelihood estimation is that the standard error can be approximated by the inverse of the curvature of the likelihood function at the peak. The two graphs below show that with a larger sample, the likelihood function has a higher curvature (i.e., steeper around the peak), which results in a smaller estimated standard error.\n\n\n\nEstimation methods for MLM\n\n\n\n\n\nCheck your learning: The deviance is\n\n\n2 x likelihood2 x log-likelihood-2 x log-likelihood\n\n\n function validate_form_5777() {var x, text; var x = document.forms[\"form_5777\"][\"answer_5777\"].value;if (x == \"-2 x log-likelihood\"){text = 'Nice job :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_5777').innerHTML = text; return false;} \nTesting\nLikelihood ratio test (LRT) for fixed effects\n\n\n\n\n\nThe LRT has been used widely across many statistical methods, so it’s useful to get familiar to doing it by hand (as it may not be available in all software in all procedures).\nPractice yourself: Consider the two models below\n\n\n\n\nModel 1\n\n\nModel 2\n\n\n(Intercept)\n\n\n12.650\n\n\n12.662\n\n\n\n\n(0.148)\n\n\n(0.148)\n\n\nmeanses\n\n\n5.863\n\n\n3.674\n\n\n\n\n(0.359)\n\n\n(0.375)\n\n\nsd__(Intercept)\n\n\n1.610\n\n\n1.627\n\n\nsd__Observation\n\n\n6.258\n\n\n6.084\n\n\nses\n\n\n\n\n2.191\n\n\n\n\n\n\n(0.109)\n\n\nAIC\n\n\n46967.1\n\n\n46573.8\n\n\nBIC\n\n\n46994.6\n\n\n46608.2\n\n\nLog.Lik.\n\n\n−23479.554\n\n\n−23281.902\n\n\nUsing R and the pchisq() function, what is the \\(\\chi^2\\) (or X2) test statistic and the \\(p\\) value for the fixed effect coefficient for ses?\n\n\nX2 = 395.3, df = 1, p < .001X2 = 198, df = 1, p < .001F = 20.1, df = 1, p < .001\n\n\n function validate_form_47043() {var x, text; var x = document.forms[\"form_47043\"][\"answer_47043\"].value;if (x == \"X2 = 395.3, df = 1, p < .001\"){text = 'Nice job :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_47043').innerHTML = text; return false;} \n\\(F\\) test with small-sample correction\n\n\n\n\n\nCheck your learning: From the results below, what is the test statistic and the \\(p\\) value for the fixed effect coefficient for meanses?\n\nType III Analysis of Variance Table with Kenward-Roger's method\n         Sum Sq Mean Sq NumDF  DenDF F value    Pr(>F)    \nmeanses  324.39  324.39     1  15.51  9.9573  0.006317 ** \nses     1874.34 1874.34     1 669.03 57.5331 1.116e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nF(1) = 57.53, p < .001X2 = 324.39, df = 15.51, p = .006F(1, 15.51) = 9.96, p = .006\n\n\n function validate_form_38879() {var x, text; var x = document.forms[\"form_38879\"][\"answer_38879\"].value;if (x == \"F(1, 15.51) = 9.96, p = .006\"){text = 'Nice job :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_38879').innerHTML = text; return false;} \nFor more information on REML and K-R, check out\nMcNeish, D. (2017). Small sample methods for multilevel modeling: A colloquial elucidation of REML and the Kenward-Roger correction.\nLRT for random slope variance\n\n\n\n\n\nCheck your learning: When testing whether the variance of a random slope term is zero, what needs to be done?\n\n\nThe p value needs to be divided by 2, because the random slope variance can only be zero or positiveThe p value needs to be divided by 2 so that the results will be a two-tailed testThe p value needs to be divided by 2, because it is a two degrees of freedom test\n\n\n function validate_form_45586() {var x, text; var x = document.forms[\"form_45586\"][\"answer_45586\"].value;if (x == \"The p value needs to be divided by 2, because the random slope variance can only be zero or positive\"){text = 'Nice job :+1:';} else {text = 'Try again. You can do it!';} document.getElementById('result_45586').innerHTML = text; return false;} \nMultilevel bootstrap\n\n\n\n\n\n\n\n\n",
      "last_modified": "2021-09-19T11:52:36-07:00"
    }
  ],
  "collections": []
}
