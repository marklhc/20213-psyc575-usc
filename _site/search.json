{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-08-29T15:16:52-07:00"
    },
    {
      "path": "index.html",
      "title": "PSYC 575 Multilevel Modeling (2021 Fall)",
      "description": "Welcome to the website. Let's have a great semester!\n",
      "author": [],
      "contents": "\nThis is the course website for PSYC 575 in the 2021 Fall semester. The course follows a flipped course model so that students will review assigned readings and prerecorded lecture videos at their own time, while the class meeting time will be dedicated to Q&A and exercises. You will find the syllabus, lecture materials, and R codes in the navigation bar in the top right corner. Homework submission will be through Blackboard.\nThe lecture videos are licensed under the standard YouTube License\nCode content is licensed under the MIT license\nOther course materials are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\n\n\n\n",
      "last_modified": "2021-09-05T14:47:48-07:00"
    },
    {
      "path": "rcode1.html",
      "title": "R Codes (Week 1)",
      "author": [],
      "contents": "\n\nContents\nRStudio Intro\nRecommended Options\n\nR Markdown\nYAML options\nChunk options\n\nIncluding Plots\nInstall packages\nLoad a package\nData Frame\nBasic Markdown Elements\nItalic and bolded texts\nLists (Ordered and Unordered)\nEquations (LaTeX)\n\nCheatsheet\nExercise\n\nClick here to download the Rmd file: week1-intro-RMarkdown.Rmd\n\n\n\n\n\n\n\n\n\n\nRStudio Intro\n\n\nprint(\"This is Thursday.\")\n\n\n[1] \"This is Thursday.\"\n\n\n\n\n\nIf you’re new to R, follow the online ModernDrive chapter https://moderndive.netlify.app/1-getting-started.html\nFour Panes:\nSource panel\nConsole\nGlobal environment\nFiles/Plots/Packages/Help/Viewer\n\nRecommended Options\nTools --> Global Options --> \n- Uncheck \"Restore .RData into workspace at startup\".\n- Set \"Save workspace to .RData on exit\" to \"Never\".  \n- (Optional) R Markdown --> Set \"Show output preview in\" to \"Viewer Pane\".\n\nUse Project\nR Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nNote: R is case sensitive. So Cars and cars are different.\n\n\n\nsummary(cars[-(1:4), ])\n\n\n\nYAML options\ntitle, author, date, output\nChunk options\ninclude\necho\neval\nresults = 'hide'\nIncluding Plots\nYou can also embed plots, for example:\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\nInstall packages\n\n\ntweetrmd::include_tweet(\"https://twitter.com/visnut/status/1248087845589274624\")\n\n\nStudents struggle with install vs library every semester, and keep including their install code in the RMarkdown report. Hope this meme might help in the future #rstats pic.twitter.com/I8xg053y0x— Dr Di Cook (@visnut) April 9, 2020\n\n\n\n\n\nCheck out https://rladiessydney.org/courses/ryouwithme/01-basicbasics-2/\n\n\n# Install the tidyverse \"meta\" package\n# install.packages(\"tidyverse\")\n# Install the lme4 package\n# install.packages(\"lme4\")\n\n\n\nLoad a package\n\nTip 1: Try the (PC) Ctrl + Alt + I/(Mac) Cmd + Option + I shortcut for a new code chunk\n\n\nTip 2: Use Tab for code completeion\n\n\nTip 3: Use Ctrl + Enter/(Mac) Cmd + Return for running a line of R code\n\n\nTip 4: Set message = FALSE to suppress messages in loading packages\n\n\n\n# Load tidyverse\n\n# Load lme4\n\n\n\nData Frame\n\n\n# Load SleepStudy\n\n# Extract one column\n\n# Extract column by index (same as last line)\n\n# Extract two rows\n\n# Compute the mean and sd, and chain them together\n\n# Correlation matrix with psych::pairs.panel()\n\n# Find out what a function does (use `?function_name`, e.g., `?pairs.panel`)\n\n\n\nBasic Markdown Elements\nFrom RStudio, click Help –> Markdown Quick Reference\nItalic and bolded texts\nThis is italic\nLists (Ordered and Unordered)\nItem 1\nItem 2\nEquations (LaTeX)\nInline: The correlation between \\(a = b + c + \\tau\\)\nDisplay:\n\\[a = b + c + \\tau\\]\nCheatsheet\nMore detailed cheatsheet: https://rmarkdown.rstudio.com/lesson-15.HTML\nExercise\nDownload the Rmd file for the exercise on Blackboard\nInsert your name on line 3.\nComplete the following in this R Markdown document:\nCopy the following LaTex equation to below: A_1 = \\pi r^2. How does this say about writing Greek letters and subscripts/superscripts? \\[[Insert equation here]\\]\nInstall and then load the modelsummary package, and run the following. You’ll need to remove eval = FALSE so that it runs. Find out what this code chunk does.\n\n\n# Install and load the modelsummary package first; otherwise it won't run\n library(modelsummary)\n fm1 <- lm(dist ~ speed, data = cars)\n fm2 <- lm(dist ~ poly(speed, 2), data = cars)\n fm3 <- lm(log(dist) ~ log(speed), data = cars)\n msummary(list(fm1, fm2, fm3))\n \n\n\nRun the following. You’ll need to remove eval = FALSE so that it runs. Find out what this code chunk does.\n\n\nggplot(cars, aes(x = log(speed), y = log(dist))) +\n   geom_point() +\n   geom_smooth()\n \n\n\nAdd a code chunk below to show the output of running sessionInfo(), which prints out the session information of your computer. Make the code chunk to show only the output, but not the code.\nKnit the document to HTML, PDF, and Word. If you run into an error when knitting to any one of the formats, record the error message. Which format do you prefer?\nGo to the top of this Rmd file, and change the line inside YAML\n  html_document: default\nto\n  html_document: \n      toc: TRUE\nKnit the document again. What does it do?\nSubmit the knitted document to Blackboard in your preferred format (HTML, PDF, or WORD)\n\n\n",
      "last_modified": "2021-08-29T15:16:54-07:00"
    },
    {
      "path": "rcode2.html",
      "title": "R Codes (Week 2)",
      "author": [],
      "contents": "\n\nContents\nSimulation\nLoad Packages and Import Data\nImport Data\n\nQuick Scatterplot Matrix\n1. Linear Regression of salary on pub\nVisualize the data\nLinear regression\nVisualize fitted regression line:\nConfidence intervals\nInterpretations\nSimulations\n\\(p\\) values\nCentering\n\n2. Categorical Predictor\nEquivalence to the \\(t\\)-test\n\n3. Multiple Predictors (Multiple Regression)\nPlotting\nInterpretations\nDiagnostics\nEffect size\n\n4. Interaction\nInteraction Plots\n\n5. Tabulate the Regression Results\nBonus: Matrix Form of Regression\nBonus: More Options in Formatting Tables\n\n\n\\[\n\\newcommand{\\bv}[1]{\\boldsymbol{\\mathbf{#1}}}\n\\]\n\nClick here to download the Rmd file: week2-review-multiple-regression.Rmd\nSimulation\n\n\n\n\n\nPractice yourself: From the simulation code provided, try to increase the sample size from 62 to something larger, like 200. How does this affect the uncertainty in the sample slopes?\nLoad Packages and Import Data\nYou can use add the message=FALSE option to suppress the package loading messages\n\n\n# To install a package, run the following ONCE (and only once on your computer)\n# install.packages(\"psych\")  \nlibrary(psych)  # for scatterplot matrix\nlibrary(here)  # makes reading data more consistent\nlibrary(tidyverse)  # for data manipulation and plotting\nlibrary(car)  # some useful functions for regression\nlibrary(modelsummary)  # for making tables\nlibrary(sjPlot)  # for plotting slopes\nlibrary(interactions)  # for plotting interactions\ntheme_set(theme_bw())  # Theme; just my personal preference\n\n\n\nImport Data\nFirst, download the data file salary.txt from https://raw.githubusercontent.com/marklhc/marklai-pages/master/data_files/salary.txt, and import the data. A robust way to do so is to download the data to a folder called data_files under the project directory, and then use the here package. This avoids a lot of data import issues that I’ve seen.\n\n\n# The `here()` function forces the use of the project directory\nhere(\"data_files\", \"salary.txt\")\n# Read in the data\nsalary_dat <- read.table(here(\"data_files\", \"salary.txt\"), header = TRUE)\n\n\n\nAlternatively, from the menu, click File → Import Dataset → From Text (base)..., and select the file.\n\n\n# Show the data\nsalary_dat\n\n\n>#    id time pub sex citation salary\n># 1   1    3  18   1       50  51876\n># 2   2    6   3   1       26  54511\n># 3   3    3   2   1       50  53425\n># 4   4    8  17   0       34  61863\n># 5   5    9  11   1       41  52926\n># 6   6    6   6   0       37  47034\n># 7   7   16  38   0       48  66432\n># 8   8   10  48   0       56  61100\n># 9   9    2   9   0       19  41934\n># 10 10    5  22   0       29  47454\n># 11 11    5  30   1       28  49832\n># 12 12    6  21   0       31  47047\n># 13 13    7  10   1       25  39115\n># 14 14   11  27   0       40  59677\n># 15 15   18  37   0       61  61458\n># 16 16    6   8   0       32  54528\n># 17 17    9  13   1       36  60327\n># 18 18    7   6   0       69  56600\n># 19 19    7  12   1       47  52542\n># 20 20    3  29   1       29  50455\n># 21 21    7  29   1       35  51647\n># 22 22    5   7   0       35  62895\n># 23 23    7   6   0       18  53740\n># 24 24   13  69   0       90  75822\n># 25 25    5  11   0       60  56596\n># 26 26    8   9   1       30  55682\n># 27 27    8  20   1       27  62091\n># 28 28    7  41   1       35  42162\n># 29 29    2   3   1       14  52646\n># 30 30   13  27   0       56  74199\n># 31 31    5  14   0       50  50729\n># 32 32    3  23   0       25  70011\n># 33 33    1   1   0       35  37939\n># 34 34    3   7   0        1  39652\n># 35 35    9  19   0       69  68987\n># 36 36    3  11   0       69  55579\n># 37 37    9  31   0       27  54671\n># 38 38    3   9   0       50  57704\n># 39 39    4  12   1       32  44045\n># 40 40   10  32   0       33  51122\n># 41 41    1  26   0       45  47082\n># 42 42   11  12   0       54  60009\n># 43 43    5   9   0       47  58632\n># 44 44    1   6   0       29  38340\n># 45 45   21  39   0       69  71219\n># 46 46    7  16   1       47  53712\n># 47 47    5  12   1       43  54782\n># 48 48   16  50   0       55  83503\n># 49 49    5  18   0       33  47212\n># 50 50    4  16   1       28  52840\n># 51 51    5   5   0       42  53650\n># 52 52   11  20   0       24  50931\n># 53 53   16  50   1       31  66784\n># 54 54    3   6   1       27  49751\n># 55 55    4  19   1       83  74343\n># 56 56    4  11   1       49  57710\n># 57 57    5  13   0       14  52676\n># 58 58    6   3   1       36  41195\n># 59 59    4   8   1       34  45662\n># 60 60    8  11   1       70  47606\n># 61 61    3  25   1       27  44301\n># 62 62    4   4   1       28  58582\n\nYou can see the description of the variables here: https://rdrr.io/cran/MBESS/man/prof.salary.html\nQuick Scatterplot Matrix\nImport to screen your data before any statistical modeling\n\n\npairs.panels(salary_dat[ , -1],  # not plotting the first column\n             ellipses = FALSE)\n\n\n\n\n1. Linear Regression of salary on pub\nVisualize the data\n\n\n# Visualize the data (\"gg\" stands for grammar of graphics)\np1 <- ggplot(salary_dat,  # specify data\n             # aesthetics: mapping variable to axes)\n             aes(x = pub, y = salary)) +  \n             # geom: geometric objects, such as points, lines, shapes, etc\n             geom_point()\n# Add a smoother geom to visualize mean salary as a function of pub\np1 + geom_smooth()\n\n\n\n\nA little bit of non-linearity on the plot. Now fit the regression model\nLinear regression\nYou can type equations (with LaTeX; see a quick reference).\nP.S. Use \\text{} to specify variable names\nP.S. Pay attention to the subscripts\n\\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}_i + e_i\\]\nOutcome: salary\nPredictor: pub\n\\(\\beta_0\\): regression intercept\n\\(\\beta_1\\): regression slope\n\\(e\\): error\n\n\n# left hand side of ~ is outcome; right hand side contains predictors\n# salary ~ (beta_0) * 1 + (beta_1) * pub\n# remove beta_0 and beta_1 to get the formula\nm1 <- lm(salary ~ 1 + pub, data = salary_dat)\n# In R, the output is not printed out if it is saved to an object (e.g., m1).\n# Summary:\nsummary(m1)\n\n\n># \n># Call:\n># lm(formula = salary ~ 1 + pub, data = salary_dat)\n># \n># Residuals:\n>#      Min       1Q   Median       3Q      Max \n># -20660.0  -7397.5    333.7   5313.9  19238.7 \n># \n># Coefficients:\n>#             Estimate Std. Error t value Pr(>|t|)    \n># (Intercept) 48439.09    1765.42  27.438  < 2e-16 ***\n># pub           350.80      77.17   4.546 2.71e-05 ***\n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 8440 on 60 degrees of freedom\n># Multiple R-squared:  0.2562, Adjusted R-squared:  0.2438 \n># F-statistic: 20.67 on 1 and 60 DF,  p-value: 2.706e-05\n\nVisualize fitted regression line:\n\n\np1 +\n  # Non-parametric fit\n  geom_smooth(se = FALSE) +\n  # Linear regression line (in red)\n  geom_smooth(method = \"lm\", col = \"red\")\n\n\n\n\nConfidence intervals\n\n\n# Confidence intervals\nconfint(m1)\n\n\n>#                 2.5 %     97.5 %\n># (Intercept) 44907.729 51970.4450\n># pub           196.441   505.1625\n\nInterpretations\n\nBased on our model, faculty with one more publication have predicted salary of $350.8, 95% CI [$196.4, $505.2], higher than those with one less publication.\n\nBut what do the confidence intervals and the standard errors mean? To understanding what exactly a regression model is, let’s run some simulations.\nSimulations\nBefore you go on, take a look on a brief introductory video by Clark Caylord on Youtube on simulating data based on a simple linear regression model.\nTo my delight, I also found out a gentle introduction of simulation method by one of our clinical science students at USC (Thanks Kayla!). You can access it through the USC library here (Sign-on required). Here’s the citation:\nTureson, K. & Odland, A. (2018). Monte Carlo simulation studies. In B. Frey (Ed.),The SAGE Encyclopedia of Educational Research, Measurement, and Evaluation. Thousand Oaks, CA: SAGE Publications.\nBased on the analyses, the sample regression line is \\[\\widehat{\\text{salary}} = 48439.09 + 350.80 \\text{pub}.\\] The numbers are only sample estimates as there are sampling errors. However, if we assume that this line truly describe the relation between salary and pub, then we can simulate some fake data, which represents what we could have obtained in a different sample.\nGoing back to the equation \\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}_i + e_i,\\] the only thing that changes across different samples, based on the statistical model, is \\(e_i\\). Conceptually, you can think of the error term \\(e_i\\) as the deviation of person \\(i\\)’s salary from the mean salary of everyone in the population who has the same number of publications as person \\(i\\). Here we assume that \\(e_i\\) is normally distributed, written as \\(e_i \\sim N(0, \\sigma)\\), where \\(\\sigma\\) describes the conditional standard deviation (i.e., the standard deviation across individuals who have the same number of publications). From the regression results, \\(\\sigma\\) is estimated as the Residual standard error = 8,440.\nBased on these model assumptions, we can imagine a large population, say with 10,000 people\n\n\n# Simulating a large population. The code in this chunk \n# is not essential for conceptual understanding\nNpop <- 10000\nbeta0 <- 48439.09\nbeta1 <- 350.80\nsigma <- 8440\n# Simulate population data\nsimulated_population <- tibble(\n  # Simulate population pub\n  pub = local({\n    dens <- density(c(-salary_dat$pub, salary_dat$pub),\n      bw = \"SJ\",\n      n = 1024\n    )\n    dens_x <- c(0, dens$x[dens$x > 0])\n    dens_y <- c(0, dens$y[dens$x > 0])\n    round(\n      approx(\n        cumsum(dens_y) / sum(dens_y),\n        dens_x,\n        runif(Npop)\n      )$y\n    )\n  }),\n  # Simulate error\n  e = rnorm(Npop, mean = 0, sd = sigma)\n)\n# Compute salary variable\nsimulated_population$salary <-\n  beta0 + beta1 * simulated_population$pub + simulated_population$e\n# Plot\np_pop <- ggplot(\n  data = simulated_population,\n  aes(x = pub, y = salary)\n) +\n  geom_point(alpha = 0.1) +\n  # Add population regression line in blue\n  geom_smooth(se = FALSE, method = \"lm\")\np_pop\n\n\n\n\nNow, we can simulate some fake (but plausible) samples. An important thing to remember is we want to simulate data that have the same size as the original sample, because we’re comparing to other plausible samples with equal size. In R there is a handy simulate() function to do that.\n\n\nsimulated_salary <- simulate(m1)\n# Add simulated salary to the original data\n# Note: the simulated variable is called `sim_1`\nsim_data1 <- bind_cols(salary_dat, simulated_salary)\n# Show the first six rows\nhead(sim_data1)\n\n\n>#   id time pub sex citation salary    sim_1\n># 1  1    3  18   1       50  51876 51047.35\n># 2  2    6   3   1       26  54511 47074.54\n># 3  3    3   2   1       50  53425 49010.13\n># 4  4    8  17   0       34  61863 64235.96\n># 5  5    9  11   1       41  52926 49844.51\n># 6  6    6   6   0       37  47034 51161.96\n\n# Plot the data (add on to the population)\np_pop + \n  geom_point(data = sim_data1, \n             aes(x = pub, y = sim_1),\n             col = \"red\") + \n  # Add sample regression line\n  geom_smooth(data = sim_data1, \n              aes(x = pub, y = sim_1),\n              method = \"lm\", se = FALSE, col = \"red\")\n\n\n\n\n\n\n# To be more transparent, here's what the simulate() function essentially is\n# doing\nsample_size <- nrow(salary_dat)\nsim_data1 <- tibble(\n  pub = salary_dat$pub,\n  # simulate error\n  e = rnorm(sample_size, mean = 0, sd = sigma)\n)\n# Compute new salary data\nsim_data1$sim_1 <- beta0 + beta1 * sim_data1$pub + sim_data1$e\n\n\n\nAs you can see, the sample regression line in red is different from the blue line.\nDrawing 100 samples\nLet’s draw more samples\n\n\nnum_samples <- 100\nsimulated_salary100 <- simulate(m1, nsim = num_samples)\n# Form a giant data set with 100 samples\nsim_data100 <- bind_cols(salary_dat, simulated_salary100) %>%\n  pivot_longer(sim_1:sim_100,\n    names_prefix = \"sim_\",\n    names_to = \"sim\",\n    values_to = \"simulated_salary\"\n  )\n# Plot by samples\np_sim100 <- ggplot(\n  sim_data100,\n  aes(x = pub, y = simulated_salary, group = sim)\n) +\n  geom_point(col = \"red\", alpha = 0.1) +\n  geom_smooth(col = \"red\", se = FALSE, method = \"lm\")\n# Use gganimate (this takes some time to render)\nlibrary(gganimate)\np_sim100 + transition_states(sim) +\n  ggtitle(\"Simulation {frame} of {nframes}\")\nanim_save(here::here(\"sim-100-samples.gif\"))\n\n\n\n\nWe can show the regression lines for all 100 samples\n\n\nggplot(sim_data100,\n       aes(x = pub, y = simulated_salary, group = sim)) +\n  stat_smooth(\n    geom = \"line\",\n    col = \"red\",\n    se = FALSE,\n    method = \"lm\",\n    alpha = 0.4\n  )\n\n\n\n\nThe confidence intervals of the intercept and the slope how much uncertainty there is.\n\\(p\\) values\nNow, we can also understand the \\(p\\) value for the regression slope of pub is. Remember that the \\(p\\) value is the probability that, if the regression slope of pub is zero (i.e., no association), how likely/unlikely would we get the sample slope (i.e., 350.80) in our data. So now we’ll repeat the simulation, but without pub as a predictor (i.e., assuming \\(\\beta_1 = 0\\)).\n\n\nnum_samples <- 100\nm0 <- lm(salary ~ 1, data = salary_dat)  # null model\nsimulated_salary100_null <- simulate(m0, nsim = num_samples)\n# Form a giant data set with 100 samples\nsim_null100 <- bind_cols(salary_dat, simulated_salary100_null) %>% \n  pivot_longer(sim_1:sim_100, \n               names_prefix = \"sim_\", \n               names_to = \"sim\", \n               values_to = \"simulated_salary\")\n# Show the null slopes\nggplot(data = salary_dat,\n       aes(x = pub, y = salary)) +\n  stat_smooth(\n    data = sim_null100,\n    aes(x = pub, y = simulated_salary, group = sim),\n    geom = \"line\",\n    col = \"darkgrey\",\n    se = FALSE,\n    method = \"lm\"\n  ) +\n  geom_smooth(method = \"lm\", col = \"red\", se = FALSE) +\n  geom_point()\n\n\n\n\nSo you can see the sample slope is larger than what you would expect to see if the true slope is zero. So the \\(p\\) value is very small, and the result is statistically significant (at, say, .05 level).\nCentering\nSo that the intercept refers to a more meaningful value. It’s a major issue in multilevel modeling.\n\\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}^c_i + e_i\\]\n\n\n# Using pipe operator\nsalary_dat <- salary_dat %>% \n  mutate(pub_c = pub - mean(pub))\n# Equivalent to:\n# salary_dat <- mutate(salary_dat, \n#                      pub_c = pub - mean(pub))\nm1c <- lm(salary ~ pub_c, data = salary_dat)\nsummary(m1c)\n\n\n># \n># Call:\n># lm(formula = salary ~ pub_c, data = salary_dat)\n># \n># Residuals:\n>#      Min       1Q   Median       3Q      Max \n># -20660.0  -7397.5    333.7   5313.9  19238.7 \n># \n># Coefficients:\n>#             Estimate Std. Error t value Pr(>|t|)    \n># (Intercept) 54815.76    1071.93  51.137  < 2e-16 ***\n># pub_c         350.80      77.17   4.546 2.71e-05 ***\n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 8440 on 60 degrees of freedom\n># Multiple R-squared:  0.2562, Adjusted R-squared:  0.2438 \n># F-statistic: 20.67 on 1 and 60 DF,  p-value: 2.706e-05\n\nThe only change is the intercept coefficient\n\n\np1 + \n  geom_smooth(method = \"lm\", col = \"red\") + \n  # Intercept without centering\n  geom_vline(aes(col = \"Not centered\", xintercept = 0)) + \n  # Intercept with centering\n  geom_vline(aes(col = \"Centered\", xintercept = mean(salary_dat$pub))) + \n  labs(col = \"\")\n\n\n\n\n2. Categorical Predictor\nRecode sex as factor variable in R (which allows R to automatically do dummy coding). This should be done in general for categorical predictors.\n\n\nsalary_dat <- salary_dat %>% \n  mutate(sex = factor(sex, levels = c(0, 1), \n                      labels = c(\"male\", \"female\")))\n\n\n\n\\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{sex}_i + e_i\\]\n\n\n(p2 <- ggplot(salary_dat, aes(x = sex, y = salary)) + \n    geom_boxplot() + \n    geom_jitter(height = 0, width = 0.1))  # move the points to left/right a bit\n\n\n\n\n\n\nm2 <- lm(salary ~ sex, data = salary_dat)\nsummary(m2)\n\n\n># \n># Call:\n># lm(formula = salary ~ sex, data = salary_dat)\n># \n># Residuals:\n>#    Min     1Q Median     3Q    Max \n># -18576  -5736    -19   4853  26988 \n># \n># Coefficients:\n>#             Estimate Std. Error t value Pr(>|t|)    \n># (Intercept)    56515       1620  34.875   <2e-16 ***\n># sexfemale      -3902       2456  -1.589    0.117    \n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 9587 on 60 degrees of freedom\n># Multiple R-squared:  0.04039,    Adjusted R-squared:  0.02439 \n># F-statistic: 2.525 on 1 and 60 DF,  p-value: 0.1173\n\nThe (Intercept) coefficient is for the ‘0’ category, i.e., predicted salary for males; the female coefficient is the difference between males and females.\nPredicted female salary = 56515 + (-3902) = 52613.\nEquivalence to the \\(t\\)-test\nWhen assuming homogeneity of variance\n\n\nt.test(salary ~ sex, data = salary_dat, var.equal = TRUE)\n\n\n># \n>#  Two Sample t-test\n># \n># data:  salary by sex\n># t = 1.5891, df = 60, p-value = 0.1173\n># alternative hypothesis: true difference in means between group male and group female is not equal to 0\n># 95 percent confidence interval:\n>#  -1009.853  8814.041\n># sample estimates:\n>#   mean in group male mean in group female \n>#             56515.06             52612.96\n\n3. Multiple Predictors (Multiple Regression)\nNow add one more predictor, time \\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}^c_i + \\beta_2 \\text{time}_i + e_i\\]\n\n\nggplot(salary_dat, aes(x = time, y = salary)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\n\n\nm3 <- lm(salary ~ pub_c + time, data = salary_dat)\nsummary(m3)  # summary\n\n\n># \n># Call:\n># lm(formula = salary ~ pub_c + time, data = salary_dat)\n># \n># Residuals:\n>#    Min     1Q Median     3Q    Max \n># -15919  -5537   -985   4861  22476 \n># \n># Coefficients:\n>#             Estimate Std. Error t value Pr(>|t|)    \n># (Intercept) 47373.38    2281.78  20.762  < 2e-16 ***\n># pub_c         133.00      92.73   1.434 0.156797    \n># time         1096.03     303.58   3.610 0.000632 ***\n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 7703 on 59 degrees of freedom\n># Multiple R-squared:  0.3908, Adjusted R-squared:  0.3701 \n># F-statistic: 18.92 on 2 and 59 DF,  p-value: 4.476e-07\n\nconfint(m3)  # confidence interval\n\n\n>#                   2.5 %     97.5 %\n># (Intercept) 42807.55413 51939.2044\n># pub_c         -52.56216   318.5589\n># time          488.56251  1703.4921\n\nThe regression coefficients are the partial effects.\n\nPlotting\nThe sjPlot::plot_model() function is handy\n\n\nsjPlot::plot_model(m3, type = \"pred\", show.data = TRUE,\n                   title = \"\")  # remove title\n\n\n># $pub_c\n\n># \n># $time\n\n\nInterpretations\n\nFaculty who have worked longer tended to have more publications For faculty who graduate around the same time, a difference of 1 publication is associated with an estimated difference in salary of $133.0, 95% CI [$-52.6, $318.6], which was not significant.\n\nDiagnostics\n\n\ncar::mmps(m3)  # marginal model plots for linearity assumptions\n\n\n\n\nThe red line is the implied association based on the model, whereas the blue line is a non-parametric smoother not based on the model. If the two lines show big discrepancies (especially if in the middle), it may suggest the linearity assumptions in the model does not hold.\nEffect size\n\n\n# Extract the R^2 number (it's sometimes tricky to \n# figure out whether R stores the numbers you need)\nsummary(m3)$r.squared\n\n\n># [1] 0.3907761\n\n# Adjusted R^2\nsummary(m3)$adj.r.squared\n\n\n># [1] 0.3701244\n\nProportion of predicted variance: \\(R^2\\) = 39%, adj. \\(R^2\\) = 37%.\n4. Interaction\nFor interpretation purposes, it’s recommended to center the predictors (at least the continuous ones)\n\\[\\text{salary}_i = \\beta_0 + \\beta_1 \\text{pub}^c_i + \\beta_2 \\text{time}^c_i + \\beta_3 (\\text{pub}^c_i)(\\text{time}^c_i) + e_i\\]\n\n\nsalary_dat <- salary_dat %>% \n  mutate(time_c = time - mean(time))\n# Fit the model with interactions:\nm4 <- lm(salary ~ pub_c * time_c, data = salary_dat)\nsummary(m4)  # summary\n\n\n># \n># Call:\n># lm(formula = salary ~ pub_c * time_c, data = salary_dat)\n># \n># Residuals:\n>#    Min     1Q Median     3Q    Max \n># -14740  -5305   -373   4385  22744 \n># \n># Coefficients:\n>#              Estimate Std. Error t value Pr(>|t|)    \n># (Intercept)  54238.08    1183.01  45.847  < 2e-16 ***\n># pub_c          104.72      98.41   1.064  0.29169    \n># time_c         964.17     339.68   2.838  0.00624 ** \n># pub_c:time_c    15.07      17.27   0.872  0.38664    \n># ---\n># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n># \n># Residual standard error: 7719 on 58 degrees of freedom\n># Multiple R-squared:  0.3987, Adjusted R-squared:  0.3676 \n># F-statistic: 12.82 on 3 and 58 DF,  p-value: 1.565e-06\n\nInteraction Plots\nInterpreting interaction effects is hard. Therefore,\n\nAlways plot the interaction to understand the dynamics\n\n\n\ninteractions::interact_plot(m4,\n  pred = \"pub_c\",\n  modx = \"time_c\",\n  # Insert specific values to plot the slopes.\n  # Pay attention that `time_c` has been centered\n  modx.values = c(1, 7, 15) - 6.79,\n  modx.labels = c(1, 7, 15),\n  plot.points = TRUE,\n  x.label = \"Number of publications (mean-centered)\",\n  y.label = \"Salary\",\n  legend.main = \"Time since Ph.D.\"\n)\n\n\n\n\nAnother approach is to plug in numbers to the equation: \\[\\widehat{\\text{salary}} = \\hat \\beta_0 + \\hat \\beta_1 \\text{pub}^c + \\hat \\beta_2 \\text{time}^c + \\hat \\beta_3 (\\text{pub}^c)(\\text{time}^c)\\] For example, consider people who’ve graduated for seven years, i.e., time = 7. First, be careful that in the model we have time_c, and time = 7 corresponds to time_c = 0.2096774 years. So if we plug that into the equation, \\[\\widehat{\\text{salary}} |_{\\text{time} = 7} = \\hat \\beta_0 + \\hat \\beta_1 \\text{pub}^c + \\hat \\beta_2 (0.21) + \\hat \\beta_3 (\\text{pub}^c)(0.21)\\] Combining terms with pubc, \\[\\widehat{\\text{salary}} |_{\\text{time} = 7} = [\\hat \\beta_0 + \\hat \\beta_2 (0.21)] + [\\hat \\beta_1 + \\hat \\beta_3 (0.21)] (\\text{pub}^c)\\] Now plug in the numbers for \\(\\hat \\beta_0\\), \\(\\hat \\beta_1\\), \\(\\hat \\beta_2\\), \\(\\hat \\beta_3\\),\n\n\n# beta0 + beta2 * 0.21\n54238.08 + 964.17 * 0.21\n\n\n># [1] 54440.56\n\n# beta1 + beta3 * 0.21\n104.72 + 15.07 * 0.21\n\n\n># [1] 107.8847\n\nresulting in \\[\\widehat{\\text{salary}} |_{\\text{time} = 7} = 54440.6 + 107.9 (\\text{pub}^c), \\] which is the regression line for time = 7. Note, however, when an interaction is present, the regression slope will be different with a different value of time. So remember that\n\nAn interaction means that the regression slope of a predictor depends on another predictor.\n\nWe will further explore this in the class exercise this week.\n5. Tabulate the Regression Results\n\n\nmsummary(list(\n  \"M1\" = m1,\n  \"M2\" = m2,\n  \"M3\" = m3,\n  \"M3 + Interaction\" = m4\n),\nfmt = \"%.1f\"\n) # keep one digit\n\n\n\n\n\nM1\n\n\nM2\n\n\nM3\n\n\nM3 + Interaction\n\n\n(Intercept)\n\n\n48439.1\n\n\n56515.1\n\n\n47373.4\n\n\n54238.1\n\n\n\n\n(1765.4)\n\n\n(1620.5)\n\n\n(2281.8)\n\n\n(1183.0)\n\n\npub\n\n\n350.8\n\n\n\n\n\n\n\n\n\n\n(77.2)\n\n\n\n\n\n\n\n\nsexfemale\n\n\n\n\n−3902.1\n\n\n\n\n\n\n\n\n\n\n(2455.6)\n\n\n\n\n\n\npub_c\n\n\n\n\n\n\n133.0\n\n\n104.7\n\n\n\n\n\n\n\n\n(92.7)\n\n\n(98.4)\n\n\ntime\n\n\n\n\n\n\n1096.0\n\n\n\n\n\n\n\n\n\n\n(303.6)\n\n\n\n\ntime_c\n\n\n\n\n\n\n\n\n964.2\n\n\n\n\n\n\n\n\n\n\n(339.7)\n\n\npub_c × time_c\n\n\n\n\n\n\n\n\n15.1\n\n\n\n\n\n\n\n\n\n\n(17.3)\n\n\nNum.Obs.\n\n\n62\n\n\n62\n\n\n62\n\n\n62\n\n\nR2\n\n\n0.256\n\n\n0.040\n\n\n0.391\n\n\n0.399\n\n\nR2 Adj.\n\n\n0.244\n\n\n0.024\n\n\n0.370\n\n\n0.368\n\n\nAIC\n\n\n1301.0\n\n\n1316.8\n\n\n1290.6\n\n\n1291.8\n\n\nBIC\n\n\n1307.4\n\n\n1323.1\n\n\n1299.1\n\n\n1302.4\n\n\nLog.Lik.\n\n\n−647.486\n\n\n−655.383\n\n\n−641.299\n\n\n−640.895\n\n\nF\n\n\n20.665\n\n\n2.525\n\n\n18.922\n\n\n12.817\n\n\nBonus: Matrix Form of Regression\nThe regression model can be represented more succintly in matrix form: \\[\\bv y = \\bv X \\bv \\beta + \\bv e,\\] where \\(\\bv y\\) is a column vector (which can be considered a \\(N \\times 1\\) matrix). For example, for our data\n\n\nhead(salary_dat)\n\n\n>#   id time pub    sex citation salary       pub_c     time_c\n># 1  1    3  18 female       50  51876  -0.1774194 -3.7903226\n># 2  2    6   3 female       26  54511 -15.1774194 -0.7903226\n># 3  3    3   2 female       50  53425 -16.1774194 -3.7903226\n># 4  4    8  17   male       34  61863  -1.1774194  1.2096774\n># 5  5    9  11 female       41  52926  -7.1774194  2.2096774\n># 6  6    6   6   male       37  47034 -12.1774194 -0.7903226\n\nSo \\[\\bv y = \\begin{bmatrix}\n            51,876 \\\\\n            54,511 \\\\\n            53,425 \\\\\n            \\vdots\n          \\end{bmatrix}\\] \\(\\bv X\\) is the predictor matrix (sometimes also called the design matrix), where the first column is the constant 1, and each subsequent column represent a predictor. You can see this in R\n\n\nhead(\n  model.matrix(m3)\n)\n\n\n>#   (Intercept)       pub_c time\n># 1           1  -0.1774194    3\n># 2           1 -15.1774194    6\n># 3           1 -16.1774194    3\n># 4           1  -1.1774194    8\n># 5           1  -7.1774194    9\n># 6           1 -12.1774194    6\n\nThe coefficient \\(\\bv \\beta\\) is a vector, with elements \\(\\beta_0, \\beta_1, \\ldots\\). The least square estimation method is used to find estimates of \\(\\beta\\) that minimizes the sum of squared differences between \\(\\bv y\\) and \\(\\bv X \\hat{\\bv \\beta}\\), which can be written as \\[(\\bv y - \\bv X \\bv \\beta)^\\top(\\bv y - \\bv X \\bv \\beta).\\] The above means that: for each value observation, subtract the predicted value of \\(y\\) from the observed \\(y\\) (i.e., \\(y_i - \\beta_0 + \\beta_1 x_{1i} + \\ldots\\)), then squared the value (\\([y_i - \\beta_0 + \\beta_1 x_{1i} + \\ldots]^2\\)), then sum these squared values across observations. Or sometimes you’ll see it written as \\[\\lVert\\bv y - \\bv X \\bv \\beta)\\rVert^2\\] It can be shown that the least square estimates can be obtained as \\[(\\bv X^\\top \\bv X)^{-1} \\bv X^\\top \\bv y\\] You can do the matrix form in R:\n\n\ny <- salary_dat$salary\nX <- model.matrix(m3)\n# beta = (X'X)^{-1} X'y\n# solve() is matrix inverse; t(X) is the transpose of X; use `%*%` for matrix multiplication\n(betahat <- solve(t(X) %*% X, t(X) %*% y))  # same as the coefficients in m3\n\n\n>#                   [,1]\n># (Intercept) 47373.3792\n># pub_c         132.9984\n># time         1096.0273\n\n# Sum of squared residual\nsum((y - X %*% betahat)^2)\n\n\n># [1] 3500978295\n\n# Root mean squared residual (Residual standard error)\nsqrt(sum((y - X %*% betahat)^2) / 59)  # same as in R\n\n\n># [1] 7703.156\n\nBonus: More Options in Formatting Tables\nHere’s some code you can explore to make the table output from msummary() to look more lik APA style (with an example here: https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables#regression). However, for this course I don’t recommend spending too much time on tailoring the tables; something clear and readable will be good enough.\n\n\n# Show confidence intervals and p values\nmsummary(\n  list(\n    \"Estimate\" = m4,\n    \"95% CI\" = m4,\n    \"p\" = m4\n  ),\n  estimate = c(\"estimate\", \"[{conf.low}, {conf.high}]\", \"p.value\"),\n  statistic = NULL,\n  # suppress gof indices (e.g., R^2)\n  gof_omit = \".*\",\n  # Rename the model terms (\"current name\" = \"new name\")\n  coef_rename = c(\n    \"(Intercept)\" = \"Intercept\",\n    \"pub_c\" = \"Number of publications\",\n    \"time_c\" = \"Time since PhD\",\n    \"pub_c:time_c\" = \"Publications x Time\"\n  )\n)\n\n\n\n\n\nEstimate\n\n\n95% CI\n\n\np\n\n\nIntercept\n\n\n54238.084\n\n\n[51870.023, 56606.145]\n\n\n0.000\n\n\nNumber of publications\n\n\n104.724\n\n\n[−92.273, 301.720]\n\n\n0.292\n\n\nTime since PhD\n\n\n964.170\n\n\n[284.218, 1644.122]\n\n\n0.006\n\n\nPublications x Time\n\n\n15.066\n\n\n[−19.506, 49.637]\n\n\n0.387\n\n\n\n\n\n\n",
      "last_modified": "2021-09-05T14:42:07-07:00"
    },
    {
      "path": "rcode3.html",
      "title": "R Codes (Week 3)",
      "author": [],
      "contents": "\n\nContents\nLoad Packages and Import Data\nImport Data\n\nRun the Random Intercept Model\nModel equations\nRunning the model in R\nShowing the variations across (a subset of) schools\nSimulating data based on the random intercept model\nPlotting the random effects (i.e., \\(u_{0j}\\))\nIntraclass correlations\n\nAdding Lv-2 Predictors\nModel Equation\nProportion of variance predicted\nComparing to OLS regression\n\n\n\n\\[\n\\newcommand{\\bv}[1]{\\boldsymbol{\\mathbf{#1}}}\n\\]\n\nClick here to download the Rmd file: week3-random-intercept-model.Rmd\nLoad Packages and Import Data\nYou can use the message=FALSE option to suppress the package loading messages\n\n\n# To install a package, run the following ONCE (and only once on your computer)\n# install.packages(\"psych\")  \nlibrary(here)  # makes reading data more consistent\nlibrary(tidyverse)  # for data manipulation and plotting\nlibrary(haven)  # for importing SPSS/SAS/Stata data\nlibrary(lme4)  # for multilevel analysis\nlibrary(lattice)  # for dotplot (working with lme4)\nlibrary(sjPlot)  # for plotting effects\nlibrary(MuMIn)  # for computing r-squared\nlibrary(r2mlm)  # for computing r-squared\nlibrary(broom.mixed)  # for summarizing results\nlibrary(modelsummary)  # for making tables\ntheme_set(theme_bw())  # Theme; just my personal preference\n\n\n\nIn R, there are many packages for multilevel modeling, two of the most common ones are the lme4 package and the nlme package. In this note I will show how to run different basic multilevel models using the lme4 package, which is newer. However, some of the models, like unstructured covariance structure, will need the nlme package or other packages (like the brms and the rstanarm packages with Bayesian estimation).\nImport Data\nFirst, download the data from https://github.com/marklhc/marklai-pages/raw/master/data_files/hsball.sav. We’ll import the data in .sav format using the read_sav() function from the haven package.\n\n\n# Read in the data (pay attention to the directory)\nhsball <- read_sav(here(\"data_files\", \"hsball.sav\"))\nhsball  # print the data\n\n\n># # A tibble: 7,185 × 11\n>#    id    minority female    ses mathach  size sector pracad disclim\n>#    <chr>    <dbl>  <dbl>  <dbl>   <dbl> <dbl>  <dbl>  <dbl>   <dbl>\n>#  1 1224         0      1 -1.53    5.88    842      0   0.35    1.60\n>#  2 1224         0      1 -0.588  19.7     842      0   0.35    1.60\n>#  3 1224         0      0 -0.528  20.3     842      0   0.35    1.60\n>#  4 1224         0      0 -0.668   8.78    842      0   0.35    1.60\n>#  5 1224         0      0 -0.158  17.9     842      0   0.35    1.60\n>#  6 1224         0      0  0.022   4.58    842      0   0.35    1.60\n>#  7 1224         0      1 -0.618  -2.83    842      0   0.35    1.60\n>#  8 1224         0      0 -0.998   0.523   842      0   0.35    1.60\n>#  9 1224         0      1 -0.888   1.53    842      0   0.35    1.60\n># 10 1224         0      0 -0.458  21.5     842      0   0.35    1.60\n># # … with 7,175 more rows, and 2 more variables: himinty <dbl>,\n># #   meanses <dbl>\n\nRun the Random Intercept Model\nModel equations\nLv-1: \\[\\text{mathach}_{ij} = \\beta_{0j} + e_{ij}\\] where \\(\\beta_{0j}\\) is the population mean math achievement of the \\(j\\)th school, and \\(e_{ij}\\) is the level-1 random error term for the \\(i\\)th individual of the \\(j\\)th school.\nLv-2: \\[\\beta_{0j} = \\gamma_{00} + u_{0j}\\] where \\(\\gamma_{00}\\) is the grand mean, and \\(u_{0j}\\) is the deviation of the mean of the \\(j\\)th school from the grand mean.\nRunning the model in R\nThe lme4 package require input in the format of\noutcome ~ fixed + (random | cluster ID)\nFor our data, the combined equation is \\[\\text{mathach}_{ij} = \\gamma_{00} + u_{0j} + e_{ij}, \\] which we can explicitly write \\[\\color{red}{\\text{mathach}}_{ij} = \\color{green}{\\gamma_{00} (1)} \n                                     + \\color{blue}{u_{0j} (1)}\n                                     + e_{ij}. \\] With that, we can see\noutcome = mathach,\nfixed = 1,\nrandom = 1, and\ncluster ID = id.\nThus the following syntax:\n\n\n# outcome = mathach\n# fixed = gamma_{00} * 1\n# random = u_{0j} * 1, with j indexing school id\nran_int <- lmer(mathach ~ 1 + (1 | id), data = hsball)\n# Summarize results\nsummary(ran_int)\n\n\n># Linear mixed model fit by REML ['lmerMod']\n># Formula: mathach ~ 1 + (1 | id)\n>#    Data: hsball\n># \n># REML criterion at convergence: 47116.8\n># \n># Scaled residuals: \n>#     Min      1Q  Median      3Q     Max \n># -3.0631 -0.7539  0.0267  0.7606  2.7426 \n># \n># Random effects:\n>#  Groups   Name        Variance Std.Dev.\n>#  id       (Intercept)  8.614   2.935   \n>#  Residual             39.148   6.257   \n># Number of obs: 7185, groups:  id, 160\n># \n># Fixed effects:\n>#             Estimate Std. Error t value\n># (Intercept)  12.6370     0.2444   51.71\n\nShowing the variations across (a subset of) schools\n\n\n# Randomly select 10 school ids\nrandom_ids <- sample(unique(hsball$id), size = 10)\n(p_subset <- hsball %>%\n    filter(id %in% random_ids) %>%  # select only 10 schools\n    ggplot(aes(x = id, y = mathach)) +\n    geom_jitter(height = 0, width = 0.1, alpha = 0.3) +\n    # Add school means\n    stat_summary(\n      fun = \"mean\",\n      geom = \"point\",\n      col = \"red\",\n      shape = 17,\n      # use triangles\n      size = 4\n    )  # make them larger\n)\n\n\n\n\nSimulating data based on the random intercept model\n\\[Y_{ij} = \\gamma_{00} + u_{0j} + e_{ij},\\]\n\n\ngamma00 <- 12.6370\ntau0 <- 2.935\nsigma <- 6.257\nnum_students <- nrow(hsball)\nnum_schools <- length(unique(hsball$id))\n# Simulate with only gamma00 (i.e., tau0 = 0 and sigma = 0)\nsimulated_data1 <- tibble(\n  id = hsball$id, \n  mathach = gamma00\n)\n# Show data with no variation\n# The `%+%` operator is use to substitute with a different data set\np_subset %+%\n  (simulated_data1 %>%\n     filter(id %in% random_ids))\n\n\n\n# Simulate with gamma00 + e_ij (i.e., tau0 = 0)\nsimulated_data2 <- tibble(\n  id = hsball$id, \n  mathach = gamma00 + rnorm(num_students, sd = sigma)\n)\n# Show data with no school-level variation\np_subset %+%\n  (simulated_data2 %>%\n     filter(id %in% random_ids))\n\n\n\n# Simulate with gamma00 + u_0j + e_ij\n# First, obtain group indices that starts from 1 to 160\ngroup_idx <- group_by(hsball, id) %>% group_indices()\n# Then simulate 160 u0j\nu0j <- rnorm(num_schools, sd = tau0)\nsimulated_data3 <- tibble(\n  id = hsball$id, \n  mathach = gamma00 + \n    u0j[group_idx] +  # expand the u0j's from 160 to 7185\n    rnorm(num_students, sd = sigma)\n)\n# Show data with both school and student variations\np_subset %+%\n  (simulated_data3 %>%\n     filter(id %in% random_ids))\n\n\n\n\nThe handy simulate() function can also be used to simulate the data\n\n\nsimulated_math <- simulate(ran_int, nsim = 1)\nsimulated_data4 <- tibble(\n  id = hsball$id, \n  mathach = simulated_math$sim_1\n)\np_subset %+%\n  (simulated_data4 %>%\n     filter(id %in% random_ids))\n\n\n\nPlotting the random effects (i.e., \\(u_{0j}\\))\nYou can easily plot the estimated school means (also called BLUP, best linear unbiased predictor, or the empirical Bayes (EB) estimates, which are different from the mean of the sample observations for a particular school) using the lattice package:\n\n\ndotplot(ranef(ran_int, condVar = TRUE))\n\n\n># $id\n\n\nHere’s a plot showing the sample schools means (with no borrowing of information) vs. the EB means (borrowing information).\n\n\n# Compute raw school means and EB means\nhsball %>% \n  group_by(id) %>% \n  # Raw means\n  summarise(mathach_raw_means = mean(mathach)) %>% \n  arrange(mathach_raw_means) %>%  # sort by the means\n  # EB means (the \".\" means using the current data)\n  mutate(mathach_eb_means = predict(ran_int, .), \n         index = row_number()) %>%  # add row number as index for plotting\n  ggplot(aes(x = index, y = mathach_raw_means)) + \n  geom_point(aes(col = \"Raw\")) + \n  # Add EB means\n  geom_point(aes(y = mathach_eb_means, col = \"EB\"), shape = 1) + \n  geom_segment(aes(x = index, xend = index, \n                   y = mathach_eb_means, yend = mathach_raw_means, \n                   col = \"EB\")) + \n  labs(y = \"School mean achievement\", col = \"\")\n\n\n\n\nIntraclass correlations\n\n\nvariance_components <- as.data.frame(VarCorr(ran_int))\nbetween_var <- variance_components$vcov[1]\nwithin_var <- variance_components$vcov[2]\n(icc <- between_var / (between_var + within_var))\n\n\n># [1] 0.1803518\n\n# 95% confidence intervals (require installing the bootmlm package)\n# if (!require(\"devtools\")) {\n#   install.packages(\"devtools\")\n# }\n# devtools::install_github(\"marklhc/bootmlm\")\nbootmlm:::prof_ci_icc(ran_int)\n\n\n>#     2.5 %    97.5 % \n># 0.1471784 0.2210131\n\nAdding Lv-2 Predictors\nWe have one predictor, meanses, in the fixed part.\n\n\nhsball %>% \n  ggplot(aes(x = meanses, y = mathach, col = id)) + \n  geom_point(alpha = 0.5, size = 0.5) + \n  guides(col = \"none\")\n\n\n\n\nModel Equation\nLv-1:\n\\[\\text{mathach}_{ij} = \\beta_{0j} + e_{ij}\\]\nLv-2:\n\\[\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{meanses}_j + u_{0j}\\] where \\(\\gamma_{00}\\) is the grand intercept, \\(\\gamma_{10}\\) is the regression coefficient of meanses that represents the expected difference in school mean achievement between two schools with one unit difference in meanses, and and \\(u_{0j}\\) is the deviation of the mean of the \\(j\\)th school from the grand mean.\n\n\nm_lv2 <- lmer(mathach ~ meanses + (1 | id), data = hsball)\nsummary(m_lv2)\n\n\n># Linear mixed model fit by REML ['lmerMod']\n># Formula: mathach ~ meanses + (1 | id)\n>#    Data: hsball\n># \n># REML criterion at convergence: 46961.3\n># \n># Scaled residuals: \n>#      Min       1Q   Median       3Q      Max \n># -3.13480 -0.75256  0.02409  0.76773  2.78501 \n># \n># Random effects:\n>#  Groups   Name        Variance Std.Dev.\n>#  id       (Intercept)  2.639   1.624   \n>#  Residual             39.157   6.258   \n># Number of obs: 7185, groups:  id, 160\n># \n># Fixed effects:\n>#             Estimate Std. Error t value\n># (Intercept)  12.6494     0.1493   84.74\n># meanses       5.8635     0.3615   16.22\n># \n># Correlation of Fixed Effects:\n>#         (Intr)\n># meanses -0.004\n\n# Likelihood-based confidence intervals for fixed effects\n# `parm = \"beta_\"` requests confidence intervals only for the fixed effects\nconfint(m_lv2, parm = \"beta_\")\n\n\n>#                 2.5 %    97.5 %\n># (Intercept) 12.356615 12.941707\n># meanses      5.155769  6.572415\n\nThe 95% confidence intervals (CIs) above showed the uncertainty associated with the estimates. Also, as the 95% CI for meanses does not contain zero, there is evidence for the positive association of SES and mathach at the school level.\n\n\nsjPlot::plot_model(m_lv2, type = \"pred\", terms = \"meanses\", \n                   show.data = TRUE, title = \"\", \n                   dot.size = 0.5) + \n  # Add the group means\n  stat_summary(data = hsball, aes(x = meanses, y = mathach), \n               fun = mean, geom = \"point\",\n               col = \"red\",\n               shape = 17,\n               # use triangles\n               size = 3, \n               alpha = 0.7)\n\n\n\n\nProportion of variance predicted\nWe will use the \\(R^2\\) statistic proposed by Nakagawa, Johnson & Schielzeth (2017) to obtain an \\(R^2\\) statistic. There are multiple versions of \\(R^2\\) in the literature, but personally I think this \\(R^2\\) avoids many of the problems in other variants and is most meaningful to interpret. Note that I only interpret the marginal \\(R^2\\).\n\n\n# Generally, you should use the marginal R^2 (R2m) for the variance predicted by\n# your predictors (`meanses` in this case).\nMuMIn::r.squaredGLMM(m_lv2)\n\n\n>#            R2m       R2c\n># [1,] 0.1233346 0.1786815\n\nAn alternative, more comprehensive approach is by Rights & Sterba (2019, Psychological Methods, https://doi.org/10.1037/met0000184), with the r2mlm package\n\n\nr2mlm::r2mlm(m_lv2)\n\n\n\n># $Decompositions\n>#                 total              within between          \n># fixed, within   0                  0      NA               \n># fixed, between  0.123334641367122  NA     0.690248683624938\n># slope variation 0                  0      NA               \n># mean variation  0.0553468169145697 NA     0.309751316375062\n># sigma2          0.821318541718308  1      NA               \n># \n># $R2s\n>#     total              within between          \n># f1  0                  0      NA               \n># f2  0.123334641367122  NA     0.690248683624938\n># v   0                  0      NA               \n># m   0.0553468169145697 NA     0.309751316375062\n># f   0.123334641367122  NA     NA               \n># fv  0.123334641367122  0      NA               \n># fvm 0.178681458281692  NA     NA\n\nNote the fixed, between number in the total column is the same as the one from MuMIn::r.squaredGLMM(). Including school means of SES in the model accounted for about 12% of the total variance of math achievement.\nComparing to OLS regression\nNotice that the standard error with regression is only half of that with MLM.\n\n\nm_lm <- lm(mathach ~ meanses, data = hsball)\nmsummary(list(\"MLM\" = m_lv2, \n              \"Linear regression\" = m_lm))\n\n\n\n\n\nMLM\n\n\nLinear regression\n\n\n(Intercept)\n\n\n12.649\n\n\n12.713\n\n\n\n\n(0.149)\n\n\n(0.076)\n\n\nmeanses\n\n\n5.864\n\n\n5.717\n\n\n\n\n(0.361)\n\n\n(0.184)\n\n\nsd__(Intercept)\n\n\n1.624\n\n\n\n\nsd__Observation\n\n\n6.258\n\n\n\n\nNum.Obs.\n\n\n\n\n7185\n\n\nR2\n\n\n\n\n0.118\n\n\nR2 Adj.\n\n\n\n\n0.118\n\n\nAIC\n\n\n46969.3\n\n\n47202.4\n\n\nBIC\n\n\n46996.8\n\n\n47223.0\n\n\nLog.Lik.\n\n\n−23480.642\n\n\n−23598.190\n\n\nF\n\n\n\n\n962.329\n\n\nREMLcrit\n\n\n46961.285\n\n\n\n\n\n\n\n\n",
      "last_modified": "2021-09-07T14:54:32-07:00"
    },
    {
      "path": "syllabus.html",
      "title": "Syllabus",
      "description": "Course syllabus (PSYC 575, 2021 Fall)\n",
      "author": [],
      "contents": "\n\nContents\nPSYC 575 Multilevel Modeling\nCourse Description\nLearning Objectives\nCourse Notes\nCommunication\nTechnological Proficiency and Hardware/Software Required\nUSC technology rental program\nUSC Technology Support Links\n\nRecommended Materials\nOptional Materials\nDescription and Assessment of Assignments\nParticipation\nGrading Breakdown\nGrading Scale\nAssignment Submission\nGrading Timeline\nLate work\nTechnology in the classroom\nPhones\nTablets and Laptops\n\nAttendance\nClassroom Norms\nCOVID-19 Guidance\nCourse Evaluation\n(Tentative) Course Schedule: A Weekly Breakdown\n\nStatement on Academic Conduct and Support Systems\nAcademic Conduct:\nSupport Systems:\n\n\n\n\n\nPSYC 575 Multilevel Modeling\nUnits: 4Term–Day–Time: Fall 2021–Tues & Thurs–10:00-11:50 am\nLocation: WPH 205\nInstructor: Hok Chio (Mark) LaiOffice Hours: Tues 12:00–1:00 pm, and by appointment.Contact Info: (Email) hokchiol@usc.edu, (Slack) https://usc.enterprise.slack.com/.\nTimeline for replying to emails: within 48 hours.\nIT Help: ITS, BlackboardContact Info:\nITS (Email, Monday – Friday, 8:00 A.M. – 6:00 P.M.) consult@usc.edu, (Phone, 24/7/365) 213-740-5555, (Online) ServiceNow Portal\nBlackboard (Email, 24/7/365) blackboard@usc.edu, (Online Help) Blackboard Help for Students\nCourse Description\nThis is a graduate-level class in statistical methods on multilevel modeling, a popular technique in behavioral and social science research. The course covers topics in multilevel modeling, including two- and three-level hierarchical linear models (HLM), random intercepts and slopes, longitudinal models and growth curve models, and some recent development in multilevel modeling.\nThe course begins with a brief overview of the ubiquity of multilevel data and the problems of using conventional methods to handle such data. It then transitions to the conceptual and statistical foundations of two-level multilevel models. Students will learn from different real data examples, and perform analyses using data of their own or provided by the instructor. Later material covers the use of multilevel modeling as a general framework for longitudinal data analysis, and other modeling considerations such as categorical data, non-hierarchical (e.g., cross-classified) data structure, and study designs. Students are also encouraged to provide input in suggesting topics to be covered for this course.\nLearning Objectives\nAfter the successful completion of this course, students will be able to . . .\nExplain the problems of analyzing clustered data with multiple regression/ANOVA;\nIdentify the types of multilevel data structure in different research scenarios;\nDescribe the statistical and conceptual foundations of multilevel modeling;\nIndependently analyze real data using statistical software for multilevel modeling;\nEvaluate published research that uses multilevel modeling;\nApply multilevel modeling in a research project, and effectively communicate findings/products in an oral research presentation or a written research report.\nPrerequisite(s): None\nCo-Requisite(s): None\nConcurrent Enrollment: None\nRecommended Preparation: PSYC 503: Regression and the General Linear Model (or a similar regression class); Experience with statistical software (preferably R)\nCourse Notes\nThis class will be in-person and will follow a flipped course design. The benefit of a flipped course model is that the lecturer can spend more time with students to go through applications of concepts and hands-on exercises of data analyses.\nThe lecture videos and course materials will be available at https://psyc575-2021fall.netlify.app by 9:00 am of each Monday, and students are expected to review these materials and the assigned readings on their own. Please note that the lecture slides only serve to guide class discussions and cannot replace the assigned readings.\nExcept for Week 1, the Tuesday meetings will be optional Q&A sessions where students can bring their questions so that the instructor and the class can discuss unclear concepts. During the mandatory Thursday meetings, students will work on quizzes, in-class exercises, and discuss questions regarding the learning materials and homework assignments. Students are expected to have reviewed the posted materials for that week before attending that week’s group exercise session.\nBefore attending the Thursday sessions, students are expected to have\nCompleted the assigned readings and reviewed the posted videos.\nIdentified questions that come up in their learning.\nStarted working on the homework problems.\nCommunication\nTo promote independence and critical thinking, students are encouraged to work through the following process for obtaining answers to course-related questions before contacting the instructor:\nconsult the course syllabus;\nconsult a classmate or post your questions on Slack;\nmeet with the instructor during office hours or Q&A sessions on Tuesdays;\nfor personal questions, email the instructor at hokchiol@usc.edu\nTechnological Proficiency and Hardware/Software Required\nR and RStudio are needed to complete the course assignments. It is highly recommended that students update to the latest versions of both software (R 4.1.0, RStudio 1.4.10XX, or above). We will discuss how to set up R and RStudio in Week 1.\nStable internet connection (for reviewing lecture videos)\nUSC technology rental program\nIf you need resources to successfully participate in your classes, such as a laptop or internet hotspot, you may be eligible for the university’s equipment rental program. To apply, please submit an application.\nUSC Technology Support Links\nBlackboard help for studentsSlack information for studentsSoftware available to USC Campus\nRecommended Materials\nSnijders, T. A. B., & Bosker, R. J. (2012). Multilevel analysis: An introduction to basic and advanced multilevel modeling (2nd ed.). Thousand Oaks, CA: Sage.\n(Alternative text) Hox, J. J., Moerbeek, M., & van de Schoot, R. (2018). Multilevel analysis: Techniques and Applications (3rd ed.). New York, NY: Routledge.\nOther required readings will be posted on Slack\nOptional Materials\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear models: applications and data analysis methods (2nd ed.). Thousand Oaks, CA: Sage.\nGelman, A., & Hill, J. (2006). _Data analysis using regression and multilevel/hierarchical models. Cambridge, UK: Cambridge University Press.\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford, UK: Oxford University Press. [For longitudinal data analysis]\nWest B. T., Welch, K. B., & Gałecki, A. T. (2014). Linear mixed models: A practical guide using statistical software (2nd ed.). Boca Raton, FL: CRC. [A reference for using different software]\nGałecki, A. T., & Burzykowski, T. (2013). Linear mixed-effects models using R: A step-by-step approach. Springer.\nLuke, D. A. (2020). Multilevel modeling (2nd ed.). Sage.\nHeck, R. H., Thomas, S. L., & Tabata, L. N. (2014). Multilevel and longitudinal modeling with IBM SPSS (2nd ed.). New York, NY: Routledge. [A reference for SPSS users]\nDescription and Assessment of Assignments\nIn-class exercises (10%). During the Thursday sessions, students will participate in group exercises. If students miss a session for any reason, they can complete the exercise posted on Blackboard within 36 hours (i.e., Friday by end of day, Pacific Time) to get credits.\nHomework problems (60%). There will be 10 homework assignments for students to apply the concepts and techniques discussed in class to analytic problems. The assignments typically involve performing data analyses using data sets of your own or provided by the instructor, and interpreting the results with some guided questions.\nYou must submit your work electronically to Blackboard by Friday 11:59 p.m. Pacific Time on the assigned due date. See policy on late work.\nFinal project (30%: 5% prospectus, 5% peer review, 20% presentation/final paper). You will complete a research project related to multilevel modeling, typically a research report of an empirical study using real data or a theoretical/methodological paper about certain aspects of multilevel modeling. Students interested in other project ideas (e.g., software package development) should discuss their ideas with the instructor. Each student can choose to work on their own or in a group of up to three people. Each student/group will schedule an appointment with the instructor to talk about their project during week 8 (October 5–9).\nThere are three grading components for your final project:\nProspectus (5%)\nA prospectus about your project should be submitted by Monday, October 25, 9:00 a.m. Pacific Time. The prospectus should contain a concise description of what you (or your group) plan to do for your project, including a preliminary plan for statistical analysis. The prospectus should be limited to 1 single-spaced page (excluding tables, figures, references, and other supplemental materials).\nPeer Review (5%)\nAfter the individual meeting with the instructor, each individual/group will refine their research questions and post a summary of their research questions and preliminary analyses to the dedicated forum on the Discussion Board by Friday, November 5. Each student will then give specific comments to the summaries of two other students/groups by Monday, November 15. More information on what feedback to give will be included in the grading rubric.\nFinal Presentation/Paper (20%)\nIf you choose to do a presentation, on November 30 or December 2, you or your group will give a 20-minute presentation (including Q&A) on your project. The final presentation should include the following four sections: introduction, method, results, and discussion; for methodological or theoretical work, students should follow organizations that are typical in previous papers in their areas of research. More emphasis should be put on describing the technical details of the analysis and the interpretations of the results. You will also need to submit your slides to Blackboard for grading on the day of your presentation, which should include a link to the reproducible codes for your analyses. A grading rubric on the research presentation will be posted on Blackboard.\nIf you choose to do a final paper, your paper will be due Tuesday, December 9, at 1:00 p.m. Pacific Time (the assigned final exam time for the class). The final paper should include four sections: introduction, method, results, and discussion, or comparable sections; however, more emphasis should be put on describing the technical details of the analysis and the interpretations of the results. There should also be a link to the reproducible codes for your analyses. The final paper should be 8-15 double-spaced pages of text (i.e., excluding title page, abstract, references, tables, figures, and appendices).\n\nParticipation\nParticipation accounts for 10% of the course grade. To earn full credit for participation, students should complete and turn in all in-class exercises.\nGrading Breakdown\nAssignment\n% of Grade\nIn-class exercises\n10\nHomework 1-9\n60\nProspectus\n5\nPeer Review\n5\nFinal Presentation/Paper\n20\nTOTAL\n100\nGrading Scale\nCourse final grades will be determined using the following scale\nA\n93-100\nA-\n89-92\nB+\n85-88\nB\n81-84\nB-\n77-80\nC+\n73-76\nC\n70-72\nC-\nBelow 70 (failing)\nAssignment Submission\nThe assignments should be submitted through Blackboard by Friday at 11:59 p.m. Pacific Time, before the class starts.\nGrading Timeline\nGenerally, all graded work will be returned no later than one week from the submission deadline. However, given the high number of students in the class, the instructor may only grade selected questions in each assignment. Solutions will be posted so that students can check their own work.\nLate work\nLate work will be penalized by a 10% deduction in the assignment grade every 24 hours late unless due to an emergency excused by the instructor. Email the instructor as soon as possible to discuss alternate arrangements due to an emergency.\nTechnology in the classroom\nPhones\nYour phone should be turned off or in silent mode (not on vibrate), and should not be used in the classroom.\nTablets and Laptops\nDuring lecture time in the classroom, students can use tablets and laptops only for purposes of viewing course materials and taking notes. Use of tablets and laptops for note taking is strongly discouraged as it may distract both yourself and your peers (Sana, Weston, & Cepeda, 2013). During the in-class exercises, students should use their laptops to complete the assignments.\nAttendance\nStudents are expected to attend all Thursday class sessions on time. If they miss a session, they should complete the class exercises and turn in their work within the timeframe specified in Description and Assessment of Assignments.\nClassroom Norms\nFrom USC’s FALL 2021 GUIDE: Return To Campus Protocols document,\n\nStudents, faculty and staff need to be aware of COVID-19 symptoms, and are required to complete a daily self-screening via Trojan Check before coming onto campus or leaving their on-campus residence.\n\n\nStudents, faculty, and staff are required to wear masks indoors, including classrooms – and no food or drink is permitted during class\n\nThe following applies to both in-person and online communications (e.g., Slack discussions and email communications)\nRespect each other’s views.\nIn written communication messages, make sure they are something you could say to someone to their face.\nRecognize and/or remember that we have different backgrounds. \nCriticize ideas, not individuals or groups.\nEither support statements with evidence, or speak from personal experience.\nCOVID-19 Guidance\nStudents should consult the latest COVID-19 testing and health protocol requirements for on-campus courses. Continuously updated requirements can be found on the USC COVID-19 resource center website at https://coronavirus.usc.edu/ and https://we-are.usc.edu/.\nCourse Evaluation\nStudent feedback is essential to the instructor and the Department to keep improving this course. Students are encouraged to share their feedback and suggestions in an early-term feedback survey around week 4 to 5, and respond to the standard USC course evaluation survey at the end of the semester.\n(Tentative) Course Schedule: A Weekly Breakdown\n\n\n\n\nTopics/Daily Activities\nReadings\nAssignment Dates\nWeek 1\nAug 24 & 26\nOverview of Multilevel Models\nR Markdown\n\nSB ch 1, 2\nR Markdown Intro\nMarkdown Quick Reference\n\nExercise: R Markdown\nHW 1\nQuiz on Regression\n\nWeek 2\nAug 31 & Sep 2\nWhat are Statistical Models?\nReview of Regression\n\nGelman et al. ch 1.1, 1.2, 1.4\nGelman et al. ch 4.1, 4.2, 4.4, 4.5\n10 quick tips to improve your regression modeling\n\nExercise: Interpreting interactions\nHW 2\n\nWeek 3\nSep 7 & 9\nThe Random Intercept Model\n\nSB ch 3.1–3.4, 4.1–4.5, 4.8\n\nExercise: Empirical Bayes estimates\nHW 3\n\nWeek 4\nSep 14 & 16\nEffect Decomposition\nRandom Coefficient Model\nCross-level Interactions\n\nSB ch 4.6, 5.1–5.3\n\nExercise: Effect decomposition\nHW 4\n\nWeek 5\nSep 21 & 23\nModel Estimation\nModel Testing\nReporting Results\n\nSB ch 4.7, 6\n\nExercise: Maximum likelihood estimation (MLE)\nHW 5\n\nWeek 6\nSep 28 & 30\nModel Assumptions and Diagnostics\n\nSB ch 10\n\nExercise: TBD\nHW 6\n\nWeek 7\nOct 5 & 7\nMLM for Experimental Designs\nCross-classified Models\n\nSB ch 13.1\nHoffman & Rovine (2007)\n\nExercise: Identifying data structure\nHW 7\n\nWeek 8\nOct 12 & 14 (Fall recess)\nModels for Longitudinal Data I\nUsing GitHub\n\nSB ch 15\n\n\nWeek 9\nOct 19 & 21\nModels for Longitudinal Data II\n\nHoffman (2014) ch 8\n\nExercise: Autoregressive and lagged effects\nHW 8\nProspectus (due Oct 25)\n\nWeek 10\nOct 26 & 28\nIndividual meeting on final research project\n\n\nWeek 11\nNov 2 & 4\nPredictive Modeling\n\nYarkoni & Westfall (2017)\n\nExercise: Model averaging\nPost draft for peer review\n\nWeek 12\nNov 9 & 11\nMultilevel logistic regression\nDiscrete outcomes\n\nSB ch 17\n\nExercise: Probability vs. odds ratio\nHW 9\nPeer review (due Nov 15)\n\nWeek 13\nNov 16 & 18\nSample size planning\n\nSB ch 11\n\nExercise: Required sample size\nHW 10\n\nWeek 14\nNov 23 & 25 (Thanksgiving)\nMissing Data\n\nSB ch 9\nTBD\n\n\nWeek 15\nNov 30 & Dec 2\nFinal Presentation\n\nUpload slides (on presentation day)\nFINAL\n\n\nFinal paper (due Dec 9, 1:00 pm)\nSB = Snijders & Bosker (2012)\nStatement on Academic Conduct and Support Systems\nAcademic Conduct:\nPlagiarism—presenting someone else’s ideas as your own, either verbatim or recast in your own words—is a serious academic offense with serious consequences. Please familiarize yourself with the discussion of plagiarism in SCampus in Part B, Section 11, “Behavior Violating University Standards” policy.usc.edu/scampus-part-b. Other forms of academic dishonesty are equally unacceptable. See additional information in SCampus and university policies on scientific misconduct, policy.usc.edu/scientific-misconduct.\nSupport Systems:\nCounseling and Mental Health - (213) 740-9355 – 24/7 on callstudenthealth.usc.edu/counseling\nFree and confidential mental health treatment for students, including short-term psychotherapy, group counseling, stress fitness workshops, and crisis intervention.\nNational Suicide Prevention Lifeline - 1 (800) 273-8255 – 24/7 on callsuicidepreventionlifeline.org\nFree and confidential emotional support to people in suicidal crisis or emotional distress 24 hours a day, 7 days a week.\nRelationship and Sexual Violence Prevention Services (RSVP) - (213) 740-9355(WELL), press “0” after hours - 24/7 on callstudenthealth.usc.edu/sexual-assault\nFree and confidential therapy services, workshops, and training for situations related to gender-based harm.\nOffice of Equity and Diversity (OED) - (213) 740-5086 | Title IX - (213) 821-8298equity.usc.edu, titleix.usc.edu\nInformation about how to get help or help someone affected by harassment or discrimination, rights of protected classes, reporting options, and additional resources for students, faculty, staff, visitors, and applicants.\nReporting Incidents of Bias or Harassment - (213) 740-5086 or (213) 821-8298usc-advocate.symplicity.com/care_report\nAvenue to report incidents of bias, hate crimes, and microaggressions to the Office of Equity and Diversity |Title IX for appropriate investigation, supportive measures, and response.\nThe Office of Disability Services and Programs - (213) 740-0776dsp.usc.edu\nSupport and accommodations for students with disabilities. Services include assistance in providing readers/notetakers/interpreters, special accommodations for test taking needs, assistance with architectural barriers, assistive technology, and support for individual needs.\nUSC Campus Support and Intervention - (213) 821-4710campussupport.usc.edu\nAssists students and families in resolving complex personal, financial, and academic issues adversely affecting their success as a student.\nDiversity at USC - (213) 740-2101diversity.usc.edu\nInformation on events, programs and training, the Provost’s Diversity and Inclusion Council, Diversity Liaisons for each academic school, chronology, participation, and various resources for students.\nUSC Emergency - UPC: (213) 740-4321, HSC: (323) 442-1000 – 24/7 on calldps.usc.edu, emergency.usc.edu\nEmergency assistance and avenue to report a crime. Latest updates regarding safety, including ways in which instruction will be continued if an officially declared emergency makes travel to campus infeasible.\nUSC Department of Public Safety - UPC: (213) 740-6000, HSC: (323) 442-1200 – 24/7 on calldps.usc.edu\nNon-emergency assistance or information.\n\n\n\n",
      "last_modified": "2021-09-08T18:15:06-07:00"
    },
    {
      "path": "week1.html",
      "title": "Week 1",
      "description": "Overview of Multilevel Models\n",
      "author": [],
      "contents": "\n\nContents\nWeek Learning Objectives\nTask Lists\n\nLecture\nSlides\nMultilevel Data Structure\nName and History\nUsage of MLM\n\n\nBefore you start the materials for week 1, make sure you have reviewed the syllabus.\nWeek Learning Objectives\nBy the end of this module, you will be able to\nIdentify alternative names for multilevel modeling (MLM)\nDescribe the types of data MLM can handle\nKnit a simple R Markdown file\nTask Lists\nAttend the Tuesday session for course introduction and Q&A\nInstall/Update R and RStudio on your computer\nReview the resources (lecture videos and slides)\nComplete the assigned readings\nSnijders & Bosker chapters 1 and 2 (they’re relatively short)\nR Markdown Intro\nMarkdown Quick Reference\n\nIntroduce yourself on the #introduction Slack channel (as part of HW 1)\nAttend the Thursday session and participate in the class exercise\nComplete Homework 1\nLecture\nSlides\nYou can view and download the slides here: HTML PDF\nMultilevel Data Structure\n\n\n\n\n\nCheck your learning: How would you describe the data structure in the video?\n\n\nobservational educational hierarchical \n\n\n function validate_form_24997() {var x, text; var x = document.forms[\"form_24997\"][\"answer_24997\"].value;if (x == \"hierarchical\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_24997').innerHTML = text; return false;} \nName and History\n\n\n\n\n\nCheck your learning: What is another name for multilevel modeling?\n\n\nmixed-effect modelinghierarchical regressionfixed-effect modeling\n\n\n function validate_form_97647() {var x, text; var x = document.forms[\"form_97647\"][\"answer_97647\"].value;if (x == \"mixed-effect modeling\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_97647').innerHTML = text; return false;} \nUsage of MLM\n\n\n\n\n\n\nAnother example that relates to ecological fallacies is the “happiness paradox” in economics, which says that income is related to happiness in a cross-sectional analysis (i.e., between-person level) but not in a longitudinal/time series analysis (i.e., within-person level).\n\nCheck your learning: In the data structure where there are multiple measurements for the same person, which level is level 1?\n\n\nmeasurement person \n\n\n function validate_form_25543() {var x, text; var x = document.forms[\"form_25543\"][\"answer_25543\"].value;if (x == \"measurement\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_25543').innerHTML = text; return false;} \n\n\n\n",
      "last_modified": "2021-08-29T15:17:45-07:00"
    },
    {
      "path": "week2.html",
      "title": "Week 2",
      "description": "Review of Regression\n",
      "author": [],
      "contents": "\n\nContents\nWeek Learning Objectives\nTask Lists\n\nLecture\nSlides\nStatistical Model\nImport Data\nLinear Regression\nSample regression line\nCentering\n\nCategorical Predictor\nMultiple Regression\nInteraction\n\n\nWeek Learning Objectives\nBy the end of this module, you will be able to\nDescribe the statistical model for regression\nWrite out the model equations\nSimulate data based on a regression model\nPlot interactions\nTask Lists\nIf you have questions, attend the Tuesday Q&A session\nComplete the assigned readings\nGelman et al. ch 1.1, 1.2, 1.4\nGelman et al. ch 4.1, 4.2, 4.4, 4.5\n10 quick tips to improve your regression modeling\n\nAttend the Thursday session and participate in the class exercise\nComplete Homework 2\nLecture\nSlides\nYou can view and download the slides here: HTML PDF\nStatistical Model\n\n\n\n\n\nCheck your learning: In the example in the video, why do we need a random component?\n\n\nBecause there is only one predictorBecause the relation between the variables is linearBecause people who spend the same amount of time studying do not always have the same mastery level\n\n\n function validate_form_45915() {var x, text; var x = document.forms[\"form_45915\"][\"answer_45915\"].value;if (x == \"Because people who spend the same amount of time studying do not always have the same mastery level\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_45915').innerHTML = text; return false;} \nImport Data\n\n\n\n\n\nCheck your learning: What is the coding for the sex variable?\n\n\n1 = male, 2 = female2 = male, 1 = female0 = female, 1 = male0 = male, 1 = female\n\n\n function validate_form_46999() {var x, text; var x = document.forms[\"form_46999\"][\"answer_46999\"].value;if (x == \"0 = male, 1 = female\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_46999').innerHTML = text; return false;} \n\n\n\n\n\nTake a pause and look at the scatterplot matrix. Ask yourself the following:\nHow does the distribution of salary look?\nAre there more males or females in the data?\nHow would you describe the relationship between number of publications and salary?\nLinear Regression\n\n\n\n\n\nSample regression line\n\n\n\n\n\nCheck your learning: How would you translate the regression line \\(y = \\beta_0 + \\beta_1 \\text{predictor1}\\) into R?\n\n\ny ~ 0 + predictor1predictor1 ~ 1 + yy ~ predictor1\n\n\n function validate_form_58019() {var x, text; var x = document.forms[\"form_58019\"][\"answer_58019\"].value;if (x == \"y ~ predictor1\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_58019').innerHTML = text; return false;} \nCentering\n\n\n\n\n\nCheck your learning: The mean of the pub variable is 18.2. If we call the mean-centered version of it as pub_c, what should be the value of pub_c for someone with 10 publications?\n\n\n 8.2  1.8  -8.2  1.82 \n\n\n function validate_form_92982() {var x, text; var x = document.forms[\"form_92982\"][\"answer_92982\"].value;if (x == \" -8.2\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_92982').innerHTML = text; return false;} \nCategorical Predictor\n\n\n\n\n\nCheck your learning: In a regression analysis, assume that there is a binary predictor that indicates whether a school is public (coded as 0) or private (coded as 1). If the coefficient for that predictor is 1.5, which category has a higher predicted score?\n\n\nprivate public \n\n\n function validate_form_59423() {var x, text; var x = document.forms[\"form_59423\"][\"answer_59423\"].value;if (x == \"private\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_59423').innerHTML = text; return false;} \nMultiple Regression\n\n\n\n\n\nThink more: the coefficient of pub_c becomes smaller after adding time into the equation. Why do you think that is the case?\nInteraction\n\n\n\n\n\n\n\n\n\n\nPratice yourself: from the interaction model obtain the regression line when pub = 50.\n\n\n\n",
      "last_modified": "2021-08-29T15:17:45-07:00"
    },
    {
      "path": "week3.html",
      "title": "Week 3",
      "description": "The Random Intercept Model\n",
      "author": [],
      "contents": "\n\nContents\nWeek Learning Objectives\nTask Lists\n\nLecture\nSlides\nOverview\nUnconditional Random Intercept Model\nEquations\nPath diagram\n\nFixed and Random Effects\nThe Intraclass Correlation\nEmpirical Bayes Estimates\nAdding a Level-2 Predictor\nThe design effect\nAggregation\nStandard error estimates under OLS and MLM\nModel equations\nMLM with a level-2 predictor in R\nStatistical inference\n\n\n\nWeek Learning Objectives\nBy the end of this module, you will be able to\nExplain the components of a random intercept model\nInterpret intraclass correlations\nUse the design effect to decide whether MLM is needed\nExplain why ignoring clustering (e.g., regression) leads to inflated chances of Type I errors\nDescribe how MLM pools information to obtain more stable inferences of groups\nTask Lists\nReview the resources (lecture videos and slides)\nComplete the assigned readings\nSnijders & Bosker ch 3.1–3.4, 4.1–4.5, 4.8\n\nAttend the Thursday session and participate in the class exercise\nComplete Homework 3\nLecture\nSlides\nYou can view and download the slides here: PDF\nOverview\n\n\n\n\n\nCheck your learning: Here’s a snapshot of the sleepstudy data:\n\n   Reaction Days Subject\n1  249.5600    0     308\n2  258.7047    1     308\n3  250.8006    2     308\n11 222.7339    0     309\n12 205.2658    1     309\n13 202.9778    2     309\n21 199.0539    0     310\n22 194.3322    1     310\n23 234.3200    2     310\n\nwhere Subject is the cluster ID. Is Days a level-1 or a level-2 variable?\n\n\nLevel-1Level-2\n\n\n function validate_form_91440() {var x, text; var x = document.forms[\"form_91440\"][\"answer_91440\"].value;if (x == \"Level-1\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_91440').innerHTML = text; return false;} \nUnconditional Random Intercept Model\nEquations\n\n\n\n\n\nCheck your learning: \\(u_{0j}\\) is the new term in a multilevel model (compared to regression). Is it a level-1 or a level-2 deviation variable?\n\n\nLevel-1Level-2\n\n\n function validate_form_91424() {var x, text; var x = document.forms[\"form_91424\"][\"answer_91424\"].value;if (x == \"Level-2\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_91424').innerHTML = text; return false;} \nPath diagram\n\n\n\n\n\nCheck your learning: For the diagram in the video, which one is an actual variable in the data?\n\n\n\\(\\gamma_{00}\\)\\(\\beta_{0j}\\)\\(u_{0j}\\)\\(Y_{ij}\\)\n\n\n function validate_form_80845() {var x, text; var x = document.forms[\"form_80845\"][\"answer_80845\"].value;if (x == \"$Y_{ij}$\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_80845').innerHTML = text; return false;} \nFixed and Random Effects\n\n\n\n\n\nCheck your learning: For the unconditional model, which of the following is a fixed effect?\n\n\nschool meansindividual scoresvariance componentsthe grand mean\n\n\n function validate_form_59999() {var x, text; var x = document.forms[\"form_59999\"][\"answer_59999\"].value;if (x == \"the grand mean\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_59999').innerHTML = text; return false;} \nThe Intraclass Correlation\n\n\n\n\n\nNote: On the slide around the 9 minute mark, the numbers labeled the “Std.Dev.” is just the square root of the variance components. That is, the standard deviation of the school means and the within-school standard deviation.\nCheck your learning: For a study, if \\(\\tau^2_0 = 5\\), \\(\\sigma^2 = 10\\), what is the ICC?\n\n\n0.52.00.333\n\n\n function validate_form_47097() {var x, text; var x = document.forms[\"form_47097\"][\"answer_47097\"].value;if (x == \"0.333\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_47097').innerHTML = text; return false;} \nCheck your learning: The graph below shows the distribution of the Reaction variable in the sleepstudy data. What do you think is a good guess for the its ICC?\n\n\n\n\n\n00.40.9\n\n\n function validate_form_50137() {var x, text; var x = document.forms[\"form_50137\"][\"answer_50137\"].value;if (x == \"0.4\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_50137').innerHTML = text; return false;} \nEmpirical Bayes Estimates\nNote: OLS = ordinary least squares, the estimation method commonly used in regular regression.\n\n\n\n\n\nThinking exercise: When \\(\\sigma^2 / n_j = 0\\), \\(\\lambda_j = 1\\), and the empirical Bayes estimate will be the same as the sample school mean, meaning that there is no borrowing of information. Why is there no need to borrow information in this situation?\nAdding a Level-2 Predictor\nNote that the ses was standardized in the data set, meaning that ses = 0 is at the sample mean, and ses = 1 means one standard deviation above the mean.\n\n\n\n\n\nCheck your learning: In regression, the independent observation assumption means that\n\n\nThe predictor variables should be independentKnowing the score of one observation gives no information about other observationsThe data should follow a hierarchical structure\n\n\n function validate_form_60804() {var x, text; var x = document.forms[\"form_60804\"][\"answer_60804\"].value;if (x == \"Knowing the score of one observation gives no information about other observations\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_60804').innerHTML = text; return false;} \nThe design effect\n\n\n\n\n\nPractice yourself: Compute the design effect for mathach for the HSB data. Which of the following is the closest to your computation?\n\n\n8.18.929.61294.1\n\n\n function validate_form_17298() {var x, text; var x = document.forms[\"form_17298\"][\"answer_17298\"].value;if (x == \"8.9\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_17298').innerHTML = text; return false;} \nBonus Challenge: What is the design effect for a longitudinal study of 5 waves with 30 individuals, and with an ICC for the outcome of 0.5?\n\n\n1315.5\n\n\n function validate_form_45163() {var x, text; var x = document.forms[\"form_45163\"][\"answer_45163\"].value;if (x == \"3\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_45163').innerHTML = text; return false;} \nAggregation\nWhile disaggregation yields results with standard errors being too small, aggregation generally results in standard errors that are slightly larger. The main problem of aggregation, however, is that it removes all the information in the lower level, so level-1 predictors cannot be studied. MLM avoids problems of both disaggregation and aggregation.\nStandard error estimates under OLS and MLM\nThis part is optional but gives a mathematical explanation of why OLS underestimates the standard error\n\n\n\n\n\nModel equations\n\n\n\n\n\nCheck your learning: In the level-2 equation with meanses as the predictor, what is the outcome variable?\n\n\nstudent math achievement scoresschool mean math achievementvariance of school means\n\n\n function validate_form_10185() {var x, text; var x = document.forms[\"form_10185\"][\"answer_10185\"].value;if (x == \"school mean math achievement\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_10185').innerHTML = text; return false;} \nMLM with a level-2 predictor in R\n\n\n\n\n\nCheck your learning: How do you interpret the coefficient for meanses?\n\n\nThe predicted difference in math achievement between two schools with one unit difference in mean SESThe mean achievement for a school with meanses = 0The variance of math achievement accounted for by meanses\n\n\n function validate_form_87116() {var x, text; var x = document.forms[\"form_87116\"][\"answer_87116\"].value;if (x == \"The predicted difference in math achievement between two schools with one unit difference in mean SES\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_87116').innerHTML = text; return false;} \nStatistical inference\n\n\n\n\n\nNote: If the 95% CI exlcudes zero, there is evidence that the predictor has a nonzero relation with the outcome.\nCheck your learning: By default, what type of confidence interval is computed by the lme4 package?\n\n\nLikelihood-basedWaldScoreBootstrap\n\n\n function validate_form_40004() {var x, text; var x = document.forms[\"form_40004\"][\"answer_40004\"].value;if (x == \"Likelihood-based\"){text = 'Correct :+1:';} else {text = 'That is not correct. Rewatch the video if needed';} document.getElementById('result_40004').innerHTML = text; return false;} \n\n\n\n",
      "last_modified": "2021-09-05T14:29:50-07:00"
    }
  ],
  "collections": []
}
